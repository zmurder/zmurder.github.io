<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
  <meta name="msvalidate.01" content="7EC20DBC74B004C2782077570E15C280">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
    <meta name="description" content="2.3 组织并行线程从前面的例子可以看出，如果使用了合适的网格和块大小来正确地组织线程，那么可以对内核性能产生很大的影响。在向量加法的例子中，为了实现最佳性能我们调整了块的大小，并基于块大小和向量数据大小计算出了网格大小。现在通过一个矩阵加法的例子来进一步说明这一点。">
<meta property="og:type" content="article">
<meta property="og:title" content="2-3 组织并行线程">
<meta property="og:url" content="http://example.com/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/index.html">
<meta property="og:site_name" content="奔跑的IC">
<meta property="og:description" content="2.3 组织并行线程从前面的例子可以看出，如果使用了合适的网格和块大小来正确地组织线程，那么可以对内核性能产生很大的影响。在向量加法的例子中，为了实现最佳性能我们调整了块的大小，并基于块大小和向量数据大小计算出了网格大小。现在通过一个矩阵加法的例子来进一步说明这一点。">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/image-20230406104150517.png">
<meta property="og:image" content="http://example.com/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/image-20230406105408293.png">
<meta property="og:image" content="http://example.com/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/2023-04-06_13-07.png">
<meta property="og:image" content="http://example.com/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/image-20230406143717303.png">
<meta property="og:image" content="http://example.com/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/image-20230406145146080.png">
<meta property="og:image" content="http://example.com/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/image-20230406150809079.png">
<meta property="og:image" content="http://example.com/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/image-20230406151834155.png">
<meta property="article:published_time" content="2024-12-01T10:13:43.571Z">
<meta property="article:modified_time" content="2024-12-01T10:13:43.571Z">
<meta property="article:author" content="奔跑的IC">
<meta property="article:tag" content="C">
<meta property="article:tag" content="GPU">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/image-20230406104150517.png">


<link rel="canonical" href="http://example.com/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://example.com/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/","path":"CUDA/CUDA C编程权威指南笔记/2-3 组织并行线程/","title":"2-3 组织并行线程"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>2-3 组织并行线程 | 奔跑的IC</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">奔跑的IC</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-reorder fa-fw"></i>文章列表</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#2-3-%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B"><span class="nav-text">2.3 组织并行线程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-1-%E4%BD%BF%E7%94%A8%E5%9D%97%E5%92%8C%E7%BA%BF%E7%A8%8B%E5%BB%BA%E7%AB%8B%E7%9F%A9%E9%98%B5%E7%B4%A2%E5%BC%95"><span class="nav-text">2.3.1 使用块和线程建立矩阵索引</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-2-%E4%BD%BF%E7%94%A8%E4%BA%8C%E7%BB%B4%E7%BD%91%E6%A0%BC%E5%92%8C%E4%BA%8C%E7%BB%B4%E5%9D%97%E5%AF%B9%E7%9F%A9%E9%98%B5%E6%B1%82%E5%92%8C"><span class="nav-text">2.3.2 使用二维网格和二维块对矩阵求和</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-3-%E4%BD%BF%E7%94%A8%E4%B8%80%E7%BB%B4%E7%BD%91%E6%A0%BC%E5%92%8C%E4%B8%80%E7%BB%B4%E5%9D%97%E5%AF%B9%E7%9F%A9%E9%98%B5%E6%B1%82%E5%92%8C"><span class="nav-text">2.3.3 使用一维网格和一维块对矩阵求和</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-4-%E4%BD%BF%E7%94%A8%E4%BA%8C%E7%BB%B4%E7%BD%91%E6%A0%BC%E5%92%8C%E4%B8%80%E7%BB%B4%E5%9D%97%E5%AF%B9%E7%9F%A9%E9%98%B5%E6%B1%82%E5%92%8C"><span class="nav-text">2.3.4 使用二维网格和一维块对矩阵求和</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="奔跑的IC"
      src="/images/zyd.gif">
  <p class="site-author-name" itemprop="name">奔跑的IC</p>
  <div class="site-description" itemprop="description">死磕牛角的IT农民工</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">186</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zyd.gif">
      <meta itemprop="name" content="奔跑的IC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="奔跑的IC">
      <meta itemprop="description" content="死磕牛角的IT农民工">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="2-3 组织并行线程 | 奔跑的IC">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          2-3 组织并行线程
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-01 18:13:43" itemprop="dateCreated datePublished" datetime="2024-12-01T18:13:43+08:00">2024-12-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CUDA/" itemprop="url" rel="index"><span itemprop="name">CUDA</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CUDA/CUDA-C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">CUDA C编程权威指南笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="2-3-组织并行线程"><a href="#2-3-组织并行线程" class="headerlink" title="2.3 组织并行线程"></a>2.3 组织并行线程</h1><p>从前面的例子可以看出，如果使用了合适的网格和块大小来正确地组织线程，那么可以对内核性能产生很大的影响。在向量加法的例子中，为了实现最佳性能我们调整了块的大小，并基于块大小和向量数据大小计算出了网格大小。<br>现在通过一个矩阵加法的例子来进一步说明这一点。</p>
<h2 id="2-3-1-使用块和线程建立矩阵索引"><a href="#2-3-1-使用块和线程建立矩阵索引" class="headerlink" title="2.3.1 使用块和线程建立矩阵索引"></a>2.3.1 使用块和线程建立矩阵索引</h2><p><strong>通常情况下，一个矩阵用行优先的方法在全局内存中进行线性存储</strong>（最简单）。图2-9所示的是一个8×6矩阵的小例子。<br>在一个矩阵加法核函数中，<strong>一个线程通常被分配一个数据元素来处理</strong>。首先要完成的任务是使用块和线程索引<strong>从全局内存中访问指定的数据</strong>。通常情况下，对一个二维示例来说，需要管理3种索引：</p>
<img src="/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/image-20230406104150517.png" class="" title="image-20230406104150517">
<ul>
<li>线程和块索引</li>
<li>矩阵中给定点的坐标</li>
<li>全局线性内存中的偏移量</li>
</ul>
<p>图2-10说明了块和线程索引、矩阵坐标以及线性全局内存索引之间的对应关系。</p>
<p>内存的索引代码如下</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">printThreadIndex</span><span class="params">(<span class="type">int</span> *A, <span class="type">const</span> <span class="type">int</span> nx, <span class="type">const</span> <span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> ix = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">int</span> iy = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx = iy * nx + ix;<span class="comment">//内存位置的索引</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;thread_id (%d,%d) block_id (%d,%d) coordinate (%d,%d) global index&quot;</span></span><br><span class="line">           <span class="string">&quot; %2d ival %2d\n&quot;</span>, threadIdx.x, threadIdx.y, blockIdx.x, blockIdx.y,</span><br><span class="line">           ix, iy, idx, A[idx]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<img src="/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/image-20230406105408293.png" class="" title="image-20230406105408293">
<p>由于我们测试的是8*6的矩阵（内存的内容和内存索引设定相同，方便分析），</p>
<ul>
<li>nx=8 ny=6 也就是<code>8*6</code>的矩阵</li>
<li>block 设定为（4，2），也就是一个block在x方向上有4个线程，在y方向上有2个线程</li>
<li>grid计算得出（2，1），也就是一个grid在x方向上有2个block，在y方向上有1个block</li>
<li><strong>一个线程对应一个内存地址计算</strong></li>
</ul>
<p>注意：下图中红色框是grid的索引（图中描述的不太合适，意思就是grid在x方向上有2个block索引，在y方向上有一个索引）。绿色框是block的索引（图中描述的不太合适，意思就是block在x方向上有4个thread索引，在y方向上有2个thread索引）。需要注意的是内存的排列，<strong>例如block（0，0）中线程对应的内存索引的是0,1,2,3和8,9,10,11，而不是0,1,2,3,4,5,6,7</strong>。下面会有程序的测试结果</p>
<img src="/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/2023-04-06_13-07.png" class="" title="2023-04-06_13-07">
<p>下面的代码打印了对应的索引</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//chapter02/checkThreadIndex.cu</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;../common/common.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * This example helps to visualize the relationship between thread/block IDs and</span></span><br><span class="line"><span class="comment"> * offsets into data. For each CUDA thread, this example displays the</span></span><br><span class="line"><span class="comment"> * intra-block thread ID, the inter-block block ID, the global coordinate of a</span></span><br><span class="line"><span class="comment"> * thread, the calculated offset into input data, and the input data at that</span></span><br><span class="line"><span class="comment"> * offset.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">printMatrix</span><span class="params">(<span class="type">int</span> *C, <span class="type">const</span> <span class="type">int</span> nx, <span class="type">const</span> <span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> *ic = C;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\nMatrix: (%d.%d)\n&quot;</span>, nx, ny);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> iy = <span class="number">0</span>; iy &lt; ny; iy++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ix = <span class="number">0</span>; ix &lt; nx; ix++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%3d&quot;</span>, ic[ix]);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ic += nx;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">printThreadIndex</span><span class="params">(<span class="type">int</span> *A, <span class="type">const</span> <span class="type">int</span> nx, <span class="type">const</span> <span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> ix = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">int</span> iy = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx = iy * nx + ix;<span class="comment">//内存位置的索引</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;thread_id (%d,%d) block_id (%d,%d) coordinate (%d,%d) global index&quot;</span></span><br><span class="line">           <span class="string">&quot; %2d ival %2d\n&quot;</span>, threadIdx.x, threadIdx.y, blockIdx.x, blockIdx.y,</span><br><span class="line">           ix, iy, idx, A[idx]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%s Starting...\n&quot;</span>, argv[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// get device information</span></span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp deviceProp;</span><br><span class="line">    CHECK(cudaGetDeviceProperties(&amp;deviceProp, dev));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Using Device %d: %s\n&quot;</span>, dev, deviceProp.name);</span><br><span class="line">    CHECK(cudaSetDevice(dev));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set matrix dimension</span></span><br><span class="line">    <span class="type">int</span> nx = <span class="number">8</span>;</span><br><span class="line">    <span class="type">int</span> ny = <span class="number">6</span>;</span><br><span class="line">    <span class="type">int</span> nxy = nx * ny;</span><br><span class="line">    <span class="type">int</span> nBytes = nxy * <span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// malloc host memory</span></span><br><span class="line">    <span class="type">int</span> *h_A;</span><br><span class="line">    h_A = (<span class="type">int</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// iniitialize host matrix with integer</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nxy; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        h_A[i] = i;</span><br><span class="line">    &#125;</span><br><span class="line">    printMatrix(h_A, nx, ny);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// malloc device memory</span></span><br><span class="line">    <span class="type">int</span> *d_MatA;</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;d_MatA, nBytes));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// transfer data from host to device</span></span><br><span class="line">    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set up execution configuration</span></span><br><span class="line">    dim3 <span class="title function_">block</span><span class="params">(<span class="number">4</span>, <span class="number">2</span>)</span>;</span><br><span class="line">    dim3 <span class="title function_">grid</span><span class="params">((nx + block.x - <span class="number">1</span>) / block.x, (ny + block.y - <span class="number">1</span>) / block.y)</span>;<span class="comment">//(2,1)  nx=8 ny=6</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// invoke the kernel</span></span><br><span class="line">    printThreadIndex&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_MatA, nx, ny);</span><br><span class="line">    CHECK(cudaGetLastError());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// free host and devide memory</span></span><br><span class="line">    CHECK(cudaFree(d_MatA));</span><br><span class="line">    <span class="built_in">free</span>(h_A);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reset device</span></span><br><span class="line">    CHECK(cudaDeviceReset());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">./checkThreadIndex Starting...</span><br><span class="line">Using Device 0: Quadro P2000</span><br><span class="line"></span><br><span class="line">Matrix: (8.6)</span><br><span class="line">  0  1  2  3  4  5  6  7</span><br><span class="line">  8  9 10 11 12 13 14 15</span><br><span class="line"> 16 17 18 19 20 21 22 23</span><br><span class="line"> 24 25 26 27 28 29 30 31</span><br><span class="line"> 32 33 34 35 36 37 38 39</span><br><span class="line"> 40 41 42 43 44 45 46 47</span><br><span class="line"></span><br><span class="line">thread_id (0,0) block_id (0,1) coordinate (0,2) global index 16 ival 16</span><br><span class="line">thread_id (1,0) block_id (0,1) coordinate (1,2) global index 17 ival 17</span><br><span class="line">thread_id (2,0) block_id (0,1) coordinate (2,2) global index 18 ival 18</span><br><span class="line">thread_id (3,0) block_id (0,1) coordinate (3,2) global index 19 ival 19</span><br><span class="line">thread_id (0,1) block_id (0,1) coordinate (0,3) global index 24 ival 24</span><br><span class="line">thread_id (1,1) block_id (0,1) coordinate (1,3) global index 25 ival 25</span><br><span class="line">thread_id (2,1) block_id (0,1) coordinate (2,3) global index 26 ival 26</span><br><span class="line">thread_id (3,1) block_id (0,1) coordinate (3,3) global index 27 ival 27</span><br><span class="line">thread_id (0,0) block_id (0,0) coordinate (0,0) global index  0 ival  0 </span><br><span class="line">thread_id (1,0) block_id (0,0) coordinate (1,0) global index  1 ival  1 </span><br><span class="line">thread_id (2,0) block_id (0,0) coordinate (2,0) global index  2 ival  2 </span><br><span class="line">thread_id (3,0) block_id (0,0) coordinate (3,0) global index  3 ival  3 </span><br><span class="line">thread_id (0,1) block_id (0,0) coordinate (0,1) global index  8 ival  8 </span><br><span class="line">thread_id (1,1) block_id (0,0) coordinate (1,1) global index  9 ival  9 </span><br><span class="line">thread_id (2,1) block_id (0,0) coordinate (2,1) global index 10 ival 10 </span><br><span class="line">thread_id (3,1) block_id (0,0) coordinate (3,1) global index 11 ival 11 </span><br><span class="line">thread_id (0,0) block_id (0,2) coordinate (0,4) global index 32 ival 32</span><br><span class="line">thread_id (1,0) block_id (0,2) coordinate (1,4) global index 33 ival 33</span><br><span class="line">thread_id (2,0) block_id (0,2) coordinate (2,4) global index 34 ival 34</span><br><span class="line">thread_id (3,0) block_id (0,2) coordinate (3,4) global index 35 ival 35</span><br><span class="line">thread_id (0,1) block_id (0,2) coordinate (0,5) global index 40 ival 40</span><br><span class="line">thread_id (1,1) block_id (0,2) coordinate (1,5) global index 41 ival 41</span><br><span class="line">thread_id (2,1) block_id (0,2) coordinate (2,5) global index 42 ival 42</span><br><span class="line">thread_id (3,1) block_id (0,2) coordinate (3,5) global index 43 ival 43</span><br><span class="line">thread_id (0,0) block_id (1,1) coordinate (4,2) global index 20 ival 20</span><br><span class="line">thread_id (1,0) block_id (1,1) coordinate (5,2) global index 21 ival 21</span><br><span class="line">thread_id (2,0) block_id (1,1) coordinate (6,2) global index 22 ival 22</span><br><span class="line">thread_id (3,0) block_id (1,1) coordinate (7,2) global index 23 ival 23</span><br><span class="line">thread_id (0,1) block_id (1,1) coordinate (4,3) global index 28 ival 28</span><br><span class="line">thread_id (1,1) block_id (1,1) coordinate (5,3) global index 29 ival 29</span><br><span class="line">thread_id (2,1) block_id (1,1) coordinate (6,3) global index 30 ival 30</span><br><span class="line">thread_id (3,1) block_id (1,1) coordinate (7,3) global index 31 ival 31</span><br><span class="line">thread_id (0,0) block_id (1,0) coordinate (4,0) global index  4 ival  4</span><br><span class="line">thread_id (1,0) block_id (1,0) coordinate (5,0) global index  5 ival  5</span><br><span class="line">thread_id (2,0) block_id (1,0) coordinate (6,0) global index  6 ival  6</span><br><span class="line">thread_id (3,0) block_id (1,0) coordinate (7,0) global index  7 ival  7</span><br><span class="line">thread_id (0,1) block_id (1,0) coordinate (4,1) global index 12 ival 12</span><br><span class="line">thread_id (1,1) block_id (1,0) coordinate (5,1) global index 13 ival 13</span><br><span class="line">thread_id (2,1) block_id (1,0) coordinate (6,1) global index 14 ival 14</span><br><span class="line">thread_id (3,1) block_id (1,0) coordinate (7,1) global index 15 ival 15</span><br><span class="line">thread_id (0,0) block_id (1,2) coordinate (4,4) global index 36 ival 36</span><br><span class="line">thread_id (1,0) block_id (1,2) coordinate (5,4) global index 37 ival 37</span><br><span class="line">thread_id (2,0) block_id (1,2) coordinate (6,4) global index 38 ival 38</span><br><span class="line">thread_id (3,0) block_id (1,2) coordinate (7,4) global index 39 ival 39</span><br><span class="line">thread_id (0,1) block_id (1,2) coordinate (4,5) global index 44 ival 44</span><br><span class="line">thread_id (1,1) block_id (1,2) coordinate (5,5) global index 45 ival 45</span><br><span class="line">thread_id (2,1) block_id (1,2) coordinate (6,5) global index 46 ival 46</span><br><span class="line">thread_id (3,1) block_id (1,2) coordinate (7,5) global index 47 ival 47</span><br></pre></td></tr></table></figure>
<p>以第一个block（0，0）为例，可以看到内存索引的确是0,1,2,3,8,9,10,11</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">thread_id (0,0) block_id (0,0) coordinate (0,0) global index  0 ival  0 </span><br><span class="line">thread_id (1,0) block_id (0,0) coordinate (1,0) global index  1 ival  1 </span><br><span class="line">thread_id (2,0) block_id (0,0) coordinate (2,0) global index  2 ival  2 </span><br><span class="line">thread_id (3,0) block_id (0,0) coordinate (3,0) global index  3 ival  3 </span><br><span class="line">thread_id (0,1) block_id (0,0) coordinate (0,1) global index  8 ival  8 </span><br><span class="line">thread_id (1,1) block_id (0,0) coordinate (1,1) global index  9 ival  9 </span><br><span class="line">thread_id (2,1) block_id (0,0) coordinate (2,1) global index 10 ival 10 </span><br><span class="line">thread_id (3,1) block_id (0,0) coordinate (3,1) global index 11 ival 11</span><br></pre></td></tr></table></figure>
<h2 id="2-3-2-使用二维网格和二维块对矩阵求和"><a href="#2-3-2-使用二维网格和二维块对矩阵求和" class="headerlink" title="2.3.2 使用二维网格和二维块对矩阵求和"></a>2.3.2 使用二维网格和二维块对矩阵求和</h2><p>有了上面一节的内存索引，就可以计算两个矩阵的和了。和上一节的kernel几乎一样，也是索引到内存，但是多了一步数组相加的代码如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// grid 2D block 2D</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">sumMatrixOnGPU2D</span><span class="params">(<span class="type">float</span> *MatA, <span class="type">float</span> *MatB, <span class="type">float</span> *MatC, <span class="type">int</span> nx,</span></span><br><span class="line"><span class="params">                                 <span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> ix = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> iy = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx = iy * nx + ix;<span class="comment">//内存索引</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ix &lt; nx &amp;&amp; iy &lt; ny)<span class="comment">//新增 数组相加</span></span><br><span class="line">        MatC[idx] = MatA[idx] + MatB[idx];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>另外修改了矩阵和block的维度</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">   <span class="comment">// set up data size of matrix</span></span><br><span class="line">   <span class="type">int</span> nx = <span class="number">1</span> &lt;&lt; <span class="number">14</span>;</span><br><span class="line">   <span class="type">int</span> ny = <span class="number">1</span> &lt;&lt; <span class="number">14</span>;</span><br><span class="line"><span class="comment">/*....*/</span></span><br><span class="line">   <span class="comment">// invoke kernel at host side</span></span><br><span class="line">   <span class="type">int</span> dimx = <span class="number">32</span>;</span><br><span class="line">   <span class="type">int</span> dimy = <span class="number">32</span>;</span><br><span class="line">   dim3 <span class="title function_">block</span><span class="params">(dimx, dimy)</span>;</span><br><span class="line">   dim3 <span class="title function_">grid</span><span class="params">((nx + block.x - <span class="number">1</span>) / block.x, (ny + block.y - <span class="number">1</span>) / block.y)</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>程序完成代码如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//sumMatrixOnGPU-2D-grid-2D-block.cu</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;../common/common.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * This example demonstrates a simple vector sum on the GPU and on the host.</span></span><br><span class="line"><span class="comment"> * sumArraysOnGPU splits the work of the vector sum across CUDA threads on the</span></span><br><span class="line"><span class="comment"> * GPU. A 2D thread block and 2D grid are used. sumArraysOnHost sequentially</span></span><br><span class="line"><span class="comment"> * iterates through vector elements on the host.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">initialData</span><span class="params">(<span class="type">float</span> *ip, <span class="type">const</span> <span class="type">int</span> size)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ip[i] = (<span class="type">float</span>)(rand() &amp; <span class="number">0xFF</span>) / <span class="number">10.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">sumMatrixOnHost</span><span class="params">(<span class="type">float</span> *A, <span class="type">float</span> *B, <span class="type">float</span> *C, <span class="type">const</span> <span class="type">int</span> nx,</span></span><br><span class="line"><span class="params">                     <span class="type">const</span> <span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> *ia = A;</span><br><span class="line">    <span class="type">float</span> *ib = B;</span><br><span class="line">    <span class="type">float</span> *ic = C;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> iy = <span class="number">0</span>; iy &lt; ny; iy++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ix = <span class="number">0</span>; ix &lt; nx; ix++)</span><br><span class="line">        &#123;</span><br><span class="line">            ic[ix] = ia[ix] + ib[ix];</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ia += nx;</span><br><span class="line">        ib += nx;</span><br><span class="line">        ic += nx;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">checkResult</span><span class="params">(<span class="type">float</span> *hostRef, <span class="type">float</span> *gpuRef, <span class="type">const</span> <span class="type">int</span> N)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">double</span> epsilon = <span class="number">1.0E-8</span>;</span><br><span class="line">    <span class="type">bool</span> match = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">abs</span>(hostRef[i] - gpuRef[i]) &gt; epsilon)</span><br><span class="line">        &#123;</span><br><span class="line">            match = <span class="number">0</span>;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;host %f gpu %f\n&quot;</span>, hostRef[i], gpuRef[i]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (match)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Arrays match.\n\n&quot;</span>);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Arrays do not match.\n\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// grid 2D block 2D</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">sumMatrixOnGPU2D</span><span class="params">(<span class="type">float</span> *MatA, <span class="type">float</span> *MatB, <span class="type">float</span> *MatC, <span class="type">int</span> nx,</span></span><br><span class="line"><span class="params">                                 <span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> ix = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> iy = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx = iy * nx + ix;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ix &lt; nx &amp;&amp; iy &lt; ny)</span><br><span class="line">        MatC[idx] = MatA[idx] + MatB[idx];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%s Starting...\n&quot;</span>, argv[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set up device</span></span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp deviceProp;</span><br><span class="line">    CHECK(cudaGetDeviceProperties(&amp;deviceProp, dev));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Using Device %d: %s\n&quot;</span>, dev, deviceProp.name);</span><br><span class="line">    CHECK(cudaSetDevice(dev));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set up data size of matrix</span></span><br><span class="line">    <span class="type">int</span> nx = <span class="number">1</span> &lt;&lt; <span class="number">14</span>;</span><br><span class="line">    <span class="type">int</span> ny = <span class="number">1</span> &lt;&lt; <span class="number">14</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nxy = nx * ny;</span><br><span class="line">    <span class="type">int</span> nBytes = nxy * <span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Matrix size: nx %d ny %d\n&quot;</span>, nx, ny);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// malloc host memory</span></span><br><span class="line">    <span class="type">float</span> *h_A, *h_B, *hostRef, *gpuRef;</span><br><span class="line">    h_A = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    h_B = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    hostRef = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    gpuRef = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// initialize data at host side</span></span><br><span class="line">    <span class="type">double</span> iStart = seconds();</span><br><span class="line">    initialData(h_A, nxy);</span><br><span class="line">    initialData(h_B, nxy);</span><br><span class="line">    <span class="type">double</span> iElaps = seconds() - iStart;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Matrix initialization elapsed %f sec\n&quot;</span>, iElaps);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">memset</span>(hostRef, <span class="number">0</span>, nBytes);</span><br><span class="line">    <span class="built_in">memset</span>(gpuRef, <span class="number">0</span>, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add matrix at host side for result checks</span></span><br><span class="line">    iStart = seconds();</span><br><span class="line">    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);</span><br><span class="line">    iElaps = seconds() - iStart;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;sumMatrixOnHost elapsed %f sec\n&quot;</span>, iElaps);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// malloc device global memory</span></span><br><span class="line">    <span class="type">float</span> *d_MatA, *d_MatB, *d_MatC;</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;d_MatA, nBytes));</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;d_MatB, nBytes));</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;d_MatC, nBytes));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// transfer data from host to device</span></span><br><span class="line">    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));</span><br><span class="line">    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// invoke kernel at host side</span></span><br><span class="line">    <span class="type">int</span> dimx = <span class="number">32</span>;</span><br><span class="line">    <span class="type">int</span> dimy = <span class="number">32</span>;</span><br><span class="line">    dim3 <span class="title function_">block</span><span class="params">(dimx, dimy)</span>;</span><br><span class="line">    dim3 <span class="title function_">grid</span><span class="params">((nx + block.x - <span class="number">1</span>) / block.x, (ny + block.y - <span class="number">1</span>) / block.y)</span>;</span><br><span class="line"></span><br><span class="line">    iStart = seconds();</span><br><span class="line">    sumMatrixOnGPU2D&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_MatA, d_MatB, d_MatC, nx, ny);</span><br><span class="line">    CHECK(cudaDeviceSynchronize());</span><br><span class="line">    iElaps = seconds() - iStart;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;sumMatrixOnGPU2D &lt;&lt;&lt;(%d,%d), (%d,%d)&gt;&gt;&gt; elapsed %f sec\n&quot;</span>, grid.x,</span><br><span class="line">           grid.y,</span><br><span class="line">           block.x, block.y, iElaps);</span><br><span class="line">    <span class="comment">// check kernel error</span></span><br><span class="line">    CHECK(cudaGetLastError());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// copy kernel result back to host side</span></span><br><span class="line">    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// check device results</span></span><br><span class="line">    checkResult(hostRef, gpuRef, nxy);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// free device global memory</span></span><br><span class="line">    CHECK(cudaFree(d_MatA));</span><br><span class="line">    CHECK(cudaFree(d_MatB));</span><br><span class="line">    CHECK(cudaFree(d_MatC));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// free host memory</span></span><br><span class="line">    <span class="built_in">free</span>(h_A);</span><br><span class="line">    <span class="built_in">free</span>(h_B);</span><br><span class="line">    <span class="built_in">free</span>(hostRef);</span><br><span class="line">    <span class="built_in">free</span>(gpuRef);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reset device</span></span><br><span class="line">    CHECK(cudaDeviceReset());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>编译运行如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./sumMatrixOnGPU-2D-grid-2D-block</span><br><span class="line">./sumMatrixOnGPU-2D-grid-2D-block Starting...</span><br><span class="line">Using Device 0: Quadro P2000</span><br><span class="line">Matrix size: nx 16384 ny 16384</span><br><span class="line">Matrix initialization elapsed 8.372224 sec</span><br><span class="line">sumMatrixOnHost elapsed 0.252538 sec</span><br><span class="line">sumMatrixOnGPU2D &lt;&lt;&lt;(512,512), (32,32)&gt;&gt;&gt; elapsed 0.027212 sec</span><br><span class="line">Arrays match.</span><br></pre></td></tr></table></figure>
<p>书中修改了bloc大小为（32，16）性能翻了一倍，但是我自己测试变化不大，但是有一个结论就是<strong>不同的block和grid配置会影响到核函数的性能。</strong></p>
<h2 id="2-3-3-使用一维网格和一维块对矩阵求和"><a href="#2-3-3-使用一维网格和一维块对矩阵求和" class="headerlink" title="2.3.3 使用一维网格和一维块对矩阵求和"></a>2.3.3 使用一维网格和一维块对矩阵求和</h2><p>前面两节都是二维网格二维块来计算二维的矩阵，比较好理解。</p>
<p>为了使用一维网格和一维块，你需要写一个新的核函数，其中每个线程处理ny个数据元素，如图2-13所示。</p>
<img src="/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/image-20230406143717303.png" class="" title="image-20230406143717303">
<p>下图假如矩阵还是8*6</p>
<ul>
<li>nx=8 ny=6 也就是<code>8*6</code>的矩阵</li>
<li>block 设定为（4，1）</li>
<li>grid计算得出（2，1）</li>
<li><strong>与上面两节不同，一个线程需要计算6个内存的数据</strong></li>
<li><strong>下图中黑色的箭头代表一个线程，每个block有4个线程</strong></li>
<li><strong>灰色的数字是内存的内容，一个线程（箭头）处理6个数据，例如第一个线程处理的就是0,8,16,54,32,40共6个数据</strong></li>
<li><strong>绿色为一个block，红色为grid</strong></li>
</ul>
<img src="/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/image-20230406145146080.png" class="" title="image-20230406145146080">
<img src="/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/image-20230406150809079.png" class="" title="image-20230406150809079">
<p>对应的核函数代码为</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// grid 1D block 1D</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">sumMatrixOnGPU1D</span><span class="params">(<span class="type">float</span> *MatA, <span class="type">float</span> *MatB, <span class="type">float</span> *MatC, <span class="type">int</span> nx,<span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> ix = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ix &lt; nx )</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> iy = <span class="number">0</span>; iy &lt; ny; iy++)<span class="comment">//这里如果是8*6的矩阵 那么ny=6，这一个核函数处理6个数据</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> idx = iy * nx + ix;</span><br><span class="line">            MatC[idx] = MatA[idx] + MatB[idx];</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>完整的代码修改了数据的维度和block的维度</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//chapter02/sumMatrixOnGPU-1D-grid-1D-block.cu</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;../common/common.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * This example demonstrates a simple vector sum on the GPU and on the host.</span></span><br><span class="line"><span class="comment"> * sumArraysOnGPU splits the work of the vector sum across CUDA threads on the</span></span><br><span class="line"><span class="comment"> * GPU. A 1D thread block and 1D grid are used. sumArraysOnHost sequentially</span></span><br><span class="line"><span class="comment"> * iterates through vector elements on the host.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">initialData</span><span class="params">(<span class="type">float</span> *ip, <span class="type">const</span> <span class="type">int</span> size)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ip[i] = (<span class="type">float</span>)(rand() &amp; <span class="number">0xFF</span> ) / <span class="number">10.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">sumMatrixOnHost</span><span class="params">(<span class="type">float</span> *A, <span class="type">float</span> *B, <span class="type">float</span> *C, <span class="type">const</span> <span class="type">int</span> nx,</span></span><br><span class="line"><span class="params">                     <span class="type">const</span> <span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> *ia = A;</span><br><span class="line">    <span class="type">float</span> *ib = B;</span><br><span class="line">    <span class="type">float</span> *ic = C;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> iy = <span class="number">0</span>; iy &lt; ny; iy++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ix = <span class="number">0</span>; ix &lt; nx; ix++)</span><br><span class="line">        &#123;</span><br><span class="line">            ic[ix] = ia[ix] + ib[ix];</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ia += nx;</span><br><span class="line">        ib += nx;</span><br><span class="line">        ic += nx;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">checkResult</span><span class="params">(<span class="type">float</span> *hostRef, <span class="type">float</span> *gpuRef, <span class="type">const</span> <span class="type">int</span> N)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">double</span> epsilon = <span class="number">1.0E-8</span>;</span><br><span class="line">    <span class="type">bool</span> match = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">abs</span>(hostRef[i] - gpuRef[i]) &gt; epsilon)</span><br><span class="line">        &#123;</span><br><span class="line">            match = <span class="number">0</span>;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;host %f gpu %f\n&quot;</span>, hostRef[i], gpuRef[i]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (match)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Arrays match.\n\n&quot;</span>);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Arrays do not match.\n\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// grid 1D block 1D</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">sumMatrixOnGPU1D</span><span class="params">(<span class="type">float</span> *MatA, <span class="type">float</span> *MatB, <span class="type">float</span> *MatC, <span class="type">int</span> nx,</span></span><br><span class="line"><span class="params">                                 <span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> ix = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ix &lt; nx )</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> iy = <span class="number">0</span>; iy &lt; ny; iy++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> idx = iy * nx + ix;</span><br><span class="line">            MatC[idx] = MatA[idx] + MatB[idx];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%s Starting...\n&quot;</span>, argv[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set up device</span></span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp deviceProp;</span><br><span class="line">    CHECK(cudaGetDeviceProperties(&amp;deviceProp, dev));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Using Device %d: %s\n&quot;</span>, dev, deviceProp.name);</span><br><span class="line">    CHECK(cudaSetDevice(dev));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set up data size of matrix</span></span><br><span class="line">    <span class="type">int</span> nx = <span class="number">1</span> &lt;&lt; <span class="number">14</span>;</span><br><span class="line">    <span class="type">int</span> ny = <span class="number">1</span> &lt;&lt; <span class="number">14</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nxy = nx * ny;</span><br><span class="line">    <span class="type">int</span> nBytes = nxy * <span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Matrix size: nx %d ny %d\n&quot;</span>, nx, ny);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// malloc host memory</span></span><br><span class="line">    <span class="type">float</span> *h_A, *h_B, *hostRef, *gpuRef;</span><br><span class="line">    h_A = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    h_B = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    hostRef = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    gpuRef = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// initialize data at host side</span></span><br><span class="line">    <span class="type">double</span> iStart = seconds();</span><br><span class="line">    initialData(h_A, nxy);</span><br><span class="line">    initialData(h_B, nxy);</span><br><span class="line">    <span class="type">double</span> iElaps = seconds() - iStart;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;initialize matrix elapsed %f sec\n&quot;</span>, iElaps);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">memset</span>(hostRef, <span class="number">0</span>, nBytes);</span><br><span class="line">    <span class="built_in">memset</span>(gpuRef, <span class="number">0</span>, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add matrix at host side for result checks</span></span><br><span class="line">    iStart = seconds();</span><br><span class="line">    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);</span><br><span class="line">    iElaps = seconds() - iStart;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;sumMatrixOnHost elapsed %f sec\n&quot;</span>, iElaps);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// malloc device global memory</span></span><br><span class="line">    <span class="type">float</span> *d_MatA, *d_MatB, *d_MatC;</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;d_MatA, nBytes));</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;d_MatB, nBytes));</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;d_MatC, nBytes));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// transfer data from host to device</span></span><br><span class="line">    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));</span><br><span class="line">    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// invoke kernel at host side</span></span><br><span class="line">    <span class="type">int</span> dimx = <span class="number">32</span>;</span><br><span class="line">    dim3 <span class="title function_">block</span><span class="params">(dimx, <span class="number">1</span>)</span>;</span><br><span class="line">    dim3 <span class="title function_">grid</span><span class="params">((nx + block.x - <span class="number">1</span>) / block.x, <span class="number">1</span>)</span>;</span><br><span class="line"></span><br><span class="line">    iStart = seconds();</span><br><span class="line">    sumMatrixOnGPU1D&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_MatA, d_MatB, d_MatC, nx, ny);</span><br><span class="line">    CHECK(cudaDeviceSynchronize());</span><br><span class="line">    iElaps = seconds() - iStart;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;sumMatrixOnGPU1D &lt;&lt;&lt;(%d,%d), (%d,%d)&gt;&gt;&gt; elapsed %f sec\n&quot;</span>, grid.x,</span><br><span class="line">           grid.y,</span><br><span class="line">           block.x, block.y, iElaps);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// check kernel error</span></span><br><span class="line">    CHECK(cudaGetLastError());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// copy kernel result back to host side</span></span><br><span class="line">    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// check device results</span></span><br><span class="line">    checkResult(hostRef, gpuRef, nxy);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// free device global memory</span></span><br><span class="line">    CHECK(cudaFree(d_MatA));</span><br><span class="line">    CHECK(cudaFree(d_MatB));</span><br><span class="line">    CHECK(cudaFree(d_MatC));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// free host memory</span></span><br><span class="line">    <span class="built_in">free</span>(h_A);</span><br><span class="line">    <span class="built_in">free</span>(h_B);</span><br><span class="line">    <span class="built_in">free</span>(hostRef);</span><br><span class="line">    <span class="built_in">free</span>(gpuRef);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reset device</span></span><br><span class="line">    CHECK(cudaDeviceReset());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>运行结果如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./sumMatrixOnGPU-1D-grid-1D-block</span><br><span class="line">./sumMatrixOnGPU-1D-grid-1D-block Starting...</span><br><span class="line">Using Device 0: Quadro P2000</span><br><span class="line">Matrix size: nx 16384 ny 16384</span><br><span class="line">initialize matrix elapsed 8.340747 sec</span><br><span class="line">sumMatrixOnHost elapsed 0.249155 sec</span><br><span class="line">sumMatrixOnGPU1D &lt;&lt;&lt;(512,1), (32,1)&gt;&gt;&gt; elapsed 0.028243 sec</span><br><span class="line">Arrays match.</span><br></pre></td></tr></table></figure>
<h2 id="2-3-4-使用二维网格和一维块对矩阵求和"><a href="#2-3-4-使用二维网格和一维块对矩阵求和" class="headerlink" title="2.3.4 使用二维网格和一维块对矩阵求和"></a>2.3.4 使用二维网格和一维块对矩阵求和</h2><p>这可以看作是含有一个二维块的二维网格的特殊情况，其中块的第二个维数是1。因此，从块和线程索引到矩阵坐标的映射就变成：</p>
<img src="/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/image-20230406151834155.png" class="" title="image-20230406151834155">
<ul>
<li><strong>一个线程对应一个内存地址计算</strong></li>
</ul>
<p>核函数代码为</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// grid 2D block 1D</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">sumMatrixOnGPUMix</span><span class="params">(<span class="type">float</span> *MatA, <span class="type">float</span> *MatB, <span class="type">float</span> *MatC, <span class="type">int</span> nx,</span></span><br><span class="line"><span class="params">                                  <span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> ix = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> iy = blockIdx.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx = iy * nx + ix;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ix &lt; nx &amp;&amp; iy &lt; ny)</span><br><span class="line">        MatC[idx] = MatA[idx] + MatB[idx];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>完整代码如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;../common/common.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * This example demonstrates a simple vector sum on the GPU and on the host.</span></span><br><span class="line"><span class="comment"> * sumArraysOnGPU splits the work of the vector sum across CUDA threads on the</span></span><br><span class="line"><span class="comment"> * GPU. A 1D thread block and 2D grid are used. sumArraysOnHost sequentially</span></span><br><span class="line"><span class="comment"> * iterates through vector elements on the host.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">initialData</span><span class="params">(<span class="type">float</span> *ip, <span class="type">const</span> <span class="type">int</span> size)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ip[i] = (<span class="type">float</span>)(rand() &amp; <span class="number">0xFF</span>) / <span class="number">10.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">sumMatrixOnHost</span><span class="params">(<span class="type">float</span> *A, <span class="type">float</span> *B, <span class="type">float</span> *C, <span class="type">const</span> <span class="type">int</span> nx,</span></span><br><span class="line"><span class="params">                     <span class="type">const</span> <span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> *ia = A;</span><br><span class="line">    <span class="type">float</span> *ib = B;</span><br><span class="line">    <span class="type">float</span> *ic = C;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> iy = <span class="number">0</span>; iy &lt; ny; iy++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ix = <span class="number">0</span>; ix &lt; nx; ix++)</span><br><span class="line">        &#123;</span><br><span class="line">            ic[ix] = ia[ix] + ib[ix];</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ia += nx;</span><br><span class="line">        ib += nx;</span><br><span class="line">        ic += nx;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">checkResult</span><span class="params">(<span class="type">float</span> *hostRef, <span class="type">float</span> *gpuRef, <span class="type">const</span> <span class="type">int</span> N)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">double</span> epsilon = <span class="number">1.0E-8</span>;</span><br><span class="line">    <span class="type">bool</span> match = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">abs</span>(hostRef[i] - gpuRef[i]) &gt; epsilon)</span><br><span class="line">        &#123;</span><br><span class="line">            match = <span class="number">0</span>;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;host %f gpu %f\n&quot;</span>, hostRef[i], gpuRef[i]);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (match)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Arrays match.\n\n&quot;</span>);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Arrays do not match.\n\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// grid 2D block 1D</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">sumMatrixOnGPUMix</span><span class="params">(<span class="type">float</span> *MatA, <span class="type">float</span> *MatB, <span class="type">float</span> *MatC, <span class="type">int</span> nx,</span></span><br><span class="line"><span class="params">                                  <span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> ix = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> iy = blockIdx.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx = iy * nx + ix;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ix &lt; nx &amp;&amp; iy &lt; ny)</span><br><span class="line">        MatC[idx] = MatA[idx] + MatB[idx];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%s Starting...\n&quot;</span>, argv[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set up device</span></span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp deviceProp;</span><br><span class="line">    CHECK(cudaGetDeviceProperties(&amp;deviceProp, dev));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Using Device %d: %s\n&quot;</span>, dev, deviceProp.name);</span><br><span class="line">    CHECK(cudaSetDevice(dev));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set up data size of matrix</span></span><br><span class="line">    <span class="type">int</span> nx = <span class="number">1</span> &lt;&lt; <span class="number">14</span>;</span><br><span class="line">    <span class="type">int</span> ny = <span class="number">1</span> &lt;&lt; <span class="number">14</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nxy = nx * ny;</span><br><span class="line">    <span class="type">int</span> nBytes = nxy * <span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Matrix size: nx %d ny %d\n&quot;</span>, nx, ny);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// malloc host memory</span></span><br><span class="line">    <span class="type">float</span> *h_A, *h_B, *hostRef, *gpuRef;</span><br><span class="line">    h_A = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    h_B = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    hostRef = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    gpuRef = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// initialize data at host side</span></span><br><span class="line">    <span class="type">double</span> iStart = seconds();</span><br><span class="line">    initialData(h_A, nxy);</span><br><span class="line">    initialData(h_B, nxy);</span><br><span class="line">    <span class="type">double</span> iElaps = seconds() - iStart;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Matrix initialization elapsed %f sec\n&quot;</span>, iElaps);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">memset</span>(hostRef, <span class="number">0</span>, nBytes);</span><br><span class="line">    <span class="built_in">memset</span>(gpuRef, <span class="number">0</span>, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add matrix at host side for result checks</span></span><br><span class="line">    iStart = seconds();</span><br><span class="line">    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);</span><br><span class="line">    iElaps = seconds() - iStart;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;sumMatrixOnHost elapsed %f sec\n&quot;</span>, iElaps);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// malloc device global memory</span></span><br><span class="line">    <span class="type">float</span> *d_MatA, *d_MatB, *d_MatC;</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;d_MatA, nBytes));</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;d_MatB, nBytes));</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;d_MatC, nBytes));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// transfer data from host to device</span></span><br><span class="line">    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));</span><br><span class="line">    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// invoke kernel at host side</span></span><br><span class="line">    <span class="type">int</span> dimx = <span class="number">32</span>;</span><br><span class="line">    dim3 <span class="title function_">block</span><span class="params">(dimx, <span class="number">1</span>)</span>;</span><br><span class="line">    dim3 <span class="title function_">grid</span><span class="params">((nx + block.x - <span class="number">1</span>) / block.x, ny)</span>;</span><br><span class="line"></span><br><span class="line">    iStart = seconds();</span><br><span class="line">    sumMatrixOnGPUMix&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_MatA, d_MatB, d_MatC, nx, ny);</span><br><span class="line">    CHECK(cudaDeviceSynchronize());</span><br><span class="line">    iElaps = seconds() - iStart;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;sumMatrixOnGPU2D &lt;&lt;&lt;(%d,%d), (%d,%d)&gt;&gt;&gt; elapsed %f sec\n&quot;</span>, grid.x,</span><br><span class="line">           grid.y,</span><br><span class="line">           block.x, block.y, iElaps);</span><br><span class="line">    <span class="comment">// check kernel error</span></span><br><span class="line">    CHECK(cudaGetLastError());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// copy kernel result back to host side</span></span><br><span class="line">    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// check device results</span></span><br><span class="line">    checkResult(hostRef, gpuRef, nxy);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// free device global memory</span></span><br><span class="line">    CHECK(cudaFree(d_MatA));</span><br><span class="line">    CHECK(cudaFree(d_MatB));</span><br><span class="line">    CHECK(cudaFree(d_MatC));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// free host memory</span></span><br><span class="line">    <span class="built_in">free</span>(h_A);</span><br><span class="line">    <span class="built_in">free</span>(h_B);</span><br><span class="line">    <span class="built_in">free</span>(hostRef);</span><br><span class="line">    <span class="built_in">free</span>(gpuRef);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reset device</span></span><br><span class="line">    CHECK(cudaDeviceReset());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>运行结果如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> ./sumMatrixOnGPU-2D-grid-1D-block</span><br><span class="line">./sumMatrixOnGPU-2D-grid-1D-block Starting...</span><br><span class="line">Using Device 0: Quadro P2000</span><br><span class="line">Matrix size: nx 16384 ny 16384</span><br><span class="line">Matrix initialization elapsed 8.361603 sec</span><br><span class="line">sumMatrixOnHost elapsed 0.249986 sec</span><br><span class="line">sumMatrixOnGPU2D &lt;&lt;&lt;(512,16384), (32,1)&gt;&gt;&gt; elapsed 0.044010 sec</span><br><span class="line">Arrays match.</span><br></pre></td></tr></table></figure>
<p>对比上面两节的结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">zmurder@zmurder:~/WorkSpace/zyd/note/cuda/CUDA C编程权威指南/CUDAC编程权威指南练习code/chapter02$ ./sumMatrixOnGPU-2D-grid-2D-block</span><br><span class="line">./sumMatrixOnGPU-2D-grid-2D-block Starting...</span><br><span class="line">Using Device 0: Quadro P2000</span><br><span class="line">Matrix size: nx 16384 ny 16384</span><br><span class="line">Matrix initialization elapsed 8.377004 sec</span><br><span class="line">sumMatrixOnHost elapsed 0.256111 sec</span><br><span class="line">sumMatrixOnGPU2D &lt;&lt;&lt;(512,1024), (32,16)&gt;&gt;&gt; elapsed 0.026595 sec</span><br><span class="line">Arrays match.</span><br><span class="line"></span><br><span class="line">zmurder@zmurder:~/WorkSpace/zyd/note/cuda/CUDA C编程权威指南/CUDAC编程权威指南练习code/chapter02$ ./sumMatrixOnGPU-1D-grid-1D-block</span><br><span class="line">./sumMatrixOnGPU-1D-grid-1D-block Starting...</span><br><span class="line">Using Device 0: Quadro P2000</span><br><span class="line">Matrix size: nx 16384 ny 16384</span><br><span class="line">initialize matrix elapsed 8.340747 sec</span><br><span class="line">sumMatrixOnHost elapsed 0.249155 sec</span><br><span class="line">sumMatrixOnGPU1D &lt;&lt;&lt;(512,1), (32,1)&gt;&gt;&gt; elapsed 0.028243 sec</span><br><span class="line">Arrays match.</span><br><span class="line"></span><br><span class="line">zmurder@zmurder:~/WorkSpace/zyd/note/cuda/CUDA C编程权威指南/CUDAC编程权威指南练习code/chapter02$ ./sumMatrixOnGPU-2D-grid-1D-block</span><br><span class="line">./sumMatrixOnGPU-2D-grid-1D-block Starting...</span><br><span class="line">Using Device 0: Quadro P2000</span><br><span class="line">Matrix size: nx 16384 ny 16384</span><br><span class="line">Matrix initialization elapsed 8.361603 sec</span><br><span class="line">sumMatrixOnHost elapsed 0.249986 sec</span><br><span class="line">sumMatrixOnGPU2D &lt;&lt;&lt;(512,16384), (32,1)&gt;&gt;&gt; elapsed 0.044010 sec</span><br><span class="line">Arrays match.</span><br><span class="line"></span><br><span class="line">zmurder@zmurder:~/WorkSpace/zyd/note/cuda/CUDA C编程权威指南/CUDAC编程权威指南练习code/chapter02$ </span><br></pre></td></tr></table></figure>
<p>从矩阵加法的例子中可以看出：</p>
<ul>
<li>改变执行配置对内核性能有影响</li>
<li>传统的核函数实现一般不能获得最佳性能</li>
<li>对于一个给定的核函数，尝试使用不同的网格和线程块大小可以获得更好的性能</li>
</ul>
<p>在第3章，将会从硬件的角度学习产生这些问题的原因。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>奔跑的IC
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://example.com/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-3%20%E7%BB%84%E7%BB%87%E5%B9%B6%E8%A1%8C%E7%BA%BF%E7%A8%8B/" title="2-3 组织并行线程">http://example.com/CUDA/CUDA C编程权威指南笔记/2-3 组织并行线程/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C</a>
              <a href="/tags/GPU/" rel="tag"><i class="fa fa-tag"></i> GPU</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/2-2%20%E7%BB%99%E6%A0%B8%E5%87%BD%E6%95%B0%E8%AE%A1%E6%97%B6/" rel="prev" title="2-2 给核函数计时">
                  <i class="fa fa-chevron-left"></i> 2-2 给核函数计时
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/CUDA/CUDA%20C%E7%BC%96%E7%A8%8B%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0/3-2%20%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%89%A7%E8%A1%8C%E7%9A%84%E6%9C%AC%E8%B4%A8/" rel="next" title="3-2 理解线程束执行的本质">
                  3-2 理解线程束执行的本质 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">奔跑的IC</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"zmurder","repo":"zmurder.github.io","client_id":"cf2343f27b6c29efe0bc","client_secret":"3268a1fa92706c7358d5421f88f76a0f7ada3188","admin_user":"zmurder","distraction_free_mode":true,"proxy":"https://strong-caramel-969805.netlify.app/github_access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"d7c4f8343d9901a5878e0d9e3c0dc0b5"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
