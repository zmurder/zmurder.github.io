<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
  <meta name="msvalidate.01" content="7EC20DBC74B004C2782077570E15C280">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
    <meta name="description" content="1 前言根据官方文档的描述，解释一下什么是cudaGraph，其次为什么要使用cudaGraph。">
<meta property="og:type" content="article">
<meta property="og:title" content="cudaGraph">
<meta property="og:url" content="http://example.com/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/index.html">
<meta property="og:site_name" content="奔跑的IC">
<meta property="og:description" content="1 前言根据官方文档的描述，解释一下什么是cudaGraph，其次为什么要使用cudaGraph。">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/child-graph.png">
<meta property="og:image" content="http://example.com/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/cuda-graphs-dynamic-parameters-1.png">
<meta property="og:image" content="http://example.com/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/create-a-graph.png">
<meta property="og:image" content="http://example.com/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/image-20240118174011390.png">
<meta property="og:image" content="http://example.com/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/image-20240118174216159.png">
<meta property="og:image" content="http://example.com/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/image-20240118174547014.png">
<meta property="article:published_time" content="2024-12-01T10:13:44.387Z">
<meta property="article:modified_time" content="2024-12-01T10:13:44.387Z">
<meta property="article:author" content="奔跑的IC">
<meta property="article:tag" content="C">
<meta property="article:tag" content="GPU">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/child-graph.png">


<link rel="canonical" href="http://example.com/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://example.com/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/","path":"CUDA/基础/cudaGraph/","title":"cudaGraph"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>cudaGraph | 奔跑的IC</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">奔跑的IC</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-reorder fa-fw"></i>文章列表</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E5%89%8D%E8%A8%80"><span class="nav-text">1 前言</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E4%BB%80%E4%B9%88%E6%98%AFcudaGraph"><span class="nav-text">1.1 什么是cudaGraph</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8Graph"><span class="nav-text">1.2 为什么要使用Graph</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E5%88%9B%E5%BB%BA%E5%B9%B6%E4%BD%BF%E7%94%A8Graph"><span class="nav-text">2 创建并使用Graph</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E4%BD%BF%E7%94%A8API%E5%88%9B%E5%BB%BAGraph"><span class="nav-text">2.1 使用API创建Graph</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E4%BD%BF%E7%94%A8Stream%E6%8D%95%E8%8E%B7%E5%88%9B%E5%BB%BAGraph"><span class="nav-text">2.2 使用Stream捕获创建Graph</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-%E6%B5%8B%E8%AF%95%E4%BE%8B%E5%AD%90"><span class="nav-text">2.3 测试例子</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95%EF%BC%9A"><span class="nav-text">附录：</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="奔跑的IC"
      src="/images/zyd.gif">
  <p class="site-author-name" itemprop="name">奔跑的IC</p>
  <div class="site-description" itemprop="description">死磕牛角的IT农民工</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">153</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zyd.gif">
      <meta itemprop="name" content="奔跑的IC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="奔跑的IC">
      <meta itemprop="description" content="死磕牛角的IT农民工">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="cudaGraph | 奔跑的IC">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          cudaGraph
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-01 18:13:44" itemprop="dateCreated datePublished" datetime="2024-12-01T18:13:44+08:00">2024-12-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CUDA/" itemprop="url" rel="index"><span itemprop="name">CUDA</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CUDA/%E5%9F%BA%E7%A1%80/" itemprop="url" rel="index"><span itemprop="name">基础</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h1><p>根据官方文档的描述，解释一下什么是cudaGraph，其次为什么要使用cudaGraph。</p>
<h2 id="1-1-什么是cudaGraph"><a href="#1-1-什么是cudaGraph" class="headerlink" title="1.1 什么是cudaGraph"></a>1.1 什么是cudaGraph</h2><p>官方的描述如下</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA Graphs present a new model for work submission in CUDA. A graph is a series of operations, such as kernel launches, connected by dependencies, which is defined separately from its execution. This allows a graph to be defined once and then launched repeatedly.</span><br></pre></td></tr></table></figure>
<p>翻译过来就是</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA 图形为 CUDA 中的工作提交提供了一种新模式。图形是一系列操作（如内核启动），由依赖关系连接，与执行分开定义。这使得图形可以一次性定义，然后重复启动</span><br></pre></td></tr></table></figure>
<p>自己的理解：</p>
<ul>
<li><p>没有Graph：CPU端每次调用一个kernel 的Launch，多个Kernel就多次Launch。</p>
</li>
<li><p>将多个kernel录制为一个Graph，一次Launch Graph操作就可以支持多个kernel。然后在这一个图中进行运算。如下图，Y就是一个Graph，其中包含了多个kernel A、kernel B、kernel C、kernelD。但是我们之需要执行Y就相当与执行了ABCD。</p>
</li>
</ul>
<img src="/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/child-graph.png" class="" title="Child Graph Example">
<h2 id="1-2-为什么要使用Graph"><a href="#1-2-为什么要使用Graph" class="headerlink" title="1.2 为什么要使用Graph"></a>1.2 为什么要使用Graph</h2><p>先通俗的理解一下，在我们的实际工程中在一个Stream中会调用多个cuda的kernel，按照Lanuch进入Stream的顺序在一个Stream中执行，每一次在CPU上调用kernel都会有一次Lanuch的操作，如果我们的kernel很多并且Lanuch的时间比较长，实际执行kernel的时间比较短，那么每一次Kernel的Lanuch操作的时间就不可忽略成为影响我们程序性能的重要因素了，描述的可能不直观，来一张官方介绍的图就明白了</p>
<img src="/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/cuda-graphs-dynamic-parameters-1.png" class="" title="img">
<p>要实现上图的Graph加速实际上需要一些前提，我下面的测试实际上并没有体现Graph的加速。这些前提就是下面的官方描述</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">eparating out the definition of a graph from its execution enables a number of optimizations: first, CPU launch costs are reduced compared to streams, because much of the setup is done in advance; second, presenting the whole workflow to CUDA enables optimizations which might not be possible with the piecewise work submission mechanism of streams.</span><br><span class="line"></span><br><span class="line">To see the optimizations possible with graphs, consider what happens in a stream: when you place a kernel into a stream, the host driver performs a sequence of operations in preparation for the execution of the kernel on the GPU. These operations, necessary for setting up and launching the kernel, are an overhead cost which must be paid for each kernel that is issued. For a GPU kernel with a short execution time, this overhead cost can be a significant fraction of the overall end-to-end execution time.</span><br></pre></td></tr></table></figure>
<p>翻译过来就是</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">将图形的定义与执行分离开来可以实现一系列优化：首先，与流相比，CPU 的启动成本降低了，因为大部分设置都是提前完成的；其次，将整个工作流程呈现给 CUDA 可以实现优化，而流的分片工作提交机制可能无法实现这些优化。</span><br><span class="line"></span><br><span class="line">要了解图形可能带来的优化，请考虑一下在流中发生的情况：当您将内核放入流中时，主机驱动程序会执行一系列操作，为在 GPU 上执行内核做准备。这些操作是设置和启动内核所必需的，也是每发布一个内核必须支付的开销。对于执行时间较短的 GPU 内核来说，这种开销成本可能占整个端到端执行时间的很大一部分。</span><br></pre></td></tr></table></figure>
<p>我总结的使用Graph的有点如下：</p>
<ul>
<li><strong>也就是单独Lanuch没一个kernel的操作时间比较长，kernel的执行执行时间短。如果构建为一个Graph之后多个kernel的Launch就可以合并为一次Launch进而实现优化</strong></li>
<li><strong>因为kernel的Launch少了，因此CPU的负载也会降低</strong></li>
<li>其他的cuda内部优化，不得而知。</li>
</ul>
<h1 id="2-创建并使用Graph"><a href="#2-创建并使用Graph" class="headerlink" title="2 创建并使用Graph"></a>2 创建并使用Graph</h1><p>Graph可以通过两种机制创建：显式API和流捕获。以下是创建和执行下面图表的示例。</p>
<img src="/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/create-a-graph.png" class="" title="Creating a Graph Using Graph APIs Example">
<h2 id="2-1-使用API创建Graph"><a href="#2-1-使用API创建Graph" class="headerlink" title="2.1 使用API创建Graph"></a>2.1 使用API创建Graph</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Create the graph - it starts out empty</span></span><br><span class="line">cudaGraphCreate(&amp;graph, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// For the purpose of this example, we&#x27;ll create</span></span><br><span class="line"><span class="comment">// the nodes separately from the dependencies to</span></span><br><span class="line"><span class="comment">// demonstrate that it can be done in two stages.</span></span><br><span class="line"><span class="comment">// Note that dependencies can also be specified</span></span><br><span class="line"><span class="comment">// at node creation.</span></span><br><span class="line">cudaGraphAddKernelNode(&amp;a, graph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;nodeParams);</span><br><span class="line">cudaGraphAddKernelNode(&amp;b, graph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;nodeParams);</span><br><span class="line">cudaGraphAddKernelNode(&amp;c, graph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;nodeParams);</span><br><span class="line">cudaGraphAddKernelNode(&amp;d, graph, <span class="literal">NULL</span>, <span class="number">0</span>, &amp;nodeParams);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Now set up dependencies on each node</span></span><br><span class="line">cudaGraphAddDependencies(graph, &amp;a, &amp;b, <span class="number">1</span>);     <span class="comment">// A-&gt;B</span></span><br><span class="line">cudaGraphAddDependencies(graph, &amp;a, &amp;c, <span class="number">1</span>);     <span class="comment">// A-&gt;C</span></span><br><span class="line">cudaGraphAddDependencies(graph, &amp;b, &amp;d, <span class="number">1</span>);     <span class="comment">// B-&gt;D</span></span><br><span class="line">cudaGraphAddDependencies(graph, &amp;c, &amp;d, <span class="number">1</span>);     <span class="comment">// C-&gt;D</span></span><br></pre></td></tr></table></figure>
<h2 id="2-2-使用Stream捕获创建Graph"><a href="#2-2-使用Stream捕获创建Graph" class="headerlink" title="2.2 使用Stream捕获创建Graph"></a>2.2 使用Stream捕获创建Graph</h2><p>流捕获提供了一种机制，可从现有的基于流的应用程序接口创建图形。在调用 <code>cudaStreamBeginCapture()</code> 和 <code>cudaStreamEndCapture()</code>时，可括弧括住向流（包括现有代码）启动工作的代码部分</p>
<p>具体参考<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#creating-a-graph-using-stream-capture">3.2.8.7.3. Creating a Graph Using Stream Capture</a></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cudaGraph_t graph;</span><br><span class="line"></span><br><span class="line">cudaStreamBeginCapture(stream);</span><br><span class="line"></span><br><span class="line">kernel_A&lt;&lt;&lt; ..., stream &gt;&gt;&gt;(...);</span><br><span class="line">kernel_B&lt;&lt;&lt; ..., stream &gt;&gt;&gt;(...);</span><br><span class="line">libraryCall(stream);</span><br><span class="line">kernel_C&lt;&lt;&lt; ..., stream &gt;&gt;&gt;(...);</span><br><span class="line"></span><br><span class="line">cudaStreamEndCapture(stream, &amp;graph);</span><br></pre></td></tr></table></figure>
<h2 id="2-3-测试例子"><a href="#2-3-测试例子" class="headerlink" title="2.3 测试例子"></a>2.3 测试例子</h2><p>因为我的测试kernel都比较简单，所以kernel的Launch都比较短，看不出使用Graph的优势，但是还是可以作为一个使用的例子来参考一下的。</p>
<p>测试代码如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;../common/common.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * This example implements matrix element-wise addition on the host and GPU.</span></span><br><span class="line"><span class="comment"> * sumMatrixOnHost iterates over the rows and columns of each matrix, adding</span></span><br><span class="line"><span class="comment"> * elements from A and B together and storing the results in C. The current</span></span><br><span class="line"><span class="comment"> * offset in each matrix is stored using pointer arithmetic. sumMatrixOnGPU2D</span></span><br><span class="line"><span class="comment"> * implements the same logic, but using CUDA threads to process each matrix.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NSTEP 5</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> NKERNEL 40</span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">initialData</span><span class="params">(<span class="type">float</span> *ip, <span class="type">const</span> <span class="type">int</span> size)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> i;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; size; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        ip[i] = (<span class="type">float</span>)(rand() &amp; <span class="number">0xFF</span>) / <span class="number">10.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">sumMatrixOnHost</span><span class="params">(<span class="type">float</span> *A, <span class="type">float</span> *B, <span class="type">float</span> *C, <span class="type">const</span> <span class="type">int</span> nx, <span class="type">const</span> <span class="type">int</span> ny)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> *ia = A;</span><br><span class="line">    <span class="type">float</span> *ib = B;</span><br><span class="line">    <span class="type">float</span> *ic = C;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> iy = <span class="number">0</span>; iy &lt; ny; iy++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ix = <span class="number">0</span>; ix &lt; nx; ix++)</span><br><span class="line">        &#123;</span><br><span class="line">            ic[ix] = ia[ix] + ib[ix];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ia += nx;</span><br><span class="line">        ib += nx;</span><br><span class="line">        ic += nx;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">checkResult</span><span class="params">(<span class="type">float</span> *hostRef, <span class="type">float</span> *gpuRef, <span class="type">const</span> <span class="type">int</span> N)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">double</span> epsilon = <span class="number">1.0E-8</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">abs</span>(hostRef[i] - gpuRef[i]) &gt; epsilon)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;host %f gpu %f &quot;</span>, hostRef[i], gpuRef[i]);</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Arrays do not match.\n\n&quot;</span>);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// grid 2D block 2D</span></span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">sumMatrixOnGPU2D</span><span class="params">(<span class="type">float</span> *A, <span class="type">float</span> *B, <span class="type">float</span> *C, <span class="type">int</span> NX, <span class="type">int</span> NY)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> ix = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> iy = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx = iy * NX + ix;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (ix &lt; NX &amp;&amp; iy &lt; NY)</span><br><span class="line">    &#123;</span><br><span class="line">        C[idx] = A[idx] + B[idx];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> elapsed_time;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set up device</span></span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp deviceProp;</span><br><span class="line">    CHECK(cudaGetDeviceProperties(&amp;deviceProp, dev));</span><br><span class="line">    CHECK(cudaSetDevice(dev));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// set up data size of matrix</span></span><br><span class="line">    <span class="type">int</span> nx = <span class="number">1</span> &lt;&lt; <span class="number">5</span>;</span><br><span class="line">    <span class="type">int</span> ny = <span class="number">1</span> &lt;&lt; <span class="number">5</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nxy = nx * ny;</span><br><span class="line">    <span class="type">int</span> nBytes = nxy * <span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// malloc host memory</span></span><br><span class="line">    <span class="type">float</span> *h_A, *h_B, *hostRef, *gpuRef;</span><br><span class="line">    h_A = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    h_B = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    hostRef = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    gpuRef = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// initialize data at host side</span></span><br><span class="line">    <span class="type">double</span> iStart = seconds();</span><br><span class="line">    initialData(h_A, nxy);</span><br><span class="line">    initialData(h_B, nxy);</span><br><span class="line">    <span class="type">double</span> iElaps = seconds() - iStart;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">memset</span>(hostRef, <span class="number">0</span>, nBytes);</span><br><span class="line">    <span class="built_in">memset</span>(gpuRef, <span class="number">0</span>, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// add matrix at host side for result checks</span></span><br><span class="line">    <span class="comment">// iStart = seconds();</span></span><br><span class="line">    sumMatrixOnHost(h_A, h_B, hostRef, nx, ny);</span><br><span class="line">    <span class="comment">// iElaps = seconds() - iStart;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// malloc device global memory</span></span><br><span class="line">    <span class="type">float</span> *d_MatA, *d_MatB, *d_MatC;</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;d_MatA, nBytes));</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;d_MatB, nBytes));</span><br><span class="line">    CHECK(cudaMalloc((<span class="type">void</span> **)&amp;d_MatC, nBytes));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// transfer data from host to device</span></span><br><span class="line">    CHECK(cudaMemcpy(d_MatA, h_A, nBytes, cudaMemcpyHostToDevice));</span><br><span class="line">    CHECK(cudaMemcpy(d_MatB, h_B, nBytes, cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// invoke kernel at host side</span></span><br><span class="line">    <span class="type">int</span> dimx = <span class="number">32</span>;</span><br><span class="line">    <span class="type">int</span> dimy = <span class="number">32</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (argc &gt; <span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        dimx = atoi(argv[<span class="number">1</span>]);</span><br><span class="line">        dimy = atoi(argv[<span class="number">2</span>]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    dim3 <span class="title function_">block</span><span class="params">(dimx, dimy)</span>;</span><br><span class="line">    dim3 <span class="title function_">grid</span><span class="params">((nx + block.x - <span class="number">1</span>) / block.x, (ny + block.y - <span class="number">1</span>) / block.y)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// execute the kernel</span></span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    CHECK(cudaStreamCreate(&amp;stream));</span><br><span class="line">    <span class="comment">// creat events</span></span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    CHECK(cudaEventCreate(&amp;start));</span><br><span class="line">    CHECK(cudaEventCreate(&amp;stop));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// record start event</span></span><br><span class="line">    CHECK(cudaEventRecord(start, <span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> istep = <span class="number">0</span>; istep &lt; NSTEP; istep++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ikrnl = <span class="number">0</span>; ikrnl &lt; NKERNEL; ikrnl++)</span><br><span class="line">        &#123;</span><br><span class="line">            sumMatrixOnGPU2D&lt;&lt;&lt;grid, block, <span class="number">0</span>, stream&gt;&gt;&gt;(d_MatA, d_MatB, d_MatC, nx, ny);</span><br><span class="line">        &#125;</span><br><span class="line">        CHECK(cudaStreamSynchronize(stream));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    CHECK(cudaEventRecord(stop, <span class="number">0</span>));</span><br><span class="line">    CHECK(cudaEventSynchronize(stop));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// calculate elapsed time</span></span><br><span class="line">    CHECK(cudaEventElapsedTime(&amp;elapsed_time, start, stop));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Measured time for Graph execution = %fs\n&quot;</span>,</span><br><span class="line">           elapsed_time / <span class="number">1000.0f</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// cudaGraph</span></span><br><span class="line">    <span class="type">bool</span> graphCreated = <span class="literal">false</span>;</span><br><span class="line">    cudaGraph_t graph;</span><br><span class="line">    cudaGraphExec_t instance;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// record start event</span></span><br><span class="line">    CHECK(cudaEventRecord(start, <span class="number">0</span>));</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> istep = <span class="number">0</span>; istep &lt; NSTEP; istep++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (!graphCreated)</span><br><span class="line">        &#123;</span><br><span class="line">            cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> ikrnl = <span class="number">0</span>; ikrnl &lt; NKERNEL; ikrnl++)</span><br><span class="line">            &#123;</span><br><span class="line">                sumMatrixOnGPU2D&lt;&lt;&lt;grid, block, <span class="number">0</span>, stream&gt;&gt;&gt;(d_MatA, d_MatB, d_MatC, nx, ny);</span><br><span class="line">            &#125;</span><br><span class="line">            cudaStreamEndCapture(stream, &amp;graph);</span><br><span class="line">            cudaGraphInstantiate(&amp;instance, graph, <span class="literal">NULL</span>, <span class="literal">NULL</span>, <span class="number">0</span>);</span><br><span class="line">            graphCreated = <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        cudaGraphLaunch(instance, stream);</span><br><span class="line">        CHECK(cudaStreamSynchronize(stream));</span><br><span class="line">    &#125;</span><br><span class="line">    CHECK(cudaEventRecord(stop, <span class="number">0</span>));</span><br><span class="line">    CHECK(cudaEventSynchronize(stop));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// calculate elapsed time</span></span><br><span class="line">    CHECK(cudaEventElapsedTime(&amp;elapsed_time, start, stop));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Measured time for Graph execution = %fs\n&quot;</span>,</span><br><span class="line">           elapsed_time / <span class="number">1000.0f</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// copy kernel result back to host side</span></span><br><span class="line">    CHECK(cudaMemcpy(gpuRef, d_MatC, nBytes, cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// check device results</span></span><br><span class="line">    checkResult(hostRef, gpuRef, nxy);</span><br><span class="line"></span><br><span class="line">    CHECK(cudaStreamDestroy(stream));</span><br><span class="line">    <span class="comment">// free device global memory</span></span><br><span class="line">    CHECK(cudaFree(d_MatA));</span><br><span class="line">    CHECK(cudaFree(d_MatB));</span><br><span class="line">    CHECK(cudaFree(d_MatC));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// free host memory</span></span><br><span class="line">    <span class="built_in">free</span>(h_A);</span><br><span class="line">    <span class="built_in">free</span>(h_B);</span><br><span class="line">    <span class="built_in">free</span>(hostRef);</span><br><span class="line">    <span class="built_in">free</span>(gpuRef);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reset device</span></span><br><span class="line">    CHECK(cudaDeviceReset());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> EXIT_SUCCESS;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>测试结果如下（一次运行40个kernel，运行5次），前面的蓝色是没有使用Graph的情况，后面的橙色是使用Graph的情况（一个Graph内是40个kernel）</p>
<img src="/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/image-20240118174011390.png" class="" title="image-20240118174011390">
<p>先看一下没有使用Graph的时候，如下图是单独一次的运行（总共运行5次，这是其中的一次），可以看出有40次kernel的Launch。</p>
<img src="/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/image-20240118174216159.png" class="" title="image-20240118174216159">
<p>再看一下使用Graph的情况，如下图是单独一次的运行（总共运行5次，这是其中的一次），可以看出之调用一次GraphLaunch。（总共调用5次）</p>
<img src="/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/image-20240118174547014.png" class="" title="image-20240118174547014">
<p>我这里使用Graph并没有减少总的时间，主要是因为我测的是kernel比较小，kernel的Launch比较快。实际的工程中Kernel可能比较慢，</p>
<p>kernel的launch会比较大，主要有以下几种情况：</p>
<ul>
<li><strong>kernel的大小比较大</strong>：kernel的大小越大，launch的开销就越大。</li>
<li><strong>kernel的参数比较多</strong>：kernel的参数越多，launch的开销就越大。</li>
<li><strong>kernel的调用频率比较高</strong>：kernel的调用频率越高，launch的开销就越大。</li>
</ul>
<h1 id="附录："><a href="#附录：" class="headerlink" title="附录："></a>附录：</h1><ul>
<li>官方文档：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-graphs">3.2.8.7. CUDA Graphs</a></li>
<li>官方博客：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cuda-graphs/">Getting Started with CUDA Graphs</a></li>
<li>官方博客：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/constructing-cuda-graphs-with-dynamic-parameters/">Constructing CUDA Graphs with Dynamic Parameters</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>奔跑的IC
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://example.com/CUDA/%E5%9F%BA%E7%A1%80/cudaGraph/" title="cudaGraph">http://example.com/CUDA/基础/cudaGraph/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C</a>
              <a href="/tags/GPU/" rel="tag"><i class="fa fa-tag"></i> GPU</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/CUDA/%E5%9F%BA%E7%A1%80/1%20cuda_API/" rel="prev" title="1 cuda_API">
                  <i class="fa fa-chevron-left"></i> 1 cuda_API
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/CUDA/%E5%9F%BA%E7%A1%80/cuda-gdb/" rel="next" title="cuda-gdb">
                  cuda-gdb <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">奔跑的IC</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"zmurder","repo":"zmurder.github.io","client_id":"cf2343f27b6c29efe0bc","client_secret":"3268a1fa92706c7358d5421f88f76a0f7ada3188","admin_user":"zmurder","distraction_free_mode":true,"proxy":"https://strong-caramel-969805.netlify.app/github_access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"f8dca60b0d45b41fe0c93cda3fe774e2"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
