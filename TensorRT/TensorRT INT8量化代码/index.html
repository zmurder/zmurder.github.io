<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
  <meta name="msvalidate.01" content="7EC20DBC74B004C2782077570E15C280">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
    <meta name="description" content="1 简介在之前的文章中7-TensorRT中的INT8介绍了TensorRT的量化理论基础，这里就根据理论实现相关的代码">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorRT INT8量化代码">
<meta property="og:url" content="http://example.com/TensorRT/TensorRT%20INT8%E9%87%8F%E5%8C%96%E4%BB%A3%E7%A0%81/index.html">
<meta property="og:site_name" content="奔跑的IC">
<meta property="og:description" content="1 简介在之前的文章中7-TensorRT中的INT8介绍了TensorRT的量化理论基础，这里就根据理论实现相关的代码">
<meta property="og:locale">
<meta property="article:published_time" content="2024-12-01T10:13:45.315Z">
<meta property="article:modified_time" content="2024-12-01T10:13:45.315Z">
<meta property="article:author" content="奔跑的IC">
<meta property="article:tag" content="git">
<meta property="article:tag" content="C">
<meta property="article:tag" content="C++">
<meta property="article:tag" content="GPU">
<meta property="article:tag" content="Tensorrt">
<meta property="article:tag" content="DeepLearning">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/TensorRT/TensorRT%20INT8%E9%87%8F%E5%8C%96%E4%BB%A3%E7%A0%81/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://example.com/TensorRT/TensorRT%20INT8%E9%87%8F%E5%8C%96%E4%BB%A3%E7%A0%81/","path":"TensorRT/TensorRT INT8量化代码/","title":"TensorRT INT8量化代码"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>TensorRT INT8量化代码 | 奔跑的IC</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">奔跑的IC</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-reorder fa-fw"></i>文章列表</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E7%AE%80%E4%BB%8B"><span class="nav-text">1 简介</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-PTQ"><span class="nav-text">2 PTQ</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-trtexec"><span class="nav-text">2.1 trtexec</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-python%E7%89%88%E6%9C%AC%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90calibratorfile%E5%92%8Cint8engine"><span class="nav-text">2.1.1 python版本代码生成calibratorfile和int8engine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-c-%E4%BB%A3%E7%A0%81%E7%89%88%E6%9C%AC%E7%94%9F%E6%88%90calibratorfile%E5%92%8Cint8engine"><span class="nav-text">2.1.2 c++代码版本生成calibratorfile和int8engine</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-1-%E7%AE%80%E5%8D%95%E7%89%88%E6%9C%AC-%E5%8F%AA%E6%9C%89IInt8EntropyCalibrator2"><span class="nav-text">2.1.2.1 简单版本 只有IInt8EntropyCalibrator2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-2-2-%E5%AE%8C%E7%9B%B8%E5%AF%B9%E5%AE%8C%E5%96%84%E7%9A%84%E9%87%8F%E5%8C%96%E5%99%A8%E6%96%B9%E6%A1%88"><span class="nav-text">2.1.2.2 完相对完善的量化器方案</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-python-onnx%E8%BD%ACtrt"><span class="nav-text">2.2 python onnx转trt</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-polygraphy%E5%B7%A5%E5%85%B7"><span class="nav-text">2.3 polygraphy工具</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-pytorch%E4%B8%AD%E6%89%A7%E8%A1%8C-%E6%8E%A8%E8%8D%90"><span class="nav-text">2.4 pytorch中执行(推荐)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-QAT"><span class="nav-text">3 QAT</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95%EF%BC%9A"><span class="nav-text">附录：</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="奔跑的IC"
      src="/images/zyd.gif">
  <p class="site-author-name" itemprop="name">奔跑的IC</p>
  <div class="site-description" itemprop="description">死磕牛角的IT农民工</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">176</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/TensorRT/TensorRT%20INT8%E9%87%8F%E5%8C%96%E4%BB%A3%E7%A0%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zyd.gif">
      <meta itemprop="name" content="奔跑的IC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="奔跑的IC">
      <meta itemprop="description" content="死磕牛角的IT农民工">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="TensorRT INT8量化代码 | 奔跑的IC">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          TensorRT INT8量化代码
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-01 18:13:45" itemprop="dateCreated datePublished" datetime="2024-12-01T18:13:45+08:00">2024-12-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TensorRT/" itemprop="url" rel="index"><span itemprop="name">TensorRT</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p>在之前的文章中<code>7-TensorRT中的INT8</code>介绍了TensorRT的量化理论基础，这里就根据理论实现相关的代码</p>
<h1 id="2-PTQ"><a href="#2-PTQ" class="headerlink" title="2 PTQ"></a>2 PTQ</h1><h2 id="2-1-trtexec"><a href="#2-1-trtexec" class="headerlink" title="2.1 trtexec"></a>2.1 trtexec</h2><p> int8量化 使用<code>trtexec</code> 参数<code>--int8</code>来生成对应的<code>--int8</code>的<code>engine</code>，但是精度损失会比较大。也可使用int8和fp16混合精度同时使用<code>--fp16 --int8</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trtexec --onnx=XX.onnx --saveEngine=model.plan --int8</span><br><span class="line">trtexec --onnx=XX.onnx --saveEngine=model.plan --int8 --fp16</span><br></pre></td></tr></table></figure>
<p>上面的方式没有使用量化文件进行校正，因此精度损失非常多。可以使用<code>trtexec</code>添加<code>--calib=</code>参数指定<code>calibration file</code>文件进行校准。如下面的指令</p>
<p><strong>这个是我目前采用的方法</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trtexec --onnx=test.onnx --verbose --dumpLayerInfo --dumpProfile  --int8 --calib=calibratorfile_test.txt --saveEngine=test.onnx.INT8.trtmodel --exportProfile=build.log</span><br></pre></td></tr></table></figure>
<p>上面指定的<code>--calib=calibratorfile_test.txt</code>是需要我们自己生成的，使用我们自己的数据集，在build engine阶段，tensorrt会根据数据分布和我们指定的校准器来生成的。如在<code>7-TensorRT中的INT8</code>中介绍的熵校准<code>Entropy calibration</code>使用的校准器就是<code>IInt8EntropyCalibrator2</code>。</p>
<p>将ONNX转换为INT8的TensorRT引擎，需要:</p>
<ol>
<li>准备一个校准集，用于在转换过程中寻找使得转换后的激活值分布与原来的FP32类型的激活值分布差异最小的阈值;</li>
<li>并写一个校准器类，该类需继承trt.IInt8EntropyCalibrator2父类，并重写get_batch_size,   get_batch, read_calibration_cache, write_calibration_cache这几个方法。可直接使用<code>myCalibrator.py</code>，需传入图片文件夹地址</li>
</ol>
<p>下面是具体的实现代码</p>
<h3 id="2-1-1-python版本代码生成calibratorfile和int8engine"><a href="#2-1-1-python版本代码生成calibratorfile和int8engine" class="headerlink" title="2.1.1 python版本代码生成calibratorfile和int8engine"></a><strong>2.1.1 python版本代码生成calibratorfile和int8engine</strong></h3><p>参考：<a target="_blank" rel="noopener" href="https://github.com/aiLiwensong/Pytorch2TensorRT">https://github.com/aiLiwensong/Pytorch2TensorRT</a></p>
<p>官方代码在<code>/samples/python/int8_caffe_mnist</code></p>
<p>下面回答来自GPT</p>
<p>在 TensorRT 中，<code>trt.Builder</code> 是用于构建 TensorRT 引擎的主要类。当你使用 <code>trt.Builder</code> 来构建一个 TensorRT 引擎时，通过调用 <code>builder.build_engine</code> 函数来生成引擎。在执行这个函数时，如果你使用了 int8 量化，TensorRT 将会在量化过程中使用校准数据，并在量化过程中生成校准数据文件。</p>
<p>具体而言，当你调用 <code>builder.build_engine</code> 函数时，TensorRT 会根据设置执行 int8 量化过程，包括使用指定的校准方法和数据集来生成校准数据。在量化过程中，TensorRT 将会不断地调用 <code>IInt8Calibrator::getBatch</code> 函数来获取数据样本，并根据这些样本来执行量化。在执行量化过程时，TensorRT 会在合适的时机将校准数据写入到校准数据文件中。</p>
<p><code>main.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># main.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> trt_convertor <span class="keyword">import</span> ONNX2TRT</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">&quot;Pytorch2TensorRT&quot;</span>)</span><br><span class="line">    <span class="comment"># parser.add_argument(&quot;--dynamic&quot;, action=&#x27;store_true&#x27;, help=&#x27;batch_size&#x27;)  # not ok yet.</span></span><br><span class="line">    parser.add_argument(<span class="string">&quot;--batch_size&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>, <span class="built_in">help</span>=<span class="string">&#x27;batch_size&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--channel&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">3</span>, <span class="built_in">help</span>=<span class="string">&#x27;input channel&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--height&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">384</span>, <span class="built_in">help</span>=<span class="string">&#x27;input height&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--width&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">640</span>, <span class="built_in">help</span>=<span class="string">&#x27;input width&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--cache_file&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;cache_file&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--mode&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;int8&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;fp32, fp16 or int8&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--onnx_file_path&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;model.onnx&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;onnx_file_path&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--engine_file_path&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;model.engine&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;engine_file_path&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--imgs_dir&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;path_to_images_dir&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;calibrator images dir&#x27;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    <span class="built_in">print</span>(args)</span><br><span class="line">    <span class="keyword">if</span> args.mode.lower() == <span class="string">&#x27;int8&#x27;</span>:</span><br><span class="line">        <span class="comment"># Note that: if use int8 mode, you should prepare a calibrate dataset and create a Calibrator class.</span></span><br><span class="line">        <span class="comment"># In Calibrator class, you should override &#x27;get_batch_size, get_batch&#x27;,</span></span><br><span class="line">        <span class="comment"># &#x27;read_calibration_cache&#x27;, &#x27;write_calibration_cache&#x27;.</span></span><br><span class="line">        <span class="comment"># You can reference implementation of MyEntropyCalibrator.</span></span><br><span class="line">        <span class="keyword">from</span> myCalibrator <span class="keyword">import</span> MyEntropyCalibrator</span><br><span class="line">        calib = MyEntropyCalibrator(tensor_shape=(args.batch_size, args.channel, args.height, args.width),</span><br><span class="line">                                    imgs_dir=args.imgs_dir)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        calib = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    ONNX2TRT(args, calib=calib)</span><br></pre></td></tr></table></figure>
<p>对应的<code>myCalibrator.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#myCalibrator.py</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Created on : 20200608</span></span><br><span class="line"><span class="string">@author: LWS</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Create custom calibrator, use to calibrate int8 TensorRT model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Need to override some methods of trt.IInt8EntropyCalibrator2, such as get_batch_size, get_batch,</span></span><br><span class="line"><span class="string">read_calibration_cache, write_calibration_cache.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br><span class="line"><span class="keyword">import</span> pycuda.driver <span class="keyword">as</span> cuda</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyEntropyCalibrator</span>(trt.IInt8EntropyCalibrator2):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tensor_shape, imgs_dir,</span></span><br><span class="line"><span class="params">                 mean=(<span class="params"><span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span></span>), std=(<span class="params"><span class="number">255.</span>, <span class="number">255.</span>, <span class="number">255.</span></span>)</span>):</span><br><span class="line">        trt.IInt8EntropyCalibrator2.__init__(self)</span><br><span class="line"></span><br><span class="line">        self.cache_file = <span class="string">&#x27;CALIBRATOR.cache&#x27;</span></span><br><span class="line">        self.mean = mean</span><br><span class="line">        self.std = std</span><br><span class="line"></span><br><span class="line">        self.batch_size, self.Channel, self.Height, self.Width = tensor_shape</span><br><span class="line"></span><br><span class="line">        self.imgs = glob.glob(os.path.join(imgs_dir, <span class="string">&quot;*&quot;</span>))</span><br><span class="line">        np.random.shuffle(self.imgs)</span><br><span class="line"></span><br><span class="line">        self.batch_idx = <span class="number">0</span></span><br><span class="line">        self.max_batch_idx = <span class="built_in">len</span>(self.imgs) // self.batch_size</span><br><span class="line">        self.data_size = trt.volume([self.batch_size, self.Channel, self.Height, self.Width]) * trt.float32.itemsize</span><br><span class="line">        self.device_input = cuda.mem_alloc(self.data_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">next_batch</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.batch_idx &lt; self.max_batch_idx:</span><br><span class="line">            batch_files = self.imgs[self.batch_idx * self.batch_size: \</span><br><span class="line">                                    (self.batch_idx + <span class="number">1</span>) * self.batch_size]</span><br><span class="line">            batch_imgs = np.zeros((self.batch_size, self.Channel, self.Height, self.Width),</span><br><span class="line">                                  dtype=np.float32)</span><br><span class="line">            <span class="keyword">for</span> i, f <span class="keyword">in</span> <span class="built_in">enumerate</span>(batch_files):</span><br><span class="line">                img = cv2.imread(f)</span><br><span class="line">                img = self.transform(img, (self.Width, self.Height))</span><br><span class="line">                <span class="keyword">assert</span> (img.nbytes == self.data_size / self.batch_size), <span class="string">&#x27;not valid img!&#x27;</span> + f</span><br><span class="line">                batch_imgs[i] = img</span><br><span class="line">            self.batch_idx += <span class="number">1</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;\rbatch:[&#123;&#125;/&#123;&#125;]&quot;</span>.<span class="built_in">format</span>(self.batch_idx, self.max_batch_idx), end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> np.ascontiguousarray(batch_imgs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> np.array([])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transform</span>(<span class="params">self, img, img_size</span>):</span><br><span class="line">        w, h = img_size</span><br><span class="line">        oh, ow = img.shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">        s = <span class="built_in">min</span>(w / ow, h / oh)</span><br><span class="line">        nw, nh = <span class="built_in">int</span>(<span class="built_in">round</span>(ow * s)), <span class="built_in">int</span>(<span class="built_in">round</span>(oh * s))</span><br><span class="line">        t, b, l, r = (h - nh) // <span class="number">2</span>, (h - nh + <span class="number">1</span>) // <span class="number">2</span>, (w - nw) // <span class="number">2</span>, (w - nw + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nw != ow <span class="keyword">or</span> nh != oh:</span><br><span class="line">            img = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_CUBIC)</span><br><span class="line">        img = cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=<span class="number">114</span>)</span><br><span class="line">        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)</span><br><span class="line">        img = (img - np.float32(self.mean)) * (<span class="number">1</span> / np.float32(self.std))</span><br><span class="line">        img = img.transpose(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).copy()</span><br><span class="line">        <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_batch_size</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.batch_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_batch</span>(<span class="params">self, names, p_str=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            batch_imgs = self.next_batch()</span><br><span class="line">            <span class="keyword">if</span> batch_imgs.size == <span class="number">0</span> <span class="keyword">or</span> batch_imgs.size != self.batch_size * self.Channel * self.Height * self.Width:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">            cuda.memcpy_htod(self.device_input, batch_imgs.astype(np.float32))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> [<span class="built_in">int</span>(self.device_input)]</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;get batch error: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(e))</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">read_calibration_cache</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># If there is a cache, use it instead of calibrating again. Otherwise, implicitly return None.</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(self.cache_file):</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(self.cache_file, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                <span class="keyword">return</span> f.read()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">write_calibration_cache</span>(<span class="params">self, cache</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(self.cache_file, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(cache)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    calib = MyEntropyCalibrator(<span class="number">1</span>, <span class="number">3</span>, <span class="number">384</span>, <span class="number">640</span>)</span><br><span class="line">    batch = calib.get_batch(<span class="literal">None</span>)</span><br><span class="line">    <span class="built_in">print</span>(batch)</span><br></pre></td></tr></table></figure>
<p><code>trt_convertor.py</code>如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># trt_convertor.py</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ONNX2TRT</span>(<span class="params">args, calib=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; convert onnx to tensorrt engine, use mode of [&#x27;fp32&#x27;, &#x27;fp16&#x27;, &#x27;int8&#x27;]</span></span><br><span class="line"><span class="string">    :return: trt engine</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> args.mode.lower() <span class="keyword">in</span> [<span class="string">&#x27;fp32&#x27;</span>, <span class="string">&#x27;fp16&#x27;</span>, <span class="string">&#x27;int8&#x27;</span>], <span class="string">&quot;mode should be in [&#x27;fp32&#x27;, &#x27;fp16&#x27;, &#x27;int8&#x27;]&quot;</span></span><br><span class="line"></span><br><span class="line">    G_LOGGER = trt.Logger(trt.Logger.WARNING)</span><br><span class="line">    <span class="comment"># TRT&gt;=7.0中onnx解析器的network，需要指定EXPLICIT_BATCH</span></span><br><span class="line">    EXPLICIT_BATCH = <span class="number">1</span> &lt;&lt; <span class="built_in">int</span>(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)</span><br><span class="line">    <span class="keyword">with</span> trt.Builder(G_LOGGER) <span class="keyword">as</span> builder, \</span><br><span class="line">            builder.create_network(EXPLICIT_BATCH) <span class="keyword">as</span> network, \</span><br><span class="line">            trt.OnnxParser(network, G_LOGGER) <span class="keyword">as</span> parser:</span><br><span class="line"></span><br><span class="line">        builder.max_batch_size = args.batch_size</span><br><span class="line"></span><br><span class="line">        config = builder.create_builder_config()</span><br><span class="line">        config.max_workspace_size = <span class="number">1</span> &lt;&lt; <span class="number">30</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> not ok yet.</span></span><br><span class="line">        <span class="keyword">if</span> args.dynamic:</span><br><span class="line">            profile = builder.create_optimization_profile()</span><br><span class="line">            profile.set_shape(<span class="string">&quot;images&quot;</span>,</span><br><span class="line">                              (<span class="number">1</span>, args.channel, args.height, args.width),</span><br><span class="line">                              (<span class="number">2</span>, args.channel, args.height, args.width),</span><br><span class="line">                              (<span class="number">4</span>, args.channel, args.height, args.width)</span><br><span class="line">                              )</span><br><span class="line">            config.add_optimization_profile(profile)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># builder.max_workspace_size = 1 &lt;&lt; 30</span></span><br><span class="line">        <span class="keyword">if</span> args.mode.lower() == <span class="string">&#x27;int8&#x27;</span>:</span><br><span class="line">            <span class="keyword">assert</span> (builder.platform_has_fast_int8 == <span class="literal">True</span>), <span class="string">&quot;not support int8&quot;</span></span><br><span class="line">            <span class="keyword">assert</span> (calib <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>), <span class="string">&quot;need calib!&quot;</span></span><br><span class="line">            config.set_flag(trt.BuilderFlag.INT8)</span><br><span class="line">            config.int8_calibrator = calib</span><br><span class="line">        <span class="keyword">elif</span> args.mode.lower() == <span class="string">&#x27;fp16&#x27;</span>:</span><br><span class="line">            <span class="keyword">assert</span> (builder.platform_has_fast_fp16 == <span class="literal">True</span>), <span class="string">&quot;not support fp16&quot;</span></span><br><span class="line">            config.set_flag(trt.BuilderFlag.FP16)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Loading ONNX file from path &#123;&#125;...&#x27;</span>.<span class="built_in">format</span>(args.onnx_file_path))</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(args.onnx_file_path, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> model:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Beginning ONNX file parsing&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> parser.parse(model.read()):</span><br><span class="line">                <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(parser.num_errors):</span><br><span class="line">                    <span class="built_in">print</span>(parser.get_error(e))</span><br><span class="line">                <span class="keyword">raise</span> TypeError(<span class="string">&quot;Parser parse failed.&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Parsing ONNX file complete!&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Building an engine from file &#123;&#125;; this may take a while...&#x27;</span>.<span class="built_in">format</span>(args.onnx_file_path))</span><br><span class="line">        engine = builder.build_engine(network, config)</span><br><span class="line">        <span class="keyword">if</span> engine <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Create engine success! &quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;ERROR: Create engine failed! &quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 保存计划文件</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Saving TRT engine file to path &#123;&#125;...&#x27;</span>.<span class="built_in">format</span>(args.engine_file_path))</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(args.engine_file_path, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="comment"># # Metadata</span></span><br><span class="line">            <span class="comment"># meta = json.dumps(self.metadata)</span></span><br><span class="line">            <span class="comment"># t.write(len(meta).to_bytes(4, byteorder=&#x27;little&#x27;, signed=True))</span></span><br><span class="line">            <span class="comment"># t.write(meta.encode())</span></span><br><span class="line">            f.write(engine.serialize())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Engine file has already saved to &#123;&#125;!&#x27;</span>.<span class="built_in">format</span>(args.engine_file_path))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> engine</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loadEngine2TensorRT</span>(<span class="params">filepath</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    通过加载计划文件，构建TensorRT运行引擎</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    G_LOGGER = trt.Logger(trt.Logger.WARNING)</span><br><span class="line">    <span class="comment"># 反序列化引擎</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filepath, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f, trt.Runtime(G_LOGGER) <span class="keyword">as</span> runtime:</span><br><span class="line">        engine = runtime.deserialize_cuda_engine(f.read())</span><br><span class="line">        <span class="keyword">return</span> engine</span><br></pre></td></tr></table></figure>
<h3 id="2-1-2-c-代码版本生成calibratorfile和int8engine"><a href="#2-1-2-c-代码版本生成calibratorfile和int8engine" class="headerlink" title="2.1.2 c++代码版本生成calibratorfile和int8engine"></a><strong>2.1.2 c++代码版本生成calibratorfile和int8engine</strong></h3><p>参考代码</p>
<ul>
<li><strong><a target="_blank" rel="noopener" href="https://github.com/SakodaShintaro/Miacis">Miacis</a></strong>  </li>
<li><strong><a target="_blank" rel="noopener" href="https://github.com/kalfazed/tensorrt_starter">tensorrt_starter</a></strong></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/TaipKang/p/15542329.html">https://www.cnblogs.com/TaipKang/p/15542329.html</a> 这篇博客中也有全套的代码供参考，但是对应的tensorrt版本比较老。我现在是8.4.有些函数对应不上，仅供参考流程。</li>
</ul>
<h4 id="2-1-2-1-简单版本-只有IInt8EntropyCalibrator2"><a href="#2-1-2-1-简单版本-只有IInt8EntropyCalibrator2" class="headerlink" title="2.1.2.1 简单版本 只有IInt8EntropyCalibrator2"></a>2.1.2.1 简单版本 只有IInt8EntropyCalibrator2</h4><p>下面代码来自    <strong><a target="_blank" rel="noopener" href="https://github.com/kalfazed/tensorrt_starter">tensorrt_starter</a></strong>   </p>
<p>强烈推荐这个 <strong><a target="_blank" rel="noopener" href="https://github.com/kalfazed/tensorrt_starter">tensorrt_starter</a></strong>   里面有很多教程。这个例子包含了整个engine的生成和推理过程。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;trt_model.hpp&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;utils.hpp&quot;</span> </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;trt_logger.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInfer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvOnnxParser.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;trt_calibrator.hpp&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line">using namespace nvinfer1;</span><br><span class="line">using namespace nvonnxparser;</span><br><span class="line"></span><br><span class="line">namespace model&#123;</span><br><span class="line"></span><br><span class="line">Model::Model(<span class="built_in">string</span> onnx_path, logger::Level level, Params params) &#123;</span><br><span class="line">    m_onnxPath      = onnx_path;</span><br><span class="line">    m_workspaceSize = WORKSPACESIZE;</span><br><span class="line">    m_logger        = make_shared&lt;logger::Logger&gt;(level);</span><br><span class="line">    m_timer         = make_shared&lt;timer::Timer&gt;();</span><br><span class="line">    m_params        = new Params(params);</span><br><span class="line">    m_enginePath    = changePath(onnx_path, <span class="string">&quot;../engine&quot;</span>, <span class="string">&quot;.engine&quot;</span>, getPrec(params.prec));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">Model::load_image</span><span class="params">(<span class="built_in">string</span> image_path)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (!fileExists(image_path))&#123;</span><br><span class="line">        LOGE(<span class="string">&quot;%s not found&quot;</span>, image_path.c_str());</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        m_imagePath = image_path;</span><br><span class="line">        LOG(<span class="string">&quot;*********************INFERENCE INFORMATION***********************&quot;</span>);</span><br><span class="line">        LOG(<span class="string">&quot;\tModel:      %s&quot;</span>, getFileName(m_onnxPath).c_str());</span><br><span class="line">        LOG(<span class="string">&quot;\tImage:      %s&quot;</span>, getFileName(m_imagePath).c_str());</span><br><span class="line">        LOG(<span class="string">&quot;\tPrecision:  %s&quot;</span>, getPrec(m_params-&gt;prec).c_str());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">Model::init_model</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">/* 一个model的engine, context这些一旦创建好了，当多次调用这个模型的时候就没必要每次都初始化了*/</span></span><br><span class="line">    <span class="keyword">if</span> (m_context == nullptr)&#123;</span><br><span class="line">        <span class="keyword">if</span> (!fileExists(m_enginePath))&#123;</span><br><span class="line">            LOG(<span class="string">&quot;%s not found. Building trt engine...&quot;</span>, m_enginePath.c_str());</span><br><span class="line">            build_engine();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            LOG(<span class="string">&quot;%s has been generated! loading trt engine...&quot;</span>, m_enginePath.c_str());</span><br><span class="line">            load_engine();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        m_timer-&gt;init();</span><br><span class="line">        reset_task();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">bool</span> <span class="title function_">Model::build_engine</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// 我们也希望在build一个engine的时候就把一系列初始化全部做完，其中包括</span></span><br><span class="line">    <span class="comment">//  1. build一个engine</span></span><br><span class="line">    <span class="comment">//  2. 创建一个context</span></span><br><span class="line">    <span class="comment">//  3. 创建推理所用的stream</span></span><br><span class="line">    <span class="comment">//  4. 创建推理所需要的device空间</span></span><br><span class="line">    <span class="comment">// 这样，我们就可以在build结束以后，就可以直接推理了。这样的写法会比较干净</span></span><br><span class="line">    <span class="keyword">auto</span> builder       = <span class="built_in">shared_ptr</span>&lt;IBuilder&gt;(createInferBuilder(*m_logger), destroy_trt_ptr&lt;IBuilder&gt;);</span><br><span class="line">    <span class="keyword">auto</span> network       = <span class="built_in">shared_ptr</span>&lt;INetworkDefinition&gt;(builder-&gt;createNetworkV2(<span class="number">1</span>), destroy_trt_ptr&lt;INetworkDefinition&gt;);</span><br><span class="line">    <span class="keyword">auto</span> config        = <span class="built_in">shared_ptr</span>&lt;IBuilderConfig&gt;(builder-&gt;createBuilderConfig(), destroy_trt_ptr&lt;IBuilderConfig&gt;);</span><br><span class="line">    <span class="keyword">auto</span> parser        = <span class="built_in">shared_ptr</span>&lt;IParser&gt;(createParser(*network, *m_logger), destroy_trt_ptr&lt;IParser&gt;);</span><br><span class="line"></span><br><span class="line">    config-&gt;setMaxWorkspaceSize(m_workspaceSize);</span><br><span class="line">    config-&gt;setProfilingVerbosity(ProfilingVerbosity::kDETAILED); <span class="comment">//这里也可以设置为kDETAIL;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!parser-&gt;parseFromFile(m_onnxPath.c_str(), <span class="number">1</span>))&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (builder-&gt;platformHasFastFp16() &amp;&amp; m_params-&gt;prec == model::FP16) &#123;</span><br><span class="line">        config-&gt;setFlag(BuilderFlag::kFP16);</span><br><span class="line">        config-&gt;setFlag(BuilderFlag::kPREFER_PRECISION_CONSTRAINTS);</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (builder-&gt;platformHasFastInt8() &amp;&amp; m_params-&gt;prec == model::INT8) &#123;</span><br><span class="line">        config-&gt;setFlag(BuilderFlag::kINT8);</span><br><span class="line">        config-&gt;setFlag(BuilderFlag::kPREFER_PRECISION_CONSTRAINTS);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;Int8EntropyCalibrator&gt; <span class="title function_">calibrator</span><span class="params">(new Int8EntropyCalibrator(</span></span><br><span class="line"><span class="params">        <span class="number">64</span>, </span></span><br><span class="line"><span class="params">        <span class="string">&quot;calibration/calibration_list_coco.txt&quot;</span>, </span></span><br><span class="line"><span class="params">        <span class="string">&quot;calibration/calibration_table.txt&quot;</span>,</span></span><br><span class="line"><span class="params">        <span class="number">3</span> * <span class="number">224</span> * <span class="number">224</span>, <span class="number">224</span>, <span class="number">224</span>))</span>;</span><br><span class="line">    config-&gt;setInt8Calibrator(calibrator.get());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> engine        = <span class="built_in">shared_ptr</span>&lt;ICudaEngine&gt;(builder-&gt;buildEngineWithConfig(*network, *config), destroy_trt_ptr&lt;ICudaEngine&gt;);</span><br><span class="line">    <span class="keyword">auto</span> plan          = builder-&gt;buildSerializedNetwork(*network, *config);</span><br><span class="line">    <span class="keyword">auto</span> runtime       = <span class="built_in">shared_ptr</span>&lt;IRuntime&gt;(createInferRuntime(*m_logger), destroy_trt_ptr&lt;IRuntime&gt;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 保存序列化后的engine</span></span><br><span class="line">    save_plan(*plan);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据runtime初始化engine, context, 以及memory</span></span><br><span class="line">    setup(plan-&gt;data(), plan-&gt;size());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 把优化前和优化后的各个层的信息打印出来</span></span><br><span class="line">    LOGV(<span class="string">&quot;Before TensorRT optimization&quot;</span>);</span><br><span class="line">    print_network(*network, <span class="literal">false</span>);</span><br><span class="line">    LOGV(<span class="string">&quot;After TensorRT optimization&quot;</span>);</span><br><span class="line">    print_network(*network, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">bool</span> <span class="title function_">Model::load_engine</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// 同样的，我们也希望在load一个engine的时候就把一系列初始化全部做完，其中包括</span></span><br><span class="line">    <span class="comment">//  1. deserialize一个engine</span></span><br><span class="line">    <span class="comment">//  2. 创建一个context</span></span><br><span class="line">    <span class="comment">//  3. 创建推理所用的stream</span></span><br><span class="line">    <span class="comment">//  4. 创建推理所需要的device空间</span></span><br><span class="line">    <span class="comment">// 这样，我们就可以在load结束以后，就可以直接推理了。这样的写法会比较干净</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (!fileExists(m_enginePath)) &#123;</span><br><span class="line">        LOGE(<span class="string">&quot;engine does not exits! Program terminated&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="type">unsigned</span> <span class="type">char</span>&gt; modelData;</span><br><span class="line">    modelData     = loadFile(m_enginePath);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 根据runtime初始化engine, context, 以及memory</span></span><br><span class="line">    setup(modelData.data(), modelData.size());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">Model::save_plan</span><span class="params">(IHostMemory&amp; plan)</span> &#123;</span><br><span class="line">    <span class="keyword">auto</span> f = fopen(m_enginePath.c_str(), <span class="string">&quot;wb&quot;</span>);</span><br><span class="line">    fwrite(plan.data(), <span class="number">1</span>, plan.size(), f);</span><br><span class="line">    fclose(f);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment">    可以根据情况选择是否在CPU上跑pre/postprocess</span></span><br><span class="line"><span class="comment">    对于一些edge设备，为了最大化GPU利用效率，我们可以考虑让CPU做一些pre/postprocess，让其执行与GPU重叠</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">Model::inference</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (m_params-&gt;dev == CPU) &#123;</span><br><span class="line">        preprocess_cpu();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        preprocess_gpu();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    enqueue_bindings();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (m_params-&gt;dev == CPU) &#123;</span><br><span class="line">        postprocess_cpu();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        postprocess_gpu();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="type">bool</span> <span class="title function_">Model::enqueue_bindings</span><span class="params">()</span> &#123;</span><br><span class="line">    m_timer-&gt;start_gpu();</span><br><span class="line">    <span class="keyword">if</span> (!m_context-&gt;enqueueV2((<span class="type">void</span>**)m_bindings, m_stream, nullptr))&#123;</span><br><span class="line">        LOG(<span class="string">&quot;Error happens during DNN inference part, program terminated&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    m_timer-&gt;stop_gpu(<span class="string">&quot;trt-inference(GPU)&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">Model::print_network</span><span class="params">(INetworkDefinition &amp;network, <span class="type">bool</span> optimized)</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> inputCount = network.getNbInputs();</span><br><span class="line">    <span class="type">int</span> outputCount = network.getNbOutputs();</span><br><span class="line">    <span class="built_in">string</span> layer_info;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; inputCount; i++) &#123;</span><br><span class="line">        <span class="keyword">auto</span> input = network.getInput(i);</span><br><span class="line">        LOGV(<span class="string">&quot;Input info: %s:%s&quot;</span>, input-&gt;getName(), printTensorShape(input).c_str());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; outputCount; i++) &#123;</span><br><span class="line">        <span class="keyword">auto</span> output = network.getOutput(i);</span><br><span class="line">        LOGV(<span class="string">&quot;Output info: %s:%s&quot;</span>, output-&gt;getName(), printTensorShape(output).c_str());</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> layerCount = optimized ? m_engine-&gt;getNbLayers() : network.getNbLayers();</span><br><span class="line">    LOGV(<span class="string">&quot;network has %d layers&quot;</span>, layerCount);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!optimized) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; layerCount; i++) &#123;</span><br><span class="line">            <span class="type">char</span> layer_info[<span class="number">1000</span>];</span><br><span class="line">            <span class="keyword">auto</span> layer   = network.getLayer(i);</span><br><span class="line">            <span class="keyword">auto</span> input   = layer-&gt;getInput(<span class="number">0</span>);</span><br><span class="line">            <span class="type">int</span> n = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">if</span> (input == nullptr)&#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">auto</span> output  = layer-&gt;getOutput(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">            LOGV(<span class="string">&quot;layer_info: %-40s:%-25s-&gt;%-25s[%s]&quot;</span>, </span><br><span class="line">                layer-&gt;getName(),</span><br><span class="line">                printTensorShape(input).c_str(),</span><br><span class="line">                printTensorShape(output).c_str(),</span><br><span class="line">                getPrecision(layer-&gt;getPrecision()).c_str());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">auto</span> inspector = <span class="built_in">shared_ptr</span>&lt;IEngineInspector&gt;(m_engine-&gt;createEngineInspector());</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; layerCount; i++) &#123;</span><br><span class="line">            LOGV(<span class="string">&quot;layer_info: %s&quot;</span>, inspector-&gt;getLayerInformation(i, nvinfer1::LayerInformationFormat::kJSON));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">string</span> <span class="title function_">Model::getPrec</span><span class="params">(model::precision prec)</span> &#123;</span><br><span class="line">    <span class="keyword">switch</span>(prec) &#123;</span><br><span class="line">        <span class="keyword">case</span> model::precision::FP16:   <span class="keyword">return</span> <span class="string">&quot;fp16&quot;</span>;</span><br><span class="line">        <span class="keyword">case</span> model::precision::INT8:   <span class="keyword">return</span> <span class="string">&quot;int8&quot;</span>;</span><br><span class="line">        <span class="keyword">default</span>:                       <span class="keyword">return</span> <span class="string">&quot;fp32&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125; <span class="comment">// namespace model</span></span><br></pre></td></tr></table></figure>
<p>src/model/calibrator.hpp代码如下</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInfer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;trt_calibrator.hpp&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;utils.hpp&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;trt_logger.hpp&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;trt_preprocess.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iterator&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/highgui/highgui.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> model&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * calibrator的构造函数</span></span><br><span class="line"><span class="comment"> * 我们在这里把calibration所需要的数据集准备好，需要保证数据集的数量可以被batchSize整除</span></span><br><span class="line"><span class="comment"> * 同时由于calibration是在device上进行的，所以需要分配空间</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">Int8EntropyCalibrator::<span class="built_in">Int8EntropyCalibrator</span>(</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span>&amp;    batchSize,</span><br><span class="line">    <span class="type">const</span> string&amp; calibrationDataPath,</span><br><span class="line">    <span class="type">const</span> string&amp; calibrationTablePath,</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span>&amp;    inputSize,</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span>&amp;    inputH,</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span>&amp;    inputW):</span><br><span class="line"></span><br><span class="line">    <span class="built_in">m_batchSize</span>(batchSize),</span><br><span class="line">    <span class="built_in">m_inputH</span>(inputH),</span><br><span class="line">    <span class="built_in">m_inputW</span>(inputW),</span><br><span class="line">    <span class="built_in">m_inputSize</span>(inputSize),</span><br><span class="line">    <span class="built_in">m_inputCount</span>(batchSize * inputSize),</span><br><span class="line">    <span class="built_in">m_calibrationTablePath</span>(calibrationTablePath)</span><br><span class="line">&#123;</span><br><span class="line">    m_imageList = <span class="built_in">loadDataList</span>(calibrationDataPath);</span><br><span class="line">    m_imageList.<span class="built_in">resize</span>(<span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(m_imageList.<span class="built_in">size</span>() / m_batchSize) * m_batchSize);</span><br><span class="line">    std::<span class="built_in">random_shuffle</span>(m_imageList.<span class="built_in">begin</span>(), m_imageList.<span class="built_in">end</span>(), </span><br><span class="line">                        [](<span class="type">int</span> i)&#123; <span class="keyword">return</span> <span class="built_in">rand</span>() % i; &#125;);</span><br><span class="line">    <span class="built_in">CUDA_CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;m_deviceInput, m_inputCount * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 获取做calibration的时候的一个batch的图片，之后上传到device上</span></span><br><span class="line"><span class="comment"> * 需要注意的是，这里面的一个batch中的每一个图片，都需要做与真正推理是一样的前处理</span></span><br><span class="line"><span class="comment"> * 这里面我们选择在GPU上进行前处理，所以处理万</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">Int8EntropyCalibrator::getBatch</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">void</span>* bindings[], <span class="type">const</span> <span class="type">char</span>* names[], <span class="type">int</span> nbBindings)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (m_imageIndex + m_batchSize &gt;= m_imageList.<span class="built_in">size</span>() + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">LOG</span>(<span class="string">&quot;%3d/%3d (%3dx%3d): %s&quot;</span>, </span><br><span class="line">        m_imageIndex + <span class="number">1</span>, m_imageList.<span class="built_in">size</span>(), m_inputH, m_inputW, m_imageList.<span class="built_in">at</span>(m_imageIndex).<span class="built_in">c_str</span>());</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 对一个batch里的所有图像进行预处理</span></span><br><span class="line"><span class="comment">     * 这里可有以及个扩展的点</span></span><br><span class="line"><span class="comment">     *  1. 可以把这个部分做成函数，以函数指针的方式传给calibrator。因为不同的task会有不同的预处理</span></span><br><span class="line"><span class="comment">     *  2. 可以实现一个bacthed preprocess</span></span><br><span class="line"><span class="comment">     * 这里留给当作今后的TODO</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    cv::Mat input_image;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; m_batchSize; i ++)&#123;</span><br><span class="line">        input_image = cv::<span class="built_in">imread</span>(m_imageList.<span class="built_in">at</span>(m_imageIndex++));</span><br><span class="line">        preprocess::<span class="built_in">preprocess_resize_gpu</span>(</span><br><span class="line">            input_image, </span><br><span class="line">            m_deviceInput + i * m_inputSize,</span><br><span class="line">            m_inputH, m_inputW, </span><br><span class="line">            preprocess::tactics::GPU_BILINEAR_CENTER);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    bindings[<span class="number">0</span>] = m_deviceInput;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * 读取calibration table的信息来创建INT8的推理引擎, </span></span><br><span class="line"><span class="comment"> * 将calibration table的信息存储到calibration cache，这样可以防止每次创建int推理引擎的时候都需要跑一次calibration</span></span><br><span class="line"><span class="comment"> * 如果没有calibration table的话就会直接跳过这一步，之后调用writeCalibrationCache来创建calibration table</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">void</span>* <span class="title">Int8EntropyCalibrator::readCalibrationCache</span><span class="params">(<span class="type">size_t</span>&amp; length)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">void</span>* output;</span><br><span class="line">    m_calibrationCache.<span class="built_in">clear</span>();</span><br><span class="line"></span><br><span class="line">    <span class="function">ifstream <span class="title">input</span><span class="params">(m_calibrationTablePath, ios::binary)</span></span>;</span><br><span class="line">    input &gt;&gt; noskipws;</span><br><span class="line">    <span class="keyword">if</span> (m_readCache &amp;&amp; input.<span class="built_in">good</span>())</span><br><span class="line">        <span class="built_in">copy</span>(<span class="built_in">istream_iterator</span>&lt;<span class="type">char</span>&gt;(input), <span class="built_in">istream_iterator</span>&lt;<span class="type">char</span>&gt;(), <span class="built_in">back_inserter</span>(m_calibrationCache));</span><br><span class="line"></span><br><span class="line">    length = m_calibrationCache.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">if</span> (length)&#123;</span><br><span class="line">        <span class="built_in">LOG</span>(<span class="string">&quot;Using cached calibration table to build INT8 trt engine...&quot;</span>);</span><br><span class="line">        output = &amp;m_calibrationCache[<span class="number">0</span>];</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="built_in">LOG</span>(<span class="string">&quot;Creating new calibration table to build INT8 trt engine...&quot;</span>);</span><br><span class="line">        output = <span class="literal">nullptr</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> output;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment"> * 将calibration cache的信息写入到calibration table中</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Int8EntropyCalibrator::writeCalibrationCache</span><span class="params">(<span class="type">const</span> <span class="type">void</span>* cache, <span class="type">size_t</span> length)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">ofstream <span class="title">output</span><span class="params">(m_calibrationTablePath, ios::binary)</span></span>;</span><br><span class="line">    output.<span class="built_in">write</span>(<span class="built_in">reinterpret_cast</span>&lt;<span class="type">const</span> <span class="type">char</span>*&gt;(cache), length);</span><br><span class="line">    output.<span class="built_in">close</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125; <span class="comment">// namespace model</span></span><br></pre></td></tr></table></figure>
<p>对应的头文件如下</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//trt_calibrator.hpp</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifndef</span> __TRT_CALIBRATOR_HPP__</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> __TRT_CALIBRATOR_HPP__</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInfer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> model&#123;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 自定义一个calibrator类</span></span><br><span class="line"><span class="comment"> * 我们在创建calibrator的时候需要继承nvinfer1中的calibrator类</span></span><br><span class="line"><span class="comment"> * TensorRT提供了五种Calibrator类</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *   - nvinfer1::IInt8EntropyCalibrator2</span></span><br><span class="line"><span class="comment"> *   - nvinfer1::IInt8MinMaxCalibrator</span></span><br><span class="line"><span class="comment"> *   - nvinfer1::IInt8EntropyCalibrator</span></span><br><span class="line"><span class="comment"> *   - nvinfer1::IInt8LegacyCalibrator</span></span><br><span class="line"><span class="comment"> *   - nvinfer1::IInt8Calibrator</span></span><br><span class="line"><span class="comment"> * 具体有什么不同，建议读一下官方文档和回顾一下之前的学习资料</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 默认下推荐使用IInt8EntropyCalibrator2</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Int8EntropyCalibrator</span>: <span class="keyword">public</span> nvinfer1::IInt8EntropyCalibrator2 &#123;</span><br><span class="line"><span class="comment">// class Int8EntropyCalibrator: public nvinfer1::IInt8MinMaxCalibrator &#123;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Int8EntropyCalibrator</span>(</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span>&amp; batchSize,</span><br><span class="line">        <span class="type">const</span> std::string&amp; calibrationSetPath,</span><br><span class="line">        <span class="type">const</span> std::string&amp; calibrationTablePath,</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span>&amp; inputSize,</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span>&amp; inputH,</span><br><span class="line">        <span class="type">const</span> <span class="type">int</span>&amp; inputW);</span><br><span class="line"></span><br><span class="line">    ~<span class="built_in">Int8EntropyCalibrator</span>()&#123;&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="type">int</span>         <span class="title">getBatchSize</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span> </span>&#123;<span class="keyword">return</span> m_batchSize;&#125;;</span><br><span class="line">    <span class="function"><span class="type">bool</span>        <span class="title">getBatch</span><span class="params">(<span class="type">void</span>* bindings[], <span class="type">const</span> <span class="type">char</span>* names[], <span class="type">int</span> nbBindings)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> <span class="type">void</span>* <span class="title">readCalibrationCache</span><span class="params">(std::<span class="type">size_t</span> &amp;length)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>        <span class="title">writeCalibrationCache</span> <span class="params">(<span class="type">const</span> <span class="type">void</span>* ptr, std::<span class="type">size_t</span> legth)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span>   m_batchSize;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span>   m_inputH;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span>   m_inputW;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span>   m_inputSize;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span>   m_inputCount;</span><br><span class="line">    <span class="type">const</span> std::string m_calibrationTablePath &#123;<span class="literal">nullptr</span>&#125;;</span><br><span class="line">    </span><br><span class="line">    std::vector&lt;std::string&gt; m_imageList;</span><br><span class="line">    std::vector&lt;<span class="type">char</span>&gt;        m_calibrationCache;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span>* m_deviceInput&#123;<span class="literal">nullptr</span>&#125;;</span><br><span class="line">    <span class="type">bool</span>   m_readCache&#123;<span class="literal">true</span>&#125;;</span><br><span class="line">    <span class="type">int</span>    m_imageIndex;</span><br><span class="line">    </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">&#125;; <span class="comment">// namespace model</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span> __TRT_CALIBRATOR_HPP__</span></span><br></pre></td></tr></table></figure>
<h4 id="2-1-2-2-完相对完善的量化器方案"><a href="#2-1-2-2-完相对完善的量化器方案" class="headerlink" title="2.1.2.2 完相对完善的量化器方案"></a>2.1.2.2 完相对完善的量化器方案</h4><p>上面的代码之实现了一个量化的方案。</p>
<p>下面的实现了三种，也很有参考价值，<a target="_blank" rel="noopener" href="https://github.com/leo-drive/avte.autoware_universe/blob/4542112f80ddbfe089cefe1dfffa20470399eed3/perception/tensorrt_yolox/include/tensorrt_yolox/calibrator.hpp#L166">参考网址</a></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//calibrator.hpp</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Copyright 2023 TIER IV, Inc.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment">// you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">// You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment">// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">// See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">// limitations under the License.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Permission is hereby granted, free of charge, to any person obtaining a</span></span><br><span class="line"><span class="comment"> * copy of this software and associated documentation files (the &quot;Software&quot;),</span></span><br><span class="line"><span class="comment"> * to deal in the Software without restriction, including without limitation</span></span><br><span class="line"><span class="comment"> * the rights to use, copy, modify, merge, publish, distribute, sublicense,</span></span><br><span class="line"><span class="comment"> * and/or sell copies of the Software, and to permit persons to whom the</span></span><br><span class="line"><span class="comment"> * Software is furnished to do so, subject to the following conditions:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The above copyright notice and this permission notice shall be included in</span></span><br><span class="line"><span class="comment"> * all copies or substantial portions of the Software.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span></span><br><span class="line"><span class="comment"> * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,</span></span><br><span class="line"><span class="comment"> * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL</span></span><br><span class="line"><span class="comment"> * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER</span></span><br><span class="line"><span class="comment"> * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING</span></span><br><span class="line"><span class="comment"> * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER</span></span><br><span class="line"><span class="comment"> * DEALINGS IN THE SOFTWARE.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifndef</span> TENSORRT_YOLOX__CALIBRATOR_HPP_</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> TENSORRT_YOLOX__CALIBRATOR_HPP_</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cuda_utils/cuda_check_error.hpp&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cuda_utils/cuda_unique_ptr.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/highgui/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/opencv.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;NvInfer.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;assert.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iterator&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> tensorrt_yolox</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImageStream</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">ImageStream</span>(</span><br><span class="line">    <span class="type">int</span> batch_size, nvinfer1::Dims input_dims, <span class="type">const</span> std::vector&lt;std::string&gt; calibration_images)</span><br><span class="line">  : <span class="built_in">batch_size_</span>(batch_size),</span><br><span class="line">    <span class="built_in">calibration_images_</span>(calibration_images),</span><br><span class="line">    <span class="built_in">current_batch_</span>(<span class="number">0</span>),</span><br><span class="line">    <span class="built_in">max_batches_</span>(calibration_images.<span class="built_in">size</span>() / batch_size_),</span><br><span class="line">    <span class="built_in">input_dims_</span>(input_dims)</span><br><span class="line">  &#123;</span><br><span class="line">    batch_.<span class="built_in">resize</span>(batch_size_ * input_dims_.d[<span class="number">1</span>] * input_dims_.d[<span class="number">2</span>] * input_dims_.d[<span class="number">3</span>]);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">getBatchSize</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> batch_size_; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">getMaxBatches</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> max_batches_; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">float</span> * <span class="title">getBatch</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> &amp;batch_[<span class="number">0</span>]; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">nvinfer1::Dims <span class="title">getInputDims</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> input_dims_; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * @brief Preprocess in calibration</span></span><br><span class="line"><span class="comment">   * @param[in] images input images</span></span><br><span class="line"><span class="comment">   * @param[in] input_dims input dimensions</span></span><br><span class="line"><span class="comment">   * @param[in] norm normalization (0.0-1.0)</span></span><br><span class="line"><span class="comment">   * @return vector&lt;float&gt; preprocessed data</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">preprocess</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> std::vector&lt;cv::Mat&gt; &amp; images, nvinfer1::Dims input_dims, <span class="type">double</span> norm)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    std::vector&lt;<span class="type">float</span>&gt; input_h_;</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> batch_size = images.<span class="built_in">size</span>();</span><br><span class="line">    input_dims.d[<span class="number">0</span>] = batch_size;</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> input_height = <span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(input_dims.d[<span class="number">2</span>]);</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> input_width = <span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(input_dims.d[<span class="number">3</span>]);</span><br><span class="line">    std::vector&lt;cv::Mat&gt; dst_images;</span><br><span class="line">    std::vector&lt;<span class="type">float</span>&gt; scales_;</span><br><span class="line">    scales_.<span class="built_in">clear</span>();</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span> &amp; image : images) &#123;</span><br><span class="line">      cv::Mat dst_image;</span><br><span class="line">      <span class="type">const</span> <span class="type">float</span> scale = std::<span class="built_in">min</span>(input_width / image.cols, input_height / image.rows);</span><br><span class="line">      scales_.<span class="built_in">emplace_back</span>(scale);</span><br><span class="line">      <span class="type">const</span> <span class="keyword">auto</span> scale_size = cv::<span class="built_in">Size</span>(image.cols * scale, image.rows * scale);</span><br><span class="line">      cv::<span class="built_in">resize</span>(image, dst_image, scale_size, <span class="number">0</span>, <span class="number">0</span>, cv::INTER_CUBIC);</span><br><span class="line">      <span class="type">const</span> <span class="keyword">auto</span> bottom = input_height - dst_image.rows;</span><br><span class="line">      <span class="type">const</span> <span class="keyword">auto</span> right = input_width - dst_image.cols;</span><br><span class="line">      <span class="built_in">copyMakeBorder</span>(</span><br><span class="line">        dst_image, dst_image, <span class="number">0</span>, bottom, <span class="number">0</span>, right, cv::BORDER_CONSTANT, &#123;<span class="number">114</span>, <span class="number">114</span>, <span class="number">114</span>&#125;);</span><br><span class="line">      dst_images.<span class="built_in">emplace_back</span>(dst_image);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> chw_images =</span><br><span class="line">      cv::dnn::<span class="built_in">blobFromImages</span>(dst_images, norm, cv::<span class="built_in">Size</span>(), cv::<span class="built_in">Scalar</span>(), <span class="literal">false</span>, <span class="literal">false</span>, CV_32F);</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> data_length = chw_images.<span class="built_in">total</span>();</span><br><span class="line">    input_h_.<span class="built_in">reserve</span>(data_length);</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> flat = chw_images.<span class="built_in">reshape</span>(<span class="number">1</span>, data_length);</span><br><span class="line">    input_h_ = chw_images.<span class="built_in">isContinuous</span>() ? flat : flat.<span class="built_in">clone</span>();</span><br><span class="line">    <span class="keyword">return</span> input_h_;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * @brief Decode data in calibration</span></span><br><span class="line"><span class="comment">   * @param[in] scale normalization (0.0-1.0)</span></span><br><span class="line"><span class="comment">   * @return bool succ or fail</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">next</span><span class="params">(<span class="type">double</span> scale)</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (current_batch_ == max_batches_) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; batch_size_; ++i) &#123;</span><br><span class="line">      <span class="keyword">auto</span> image =</span><br><span class="line">        cv::<span class="built_in">imread</span>(calibration_images_[batch_size_ * current_batch_ + i].<span class="built_in">c_str</span>(), cv::IMREAD_COLOR);</span><br><span class="line">      std::cout &lt;&lt; current_batch_ &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; Preprocess &quot;</span></span><br><span class="line">                &lt;&lt; calibration_images_[batch_size_ * current_batch_ + i].<span class="built_in">c_str</span>() &lt;&lt; std::endl;</span><br><span class="line">      <span class="keyword">auto</span> input = <span class="built_in">preprocess</span>(&#123;image&#125;, input_dims_, scale);</span><br><span class="line">      batch_.<span class="built_in">insert</span>(</span><br><span class="line">        batch_.<span class="built_in">begin</span>() + i * input_dims_.d[<span class="number">1</span>] * input_dims_.d[<span class="number">2</span>] * input_dims_.d[<span class="number">3</span>], input.<span class="built_in">begin</span>(),</span><br><span class="line">        input.<span class="built_in">end</span>());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ++current_batch_;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * @brief Reset calibration</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">reset</span><span class="params">()</span> </span>&#123; current_batch_ = <span class="number">0</span>; &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  <span class="type">int</span> batch_size_;</span><br><span class="line">  std::vector&lt;std::string&gt; calibration_images_;</span><br><span class="line">  <span class="type">int</span> current_batch_;</span><br><span class="line">  <span class="type">int</span> max_batches_;</span><br><span class="line">  nvinfer1::Dims input_dims_;</span><br><span class="line">  std::vector&lt;<span class="type">float</span>&gt; batch_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**Percentile calibration using legacy calibrator*/</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @class Int8LegacyCalibrator</span></span><br><span class="line"><span class="comment"> * @brief Calibrator for Percentle</span></span><br><span class="line"><span class="comment"> * @warning We are confirming bug on Tegra like Xavier and Orin. We recommand use MinMax calibrator</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Int8LegacyCalibrator</span> : <span class="keyword">public</span> nvinfer1::IInt8LegacyCalibrator</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">Int8LegacyCalibrator</span>(</span><br><span class="line">    ImageStream &amp; stream, <span class="type">const</span> std::string calibration_cache_file,</span><br><span class="line">    <span class="type">const</span> std::string histogram_cache_file, <span class="type">double</span> scale = <span class="number">1.0</span>, <span class="type">bool</span> read_cache = <span class="literal">true</span>,</span><br><span class="line">    <span class="type">double</span> quantile = <span class="number">0.999999</span>, <span class="type">double</span> cutoff = <span class="number">0.999999</span>)</span><br><span class="line">  : <span class="built_in">stream_</span>(stream),</span><br><span class="line">    <span class="built_in">calibration_cache_file_</span>(calibration_cache_file),</span><br><span class="line">    <span class="built_in">histogranm_cache_file_</span>(histogram_cache_file),</span><br><span class="line">    <span class="built_in">read_cache_</span>(read_cache)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">auto</span> d = stream_.<span class="built_in">getInputDims</span>();</span><br><span class="line">    input_count_ = stream_.<span class="built_in">getBatchSize</span>() * d.d[<span class="number">1</span>] * d.d[<span class="number">2</span>] * d.d[<span class="number">3</span>];</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMalloc</span>(&amp;device_input_, input_count_ * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    scale_ = scale;</span><br><span class="line">    quantile_ = quantile;</span><br><span class="line">    cutoff_ = cutoff;</span><br><span class="line">    <span class="keyword">auto</span> algType = <span class="built_in">getAlgorithm</span>();</span><br><span class="line">    <span class="keyword">switch</span> (algType) &#123;</span><br><span class="line">      <span class="built_in">case</span> (nvinfer1::CalibrationAlgoType::kLEGACY_CALIBRATION):</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;CalibratioAlgoType : kLEGACY_CALIBRATION&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="built_in">case</span> (nvinfer1::CalibrationAlgoType::kENTROPY_CALIBRATION):</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;CalibratioAlgoType : kENTROPY_CALIBRATION&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="built_in">case</span> (nvinfer1::CalibrationAlgoType::kENTROPY_CALIBRATION_2):</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;CalibratioAlgoType : kENTROPY_CALIBRATION_2&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="built_in">case</span> (nvinfer1::CalibrationAlgoType::kMINMAX_CALIBRATION):</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;CalibratioAlgoType : kMINMAX_CALIBRATION&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;No CalibrationAlgType&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">getBatchSize</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span> </span>&#123; <span class="keyword">return</span> stream_.<span class="built_in">getBatchSize</span>(); &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">virtual</span> ~<span class="built_in">Int8LegacyCalibrator</span>() &#123; <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(device_input_)); &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">getBatch</span><span class="params">(<span class="type">void</span> * bindings[], <span class="type">const</span> <span class="type">char</span> * names[], <span class="type">int</span> nb_bindings)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    (<span class="type">void</span>)names;</span><br><span class="line">    (<span class="type">void</span>)nb_bindings;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!stream_.<span class="built_in">next</span>(scale_)) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpy</span>(</span><br><span class="line">        device_input_, stream_.<span class="built_in">getBatch</span>(), input_count_ * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice));</span><br><span class="line">    &#125; <span class="built_in">catch</span> (<span class="type">const</span> std::exception &amp; e) &#123;</span><br><span class="line">      <span class="comment">// Do nothing</span></span><br><span class="line">    &#125;</span><br><span class="line">    bindings[<span class="number">0</span>] = device_input_;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">const</span> <span class="type">void</span> * <span class="title">readCalibrationCache</span><span class="params">(<span class="type">size_t</span> &amp; length)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    calib_cache_.<span class="built_in">clear</span>();</span><br><span class="line">    <span class="function">std::ifstream <span class="title">input</span><span class="params">(calibration_cache_file_, std::ios::binary)</span></span>;</span><br><span class="line">    input &gt;&gt; std::noskipws;</span><br><span class="line">    <span class="keyword">if</span> (read_cache_ &amp;&amp; input.<span class="built_in">good</span>()) &#123;</span><br><span class="line">      std::<span class="built_in">copy</span>(</span><br><span class="line">        std::<span class="built_in">istream_iterator</span>&lt;<span class="type">char</span>&gt;(input), std::<span class="built_in">istream_iterator</span>&lt;<span class="type">char</span>&gt;(),</span><br><span class="line">        std::<span class="built_in">back_inserter</span>(calib_cache_));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    length = calib_cache_.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">if</span> (length) &#123;</span><br><span class="line">      std::cout &lt;&lt; <span class="string">&quot;Using cached calibration table to build the engine&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      std::cout &lt;&lt; <span class="string">&quot;New calibration table will be created to build the engine&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> length ? &amp;calib_cache_[<span class="number">0</span>] : <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">writeCalibrationCache</span><span class="params">(<span class="type">const</span> <span class="type">void</span> * cache, <span class="type">size_t</span> length)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="function">std::ofstream <span class="title">output</span><span class="params">(calibration_cache_file_, std::ios::binary)</span></span>;</span><br><span class="line">    output.<span class="built_in">write</span>(<span class="built_in">reinterpret_cast</span>&lt;<span class="type">const</span> <span class="type">char</span> *&gt;(cache), length);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">double</span> <span class="title">getQuantile</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Quantile %f\n&quot;</span>, quantile_);</span><br><span class="line">    <span class="keyword">return</span> quantile_;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">double</span> <span class="title">getRegressionCutoff</span><span class="params">(<span class="type">void</span>)</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Cutoff %f\n&quot;</span>, cutoff_);</span><br><span class="line">    <span class="keyword">return</span> cutoff_;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">const</span> <span class="type">void</span> * <span class="title">readHistogramCache</span><span class="params">(std::<span class="type">size_t</span> &amp; length)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    hist_cache_.<span class="built_in">clear</span>();</span><br><span class="line">    <span class="function">std::ifstream <span class="title">input</span><span class="params">(histogranm_cache_file_, std::ios::binary)</span></span>;</span><br><span class="line">    input &gt;&gt; std::noskipws;</span><br><span class="line">    <span class="keyword">if</span> (read_cache_ &amp;&amp; input.<span class="built_in">good</span>()) &#123;</span><br><span class="line">      std::<span class="built_in">copy</span>(</span><br><span class="line">        std::<span class="built_in">istream_iterator</span>&lt;<span class="type">char</span>&gt;(input), std::<span class="built_in">istream_iterator</span>&lt;<span class="type">char</span>&gt;(),</span><br><span class="line">        std::<span class="built_in">back_inserter</span>(hist_cache_));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    length = hist_cache_.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">if</span> (length) &#123;</span><br><span class="line">      std::cout &lt;&lt; <span class="string">&quot;Using cached histogram table to build the engine&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      std::cout &lt;&lt; <span class="string">&quot;New histogram table will be created to build the engine&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> length ? &amp;hist_cache_[<span class="number">0</span>] : <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">writeHistogramCache</span><span class="params">(<span class="type">void</span> <span class="type">const</span> * ptr, std::<span class="type">size_t</span> length)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="function">std::ofstream <span class="title">output</span><span class="params">(histogranm_cache_file_, std::ios::binary)</span></span>;</span><br><span class="line">    output.<span class="built_in">write</span>(<span class="built_in">reinterpret_cast</span>&lt;<span class="type">const</span> <span class="type">char</span> *&gt;(ptr), length);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  ImageStream stream_;</span><br><span class="line">  <span class="type">const</span> std::string calibration_cache_file_;</span><br><span class="line">  <span class="type">const</span> std::string histogranm_cache_file_;</span><br><span class="line">  <span class="type">bool</span> read_cache_&#123;<span class="literal">true</span>&#125;;</span><br><span class="line">  <span class="type">size_t</span> input_count_;</span><br><span class="line">  <span class="type">void</span> * device_input_&#123;<span class="literal">nullptr</span>&#125;;</span><br><span class="line">  std::vector&lt;<span class="type">char</span>&gt; calib_cache_;</span><br><span class="line">  std::vector&lt;<span class="type">char</span>&gt; hist_cache_;</span><br><span class="line">  <span class="type">double</span> scale_;</span><br><span class="line">  <span class="type">double</span> quantile_;</span><br><span class="line">  <span class="type">double</span> cutoff_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @class Int8LegacyCalibrator</span></span><br><span class="line"><span class="comment"> * @brief Calibrator for Percentle</span></span><br><span class="line"><span class="comment"> * @warning This calibrator causes crucial accuracy drop for YOLOX.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Int8EntropyCalibrator</span> : <span class="keyword">public</span> nvinfer1::IInt8EntropyCalibrator2</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">Int8EntropyCalibrator</span>(</span><br><span class="line">    ImageStream &amp; stream, <span class="type">const</span> std::string calibration_cache_file, <span class="type">double</span> scale = <span class="number">1.0</span>,</span><br><span class="line">    <span class="type">bool</span> read_cache = <span class="literal">true</span>)</span><br><span class="line">  : <span class="built_in">stream_</span>(stream), <span class="built_in">calibration_cache_file_</span>(calibration_cache_file), <span class="built_in">read_cache_</span>(read_cache)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">auto</span> d = stream_.<span class="built_in">getInputDims</span>();</span><br><span class="line">    input_count_ = stream_.<span class="built_in">getBatchSize</span>() * d.d[<span class="number">1</span>] * d.d[<span class="number">2</span>] * d.d[<span class="number">3</span>];</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMalloc</span>(&amp;device_input_, input_count_ * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    scale_ = scale;</span><br><span class="line">    <span class="keyword">auto</span> algType = <span class="built_in">getAlgorithm</span>();</span><br><span class="line">    <span class="keyword">switch</span> (algType) &#123;</span><br><span class="line">      <span class="built_in">case</span> (nvinfer1::CalibrationAlgoType::kLEGACY_CALIBRATION):</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;CalibratioAlgoType : kLEGACY_CALIBRATION&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="built_in">case</span> (nvinfer1::CalibrationAlgoType::kENTROPY_CALIBRATION):</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;CalibratioAlgoType : kENTROPY_CALIBRATION&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="built_in">case</span> (nvinfer1::CalibrationAlgoType::kENTROPY_CALIBRATION_2):</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;CalibratioAlgoType : kENTROPY_CALIBRATION_2&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="built_in">case</span> (nvinfer1::CalibrationAlgoType::kMINMAX_CALIBRATION):</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;CalibratioAlgoType : kMINMAX_CALIBRATION&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;No CalibrationAlgType&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">getBatchSize</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span> </span>&#123; <span class="keyword">return</span> stream_.<span class="built_in">getBatchSize</span>(); &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">virtual</span> ~<span class="built_in">Int8EntropyCalibrator</span>() &#123; <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(device_input_)); &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">getBatch</span><span class="params">(<span class="type">void</span> * bindings[], <span class="type">const</span> <span class="type">char</span> * names[], <span class="type">int</span> nb_bindings)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    (<span class="type">void</span>)names;</span><br><span class="line">    (<span class="type">void</span>)nb_bindings;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!stream_.<span class="built_in">next</span>(scale_)) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpy</span>(</span><br><span class="line">        device_input_, stream_.<span class="built_in">getBatch</span>(), input_count_ * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice));</span><br><span class="line">    &#125; <span class="built_in">catch</span> (<span class="type">const</span> std::exception &amp; e) &#123;</span><br><span class="line">      <span class="comment">// Do nothing</span></span><br><span class="line">    &#125;</span><br><span class="line">    bindings[<span class="number">0</span>] = device_input_;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">const</span> <span class="type">void</span> * <span class="title">readCalibrationCache</span><span class="params">(<span class="type">size_t</span> &amp; length)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    calib_cache_.<span class="built_in">clear</span>();</span><br><span class="line">    <span class="function">std::ifstream <span class="title">input</span><span class="params">(calibration_cache_file_, std::ios::binary)</span></span>;</span><br><span class="line">    input &gt;&gt; std::noskipws;</span><br><span class="line">    <span class="keyword">if</span> (read_cache_ &amp;&amp; input.<span class="built_in">good</span>()) &#123;</span><br><span class="line">      std::<span class="built_in">copy</span>(</span><br><span class="line">        std::<span class="built_in">istream_iterator</span>&lt;<span class="type">char</span>&gt;(input), std::<span class="built_in">istream_iterator</span>&lt;<span class="type">char</span>&gt;(),</span><br><span class="line">        std::<span class="built_in">back_inserter</span>(calib_cache_));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    length = calib_cache_.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">if</span> (length) &#123;</span><br><span class="line">      std::cout &lt;&lt; <span class="string">&quot;Using cached calibration table to build the engine&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      std::cout &lt;&lt; <span class="string">&quot;New calibration table will be created to build the engine&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> length ? &amp;calib_cache_[<span class="number">0</span>] : <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">writeCalibrationCache</span><span class="params">(<span class="type">const</span> <span class="type">void</span> * cache, <span class="type">size_t</span> length)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="function">std::ofstream <span class="title">output</span><span class="params">(calibration_cache_file_, std::ios::binary)</span></span>;</span><br><span class="line">    output.<span class="built_in">write</span>(<span class="built_in">reinterpret_cast</span>&lt;<span class="type">const</span> <span class="type">char</span> *&gt;(cache), length);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  ImageStream stream_;</span><br><span class="line">  <span class="type">const</span> std::string calibration_cache_file_;</span><br><span class="line">  <span class="type">bool</span> read_cache_&#123;<span class="literal">true</span>&#125;;</span><br><span class="line">  <span class="type">size_t</span> input_count_;</span><br><span class="line">  <span class="type">void</span> * device_input_&#123;<span class="literal">nullptr</span>&#125;;</span><br><span class="line">  std::vector&lt;<span class="type">char</span>&gt; calib_cache_;</span><br><span class="line">  std::vector&lt;<span class="type">char</span>&gt; hist_cache_;</span><br><span class="line">  <span class="type">double</span> scale_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @class Int8MinMaxCalibrator</span></span><br><span class="line"><span class="comment"> * @brief Calibrator for MinMax</span></span><br><span class="line"><span class="comment"> * @warning We strongly recommand MinMax calibrator for YOLOX</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Int8MinMaxCalibrator</span> : <span class="keyword">public</span> nvinfer1::IInt8MinMaxCalibrator</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">Int8MinMaxCalibrator</span>(</span><br><span class="line">    ImageStream &amp; stream, <span class="type">const</span> std::string calibration_cache_file, <span class="type">double</span> scale = <span class="number">1.0</span>,</span><br><span class="line">    <span class="type">bool</span> read_cache = <span class="literal">true</span>)</span><br><span class="line">  : <span class="built_in">stream_</span>(stream), <span class="built_in">calibration_cache_file_</span>(calibration_cache_file), <span class="built_in">read_cache_</span>(read_cache)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">auto</span> d = stream_.<span class="built_in">getInputDims</span>();</span><br><span class="line">    input_count_ = stream_.<span class="built_in">getBatchSize</span>() * d.d[<span class="number">1</span>] * d.d[<span class="number">2</span>] * d.d[<span class="number">3</span>];</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMalloc</span>(&amp;device_input_, input_count_ * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    scale_ = scale;</span><br><span class="line">    <span class="keyword">auto</span> algType = <span class="built_in">getAlgorithm</span>();</span><br><span class="line">    <span class="keyword">switch</span> (algType) &#123;</span><br><span class="line">      <span class="built_in">case</span> (nvinfer1::CalibrationAlgoType::kLEGACY_CALIBRATION):</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;CalibratioAlgoType : kLEGACY_CALIBRATION&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="built_in">case</span> (nvinfer1::CalibrationAlgoType::kENTROPY_CALIBRATION):</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;CalibratioAlgoType : kENTROPY_CALIBRATION&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="built_in">case</span> (nvinfer1::CalibrationAlgoType::kENTROPY_CALIBRATION_2):</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;CalibratioAlgoType : kENTROPY_CALIBRATION_2&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="built_in">case</span> (nvinfer1::CalibrationAlgoType::kMINMAX_CALIBRATION):</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;CalibratioAlgoType : kMINMAX_CALIBRATION&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">      <span class="keyword">default</span>:</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;No CalibrationAlgType&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="type">int</span> <span class="title">getBatchSize</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span> </span>&#123; <span class="keyword">return</span> stream_.<span class="built_in">getBatchSize</span>(); &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">virtual</span> ~<span class="built_in">Int8MinMaxCalibrator</span>() &#123; <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(device_input_)); &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">getBatch</span><span class="params">(<span class="type">void</span> * bindings[], <span class="type">const</span> <span class="type">char</span> * names[], <span class="type">int</span> nb_bindings)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    (<span class="type">void</span>)names;</span><br><span class="line">    (<span class="type">void</span>)nb_bindings;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!stream_.<span class="built_in">next</span>(scale_)) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpy</span>(</span><br><span class="line">        device_input_, stream_.<span class="built_in">getBatch</span>(), input_count_ * <span class="built_in">sizeof</span>(<span class="type">float</span>), cudaMemcpyHostToDevice));</span><br><span class="line">    &#125; <span class="built_in">catch</span> (<span class="type">const</span> std::exception &amp; e) &#123;</span><br><span class="line">      <span class="comment">// Do nothing</span></span><br><span class="line">    &#125;</span><br><span class="line">    bindings[<span class="number">0</span>] = device_input_;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">const</span> <span class="type">void</span> * <span class="title">readCalibrationCache</span><span class="params">(<span class="type">size_t</span> &amp; length)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    calib_cache_.<span class="built_in">clear</span>();</span><br><span class="line">    <span class="function">std::ifstream <span class="title">input</span><span class="params">(calibration_cache_file_, std::ios::binary)</span></span>;</span><br><span class="line">    input &gt;&gt; std::noskipws;</span><br><span class="line">    <span class="keyword">if</span> (read_cache_ &amp;&amp; input.<span class="built_in">good</span>()) &#123;</span><br><span class="line">      std::<span class="built_in">copy</span>(</span><br><span class="line">        std::<span class="built_in">istream_iterator</span>&lt;<span class="type">char</span>&gt;(input), std::<span class="built_in">istream_iterator</span>&lt;<span class="type">char</span>&gt;(),</span><br><span class="line">        std::<span class="built_in">back_inserter</span>(calib_cache_));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    length = calib_cache_.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">if</span> (length) &#123;</span><br><span class="line">      std::cout &lt;&lt; <span class="string">&quot;Using cached calibration table to build the engine&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      std::cout &lt;&lt; <span class="string">&quot;New calibration table will be created to build the engine&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> length ? &amp;calib_cache_[<span class="number">0</span>] : <span class="literal">nullptr</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">writeCalibrationCache</span><span class="params">(<span class="type">const</span> <span class="type">void</span> * cache, <span class="type">size_t</span> length)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    <span class="function">std::ofstream <span class="title">output</span><span class="params">(calibration_cache_file_, std::ios::binary)</span></span>;</span><br><span class="line">    output.<span class="built_in">write</span>(<span class="built_in">reinterpret_cast</span>&lt;<span class="type">const</span> <span class="type">char</span> *&gt;(cache), length);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  ImageStream stream_;</span><br><span class="line">  <span class="type">const</span> std::string calibration_cache_file_;</span><br><span class="line">  <span class="type">bool</span> read_cache_&#123;<span class="literal">true</span>&#125;;</span><br><span class="line">  <span class="type">size_t</span> input_count_;</span><br><span class="line">  <span class="type">void</span> * device_input_&#123;<span class="literal">nullptr</span>&#125;;</span><br><span class="line">  std::vector&lt;<span class="type">char</span>&gt; calib_cache_;</span><br><span class="line">  std::vector&lt;<span class="type">char</span>&gt; hist_cache_;</span><br><span class="line">  <span class="type">double</span> scale_;</span><br><span class="line">&#125;;</span><br><span class="line">&#125;  <span class="comment">// namespace tensorrt_yolox</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span>  <span class="comment">// TENSORRT_YOLOX__CALIBRATOR_HPP_</span></span></span><br></pre></td></tr></table></figure>
<p>对应的调用代码如下</p>
<p>其中<code>calibration_image_list_file</code>是一个txt文件，里面包含了量化需要的文件路径文件内容类似</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/mnt/data/dataset/ImageNet/ILSVRC2012/test/ILSVRC2012_test_00000001.JPEG</span><br><span class="line">/mnt/data/dataset/ImageNet/ILSVRC2012/test/ILSVRC2012_test_00000002.JPEG</span><br><span class="line">/mnt/data/dataset/ImageNet/ILSVRC2012/test/ILSVRC2012_test_00000003.JPEG</span><br><span class="line">/mnt/data/dataset/ImageNet/ILSVRC2012/test/ILSVRC2012_test_00000004.JPEG</span><br><span class="line">/mnt/data/dataset/ImageNet/ILSVRC2012/test/ILSVRC2012_test_00000005.JPEG</span><br><span class="line">/mnt/data/dataset/ImageNet/ILSVRC2012/test/ILSVRC2012_test_00000006.JPEG</span><br><span class="line">/mnt/data/dataset/ImageNet/ILSVRC2012/test/ILSVRC2012_test_00000007.JPEG</span><br><span class="line">/mnt/data/dataset/ImageNet/ILSVRC2012/test/ILSVRC2012_test_00000008.JPEG</span><br><span class="line">/mnt/data/dataset/ImageNet/ILSVRC2012/test/ILSVRC2012_test_00000009.JPEG</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> max_batch_size = <span class="number">1</span>;</span><br><span class="line">  <span class="type">float</span> cutoff = <span class="number">1.0</span>;</span><br><span class="line">  nvinfer1::Dims input_dims = tensorrt_common::<span class="built_in">get_input_dims</span>(model_path);</span><br><span class="line">  std::vector&lt;std::string&gt; calibration_images = <span class="built_in">loadImageList</span>(calibration_image_list_file, <span class="string">&quot;&quot;</span>);</span><br><span class="line">  <span class="function">tensorrt_yolox::ImageStream <span class="title">stream</span><span class="params">(max_batch_size, input_dims, calibration_images)</span></span>;</span><br><span class="line">  fs::path calibration_table&#123;model_path&#125;;</span><br><span class="line">  std::string calibName = <span class="string">&quot;&quot;</span>;</span><br><span class="line">  std::string ext = <span class="string">&quot;&quot;</span>;</span><br><span class="line">  <span class="keyword">if</span> (build_config.calib_type_str == <span class="string">&quot;Entropy&quot;</span>) &#123;</span><br><span class="line">    ext = <span class="string">&quot;EntropyV2-&quot;</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (</span><br><span class="line">    build_config.calib_type_str == <span class="string">&quot;Legacy&quot;</span> || build_config.calib_type_str == <span class="string">&quot;Percentile&quot;</span>) &#123;</span><br><span class="line">    ext = <span class="string">&quot;Legacy-&quot;</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    ext = <span class="string">&quot;MinMax-&quot;</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ext += <span class="string">&quot;calibration.table&quot;</span>;</span><br><span class="line">  calibration_table.<span class="built_in">replace_extension</span>(ext);</span><br><span class="line">  fs::path histogram_table&#123;model_path&#125;;</span><br><span class="line">  ext = <span class="string">&quot;histogram.table&quot;</span>;</span><br><span class="line">  histogram_table.<span class="built_in">replace_extension</span>(ext);</span><br><span class="line"></span><br><span class="line">  std::unique_ptr&lt;nvinfer1::IInt8Calibrator&gt; calibrator;</span><br><span class="line">  <span class="keyword">if</span> (build_config.calib_type_str == <span class="string">&quot;Entropy&quot;</span>) &#123;</span><br><span class="line">    calibrator.<span class="built_in">reset</span>(</span><br><span class="line">      <span class="keyword">new</span> tensorrt_yolox::<span class="built_in">Int8EntropyCalibrator</span>(stream, calibration_table, norm_factor_));</span><br><span class="line"></span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (</span><br><span class="line">    build_config.calib_type_str == <span class="string">&quot;Legacy&quot;</span> || build_config.calib_type_str == <span class="string">&quot;Percentile&quot;</span>) &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">double</span> quantile = <span class="number">0.999999</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">double</span> cutoff = <span class="number">0.999999</span>;</span><br><span class="line">    calibrator.<span class="built_in">reset</span>(<span class="keyword">new</span> tensorrt_yolox::<span class="built_in">Int8LegacyCalibrator</span>(</span><br><span class="line">      stream, calibration_table, histogram_table, norm_factor_, <span class="literal">true</span>, quantile, cutoff));</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    calibrator.<span class="built_in">reset</span>(</span><br><span class="line">      <span class="keyword">new</span> tensorrt_yolox::<span class="built_in">Int8MinMaxCalibrator</span>(stream, calibration_table, norm_factor_));</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h2 id="2-2-python-onnx转trt"><a href="#2-2-python-onnx转trt" class="headerlink" title="2.2 python onnx转trt"></a>2.2 <strong>python onnx转trt</strong></h2><ul>
<li><strong>操作流程</strong>：按照常规方案导出onnx，onnx序列化为tensorrt engine之前打开int8量化模式并采用校正数据集进行校正；</li>
<li><strong>优点</strong>：<ul>
<li>导出onnx之前的所有操作都为常规操作；</li>
<li>相比在pytorch中进行PTQ int8量化，所需显存小；</li>
</ul>
</li>
<li><strong>缺点</strong>：<ul>
<li>量化过程为黑盒子，无法看到中间过程；</li>
<li>校正过程需在实际运行的tensorrt版本中进行并保存tensorrt  engine；</li>
<li>量化过程中发现，即使模型为动态输入，校正数据集使用时也必须与推理时的输入shape[N, C, H,  W]完全一致，否则，效果非常非常差，动态模型慎用。</li>
</ul>
</li>
<li>操作示例参看<a href="https://link.zhihu.com/?target=https%3A//github.com/Susan19900316/yolov5_tensorrt_int8/blob/master/onnx2trt_ptq.py">onnx2trt_ptq.py</a></li>
</ul>
<p>下面的代码其实就是上面的2.1.1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pycuda.driver <span class="keyword">as</span> cuda</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_crop_bbox</span>(<span class="params">img, crop_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Randomly get a crop bounding box.&quot;&quot;&quot;</span></span><br><span class="line">    margin_h = <span class="built_in">max</span>(img.shape[<span class="number">0</span>] - crop_size[<span class="number">0</span>], <span class="number">0</span>)</span><br><span class="line">    margin_w = <span class="built_in">max</span>(img.shape[<span class="number">1</span>] - crop_size[<span class="number">1</span>], <span class="number">0</span>)</span><br><span class="line">    offset_h = np.random.randint(<span class="number">0</span>, margin_h + <span class="number">1</span>)</span><br><span class="line">    offset_w = np.random.randint(<span class="number">0</span>, margin_w + <span class="number">1</span>)</span><br><span class="line">    crop_y1, crop_y2 = offset_h, offset_h + crop_size[<span class="number">0</span>]</span><br><span class="line">    crop_x1, crop_x2 = offset_w, offset_w + crop_size[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> crop_x1, crop_y1, crop_x2, crop_y2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">crop</span>(<span class="params">img, crop_bbox</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Crop from ``img``&quot;&quot;&quot;</span></span><br><span class="line">    crop_x1, crop_y1, crop_x2, crop_y2 = crop_bbox</span><br><span class="line">    img = img[crop_y1:crop_y2, crop_x1:crop_x2, ...]</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">yolov5EntropyCalibrator</span>(trt.IInt8EntropyCalibrator2):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, imgpath, batch_size, channel, inputsize=[<span class="number">384</span>, <span class="number">1280</span>]</span>):</span><br><span class="line">        trt.IInt8EntropyCalibrator2.__init__(self)</span><br><span class="line">        self.cache_file = <span class="string">&#x27;yolov5.cache&#x27;</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.Channel = channel</span><br><span class="line">        self.height = inputsize[<span class="number">0</span>]</span><br><span class="line">        self.width = inputsize[<span class="number">1</span>]</span><br><span class="line">        self.imgs = [os.path.join(imgpath, file) <span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(imgpath) <span class="keyword">if</span> file.endswith(<span class="string">&#x27;jpg&#x27;</span>)]</span><br><span class="line">        np.random.shuffle(self.imgs)</span><br><span class="line">        self.imgs = self.imgs[:<span class="number">2000</span>]</span><br><span class="line">        self.batch_idx = <span class="number">0</span></span><br><span class="line">        self.max_batch_idx = <span class="built_in">len</span>(self.imgs) // self.batch_size</span><br><span class="line">        self.calibration_data = np.zeros((self.batch_size, <span class="number">3</span>, self.height, self.width), dtype=np.float32)</span><br><span class="line">        <span class="comment"># self.data_size = trt.volume([self.batch_size, self.Channel, self.height, self.width]) * trt.float32.itemsize</span></span><br><span class="line">        self.data_size = self.calibration_data.nbytes</span><br><span class="line">        self.device_input = cuda.mem_alloc(self.data_size)</span><br><span class="line">        <span class="comment"># self.device_input = cuda.mem_alloc(self.calibration_data.nbytes)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">free</span>(<span class="params">self</span>):</span><br><span class="line">        self.device_input.free()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_batch_size</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.batch_size</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_batch</span>(<span class="params">self, names, p_str=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            batch_imgs = self.next_batch()</span><br><span class="line">            <span class="keyword">if</span> batch_imgs.size == <span class="number">0</span> <span class="keyword">or</span> batch_imgs.size != self.batch_size * self.Channel * self.height * self.width:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">            cuda.memcpy_htod(self.device_input, batch_imgs)</span><br><span class="line">            <span class="keyword">return</span> [self.device_input]</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;wrong&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">next_batch</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.batch_idx &lt; self.max_batch_idx:</span><br><span class="line">            batch_files = self.imgs[self.batch_idx * self.batch_size: \</span><br><span class="line">                                    (self.batch_idx + <span class="number">1</span>) * self.batch_size]</span><br><span class="line">            batch_imgs = np.zeros((self.batch_size, self.Channel, self.height, self.width),</span><br><span class="line">                                  dtype=np.float32)</span><br><span class="line">            <span class="keyword">for</span> i, f <span class="keyword">in</span> <span class="built_in">enumerate</span>(batch_files):</span><br><span class="line">                img = cv2.imread(f)  <span class="comment"># BGR</span></span><br><span class="line">                crop_size = [self.height, self.width]</span><br><span class="line">                crop_bbox = get_crop_bbox(img, crop_size)</span><br><span class="line">                <span class="comment"># crop the image</span></span><br><span class="line">                img = crop(img, crop_bbox)</span><br><span class="line">                img = img.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))[::-<span class="number">1</span>, :, :]  <span class="comment"># BHWC to BCHW ,BGR to RGB</span></span><br><span class="line">                img = np.ascontiguousarray(img)</span><br><span class="line">                img = img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line">                <span class="keyword">assert</span> (img.nbytes == self.data_size / self.batch_size), <span class="string">&#x27;not valid img!&#x27;</span> + f</span><br><span class="line">                batch_imgs[i] = img</span><br><span class="line">            self.batch_idx += <span class="number">1</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;batch:[&#123;&#125;/&#123;&#125;]&quot;</span>.<span class="built_in">format</span>(self.batch_idx, self.max_batch_idx))</span><br><span class="line">            <span class="keyword">return</span> np.ascontiguousarray(batch_imgs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> np.array([])</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">read_calibration_cache</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># If there is a cache, use it instead of calibrating again. Otherwise, implicitly return None.</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(self.cache_file):</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(self.cache_file, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                <span class="keyword">return</span> f.read()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">write_calibration_cache</span>(<span class="params">self, cache</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(self.cache_file, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(cache)</span><br><span class="line">            f.flush()</span><br><span class="line">            <span class="comment"># os.fsync(f)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_engine</span>(<span class="params">onnx_file_path, engine_file_path, cali_img, mode=<span class="string">&#x27;FP32&#x27;</span>, workspace_size=<span class="number">4096</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Attempts to load a serialized engine if available, otherwise builds a new TensorRT engine and saves it.&quot;&quot;&quot;</span></span><br><span class="line">    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_engine</span>():</span><br><span class="line">        <span class="keyword">assert</span> mode.lower() <span class="keyword">in</span> [<span class="string">&#x27;fp32&#x27;</span>, <span class="string">&#x27;fp16&#x27;</span>, <span class="string">&#x27;int8&#x27;</span>], <span class="string">&quot;mode should be in [&#x27;fp32&#x27;, &#x27;fp16&#x27;, &#x27;int8&#x27;]&quot;</span></span><br><span class="line">        explicit_batch_flag = <span class="number">1</span> &lt;&lt; <span class="built_in">int</span>(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)</span><br><span class="line">        <span class="keyword">with</span> trt.Builder(TRT_LOGGER) <span class="keyword">as</span> builder, builder.create_network(</span><br><span class="line">            explicit_batch_flag</span><br><span class="line">        ) <span class="keyword">as</span> network, builder.create_builder_config() <span class="keyword">as</span> config, trt.OnnxParser(</span><br><span class="line">            network, TRT_LOGGER</span><br><span class="line">        ) <span class="keyword">as</span> parser:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(onnx_file_path, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> model:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Beginning ONNX file parsing&quot;</span>)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> parser.parse(model.read()):</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;ERROR: Failed to parse the ONNX file.&quot;</span>)</span><br><span class="line">                    <span class="keyword">for</span> error <span class="keyword">in</span> <span class="built_in">range</span>(parser.num_errors):</span><br><span class="line">                        <span class="built_in">print</span>(parser.get_error(error))</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">            config.max_workspace_size = workspace_size * (<span class="number">1024</span> * <span class="number">1024</span>)  <span class="comment"># workspace_sizeMiB</span></span><br><span class="line">            <span class="comment"># 构建精度</span></span><br><span class="line">            <span class="keyword">if</span> mode.lower() == <span class="string">&#x27;fp16&#x27;</span>:</span><br><span class="line">                config.flags |= <span class="number">1</span> &lt;&lt; <span class="built_in">int</span>(trt.BuilderFlag.FP16)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> mode.lower() == <span class="string">&#x27;int8&#x27;</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;trt.DataType.INT8&#x27;</span>)</span><br><span class="line">                config.flags |= <span class="number">1</span> &lt;&lt; <span class="built_in">int</span>(trt.BuilderFlag.INT8)</span><br><span class="line">                config.flags |= <span class="number">1</span> &lt;&lt; <span class="built_in">int</span>(trt.BuilderFlag.FP16)</span><br><span class="line">                calibrator = yolov5EntropyCalibrator(cali_img, <span class="number">26</span>, <span class="number">3</span>, [<span class="number">384</span>, <span class="number">1280</span>])</span><br><span class="line">                <span class="comment"># config.set_quantization_flag(trt.QuantizationFlag.CALIBRATE_BEFORE_FUSION)</span></span><br><span class="line">                config.int8_calibrator = calibrator</span><br><span class="line">            <span class="comment"># if True:</span></span><br><span class="line">            <span class="comment">#     config.profiling_verbosity = trt.ProfilingVerbosity.DETAILED</span></span><br><span class="line"></span><br><span class="line">            profile = builder.create_optimization_profile()</span><br><span class="line">            profile.set_shape(network.get_input(<span class="number">0</span>).name, <span class="built_in">min</span>=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">384</span>, <span class="number">1280</span>), opt=(<span class="number">12</span>, <span class="number">3</span>, <span class="number">384</span>, <span class="number">1280</span>), <span class="built_in">max</span>=(<span class="number">26</span>, <span class="number">3</span>, <span class="number">384</span>, <span class="number">1280</span>))</span><br><span class="line">            config.add_optimization_profile(profile)</span><br><span class="line">            <span class="comment"># config.set_calibration_profile(profile)</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Completed parsing of ONNX file&quot;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Building an engine from file &#123;&#125;; this may take a while...&quot;</span>.<span class="built_in">format</span>(onnx_file_path))</span><br><span class="line">            <span class="comment"># plan = builder.build_serialized_network(network, config)</span></span><br><span class="line">            <span class="comment"># engine = runtime.deserialize_cuda_engine(plan)</span></span><br><span class="line">            engine = builder.build_engine(network,config)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Completed creating Engine&quot;</span>)</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(engine_file_path, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                <span class="comment"># f.write(plan)</span></span><br><span class="line">                f.write(engine.serialize())</span><br><span class="line">            <span class="keyword">return</span> engine</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> os.path.exists(engine_file_path):</span><br><span class="line">        <span class="comment"># If a serialized engine exists, use it instead of building an engine.</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Reading engine from file &#123;&#125;&quot;</span>.<span class="built_in">format</span>(engine_file_path))</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(engine_file_path, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f, trt.Runtime(TRT_LOGGER) <span class="keyword">as</span> runtime:</span><br><span class="line">            <span class="keyword">return</span> runtime.deserialize_cuda_engine(f.read())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> build_engine()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">onnx_file_path, engine_file_path, cali_img_path, mode=<span class="string">&#x27;FP32&#x27;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create a TensorRT engine for ONNX-based YOLOv3-608 and run inference.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Try to load a previously generated YOLOv3-608 network graph in ONNX format:</span></span><br><span class="line">    get_engine(onnx_file_path, engine_file_path, cali_img_path, mode)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    onnx_file_path = <span class="string">&#x27;/home/models/boatdetect_yolov5/last_nms_dynamic.onnx&#x27;</span></span><br><span class="line">    engine_file_path = <span class="string">&quot;/home/models/boatdetect_yolov5/last_nms_dynamic_onnx2trtptq.plan&quot;</span></span><br><span class="line">    cali_img_path = <span class="string">&#x27;/home/data/frontview/test&#x27;</span></span><br><span class="line">    main(onnx_file_path, engine_file_path, cali_img_path, mode=<span class="string">&#x27;int8&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="2-3-polygraphy工具"><a href="#2-3-polygraphy工具" class="headerlink" title="2.3 polygraphy工具"></a>2.3 <strong>polygraphy工具</strong></h2><ul>
<li><strong>操作流程：</strong>按照常规方案导出onnx，onnx序列化为tensorrt engine之前打开int8量化模式并采用校正数据集进行校正；</li>
<li><strong>优点：</strong>1. 相较于1.1，代码量更少，只需完成校正数据的处理代码；</li>
<li><strong>缺点</strong>：1<ul>
<li>同上所有; </li>
<li>动态尺寸时，校正数据需与–trt-opt-shapes相同</li>
<li>内部默认最多校正20个epoch；</li>
</ul>
</li>
<li>安装polygraphy</li>
</ul>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install colored polygraphy --extra-index-url https://pypi.ngc.nvidia.com</span><br></pre></td></tr></table></figure>
<ul>
<li>量化</li>
</ul>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">polygraphy convert XX.onnx --int8 --data-loader-script loader_data.py --calibration-cache XX.cache -o XX.plan --trt-min-shapes images:[1,3,384,1280] --trt-opt-shapes images:[26,3,384,1280] --trt-max-shapes images:[26,3,384,1280] #量化</span><br></pre></td></tr></table></figure>
<ul>
<li><a href="https://link.zhihu.com/?target=https%3A//github.com/Susan19900316/yolov5_tensorrt_int8/blob/master/loader_data.py">loader_data.py</a>为较正数据集加载过程，自动调用脚本中的load_data()函数：</li>
</ul>
<h2 id="2-4-pytorch中执行-推荐"><a href="#2-4-pytorch中执行-推荐" class="headerlink" title="2.4 pytorch中执行(推荐)"></a>2.4 pytorch中执行(推荐)</h2><p>实际上是使用<code>pytorch-quantization</code> PyTorch 中进行量化（Quantization）的库，支持PTQ和QAT的量化。这里给出就是PTQ的例子</p>
<blockquote>
<p>注：在pytorch中执行导出的onnx将产生一个明确量化的模型，属于显示量化</p>
</blockquote>
<ul>
<li><p><strong>操作流程：</strong>安装pytorch_quantization库-&gt;加载校正数据-&gt;加载模型（在加载模型之前，启用quant_modules.initialize() 以保证原始模型层替换为量化层）-&gt;校正-&gt;导出onnx;</p>
</li>
<li><p><strong>优点：</strong></p>
<ul>
<li>通过导出的onnx能够看到每层量化的过程；</li>
<li>onnx导出为tensort  engine时可以采用trtexec(注：命令行需加–int8，需要fp16和int8混合精度时，再添加–fp16，这里有些疑问，GPT说导出 ONNX 模型时进行了量化，那么在使用 <code>trtexec</code> 转换为 TensorRT Engine 时，你不需要添加任何特别的参数。因为 ONNX 模型中已经包含了量化后的信息，TensorRT 在转换过程中会自动识别并保留这些信息。因此不知道是不是需要<code>--int8</code>，我感觉不需要了。)，比较简单；</li>
<li>pytorch校正过程可在任意设备中进行；</li>
<li>相较上述方法，校正数据集使用shape无需与推理shape一致，也能获得较好的结果，动态输入时，推荐采用此种方式。</li>
</ul>
</li>
<li><p><strong>缺点：</strong>导出onnx时，显存占用非常大；</p>
</li>
<li><p>操作示例参看：pytorch模型进行量化导出<a href="https://link.zhihu.com/?target=https%3A//github.com/Susan19900316/yolov5_tensorrt_int8/blob/master/pytorch_yolov5_ptq.py">yolov5_pytorch_ptq.py</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models, datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pytorch_quantization</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization <span class="keyword">import</span> nn <span class="keyword">as</span> quant_nn</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization <span class="keyword">import</span> quant_modules</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization <span class="keyword">import</span> calib</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pytorch_quantization.__version__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> wget</span><br><span class="line"><span class="keyword">import</span> tarfile</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> models.yolo <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> models.experimental <span class="keyword">import</span> End2End</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_amax</span>(<span class="params">model, **kwargs</span>):</span><br><span class="line">    <span class="comment"># Load calib result</span></span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(module._calibrator, calib.MaxCalibrator):</span><br><span class="line">                    module.load_calib_amax()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    module.load_calib_amax(**kwargs)</span><br><span class="line">    model.cuda()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collect_stats</span>(<span class="params">model, data_loader</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Feed data to the network and collect statistics&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Enable calibrators</span></span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                module.disable_quant()</span><br><span class="line">                module.enable_calib()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                module.disable()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Feed data to the network for collecting stats</span></span><br><span class="line">    <span class="keyword">for</span> i, image <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(data_loader)):</span><br><span class="line">        model(image.cuda())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Disable calibrators</span></span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                module.enable_quant()</span><br><span class="line">                module.disable_calib()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                module.enable()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_crop_bbox</span>(<span class="params">img, crop_size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Randomly get a crop bounding box.&quot;&quot;&quot;</span></span><br><span class="line">    margin_h = <span class="built_in">max</span>(img.shape[<span class="number">0</span>] - crop_size[<span class="number">0</span>], <span class="number">0</span>)</span><br><span class="line">    margin_w = <span class="built_in">max</span>(img.shape[<span class="number">1</span>] - crop_size[<span class="number">1</span>], <span class="number">0</span>)</span><br><span class="line">    offset_h = np.random.randint(<span class="number">0</span>, margin_h + <span class="number">1</span>)</span><br><span class="line">    offset_w = np.random.randint(<span class="number">0</span>, margin_w + <span class="number">1</span>)</span><br><span class="line">    crop_y1, crop_y2 = offset_h, offset_h + crop_size[<span class="number">0</span>]</span><br><span class="line">    crop_x1, crop_x2 = offset_w, offset_w + crop_size[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> crop_x1, crop_y1, crop_x2, crop_y2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">crop</span>(<span class="params">img, crop_bbox</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Crop from ``img``&quot;&quot;&quot;</span></span><br><span class="line">    crop_x1, crop_y1, crop_x2, crop_y2 = crop_bbox</span><br><span class="line">    img = img[crop_y1:crop_y2, crop_x1:crop_x2, ...]</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CaliData</span>(data.Dataset):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, path, num, inputsize=[<span class="number">384</span>, <span class="number">1280</span>]</span>):</span><br><span class="line">        self.img_files = [os.path.join(path, p) <span class="keyword">for</span> p <span class="keyword">in</span> os.listdir(path) <span class="keyword">if</span> p.endswith(<span class="string">&#x27;jpg&#x27;</span>)]</span><br><span class="line">        random.shuffle(self.img_files)</span><br><span class="line">        self.img_files = self.img_files[:num]</span><br><span class="line">        self.height = inputsize[<span class="number">0</span>]</span><br><span class="line">        self.width = inputsize[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        f = self.img_files[index]</span><br><span class="line">        img = cv2.imread(f)  <span class="comment"># BGR</span></span><br><span class="line">        crop_size = [self.height, self.width]</span><br><span class="line">        crop_bbox = get_crop_bbox(img, crop_size)</span><br><span class="line">        <span class="comment"># crop the image</span></span><br><span class="line">        img = crop(img, crop_bbox)</span><br><span class="line">        img = img.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))[::-<span class="number">1</span>, :, :]  <span class="comment"># BHWC to BCHW ,BGR to RGB</span></span><br><span class="line">        img = np.ascontiguousarray(img)</span><br><span class="line">        img = img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line">        <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_files)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    pt_file = <span class="string">&#x27;runs/train/exp/weights/best.pt&#x27;</span></span><br><span class="line">    calib_path = <span class="string">&#x27;XX/train&#x27;</span></span><br><span class="line">    num = <span class="number">2000</span> <span class="comment"># 用来校正的数目</span></span><br><span class="line">    batchsize = <span class="number">4</span></span><br><span class="line">    <span class="comment"># 准备数据</span></span><br><span class="line">    dataset = CaliData(calib_path, num)</span><br><span class="line">    dataloader = data.DataLoader(dataset, batch_size=batchsize)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型加载</span></span><br><span class="line">    quant_modules.initialize() <span class="comment">#保证原始模型层替换为量化层</span></span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">    ckpt = torch.load(pt_file, map_location=<span class="string">&#x27;cpu&#x27;</span>)  <span class="comment"># load checkpoint to CPU to avoid CUDA memory leak</span></span><br><span class="line">    <span class="comment"># QAT</span></span><br><span class="line">    q_model = ckpt[<span class="string">&#x27;model&#x27;</span>]</span><br><span class="line">    yaml = ckpt[<span class="string">&#x27;model&#x27;</span>].yaml</span><br><span class="line">    q_model = Model(yaml, ch=yaml[<span class="string">&#x27;ch&#x27;</span>], nc=yaml[<span class="string">&#x27;nc&#x27;</span>]).to(device)  <span class="comment"># creat</span></span><br><span class="line">    q_model.<span class="built_in">eval</span>()</span><br><span class="line">    q_model = End2End(q_model).cuda()</span><br><span class="line">    ckpt = ckpt[<span class="string">&#x27;model&#x27;</span>]</span><br><span class="line">    modified_state_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> key, val <span class="keyword">in</span> ckpt.state_dict().items():</span><br><span class="line">        <span class="comment"># Remove &#x27;module.&#x27; from the key names</span></span><br><span class="line">        <span class="keyword">if</span> key.startswith(<span class="string">&#x27;module&#x27;</span>):</span><br><span class="line">            modified_state_dict[key[<span class="number">7</span>:]] = val</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            modified_state_dict[key] = val</span><br><span class="line">    q_model.model.load_state_dict(modified_state_dict)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calibrate the model using calibration technique.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        collect_stats(q_model, dataloader)</span><br><span class="line">        compute_amax(q_model, method=<span class="string">&quot;entropy&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set static member of TensorQuantizer to use Pytorch’s own fake quantization functions</span></span><br><span class="line">    quant_nn.TensorQuantizer.use_fb_fake_quant = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Exporting to ONNX</span></span><br><span class="line">    dummy_input = torch.randn(<span class="number">26</span>, <span class="number">3</span>, <span class="number">384</span>, <span class="number">1280</span>, device=<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    input_names = [<span class="string">&quot;images&quot;</span>]</span><br><span class="line">    output_names = [<span class="string">&quot;num_dets&quot;</span>, <span class="string">&#x27;det_boxes&#x27;</span>]</span><br><span class="line">    <span class="comment"># output_names = [&#x27;outputs&#x27;]</span></span><br><span class="line">    save_path = <span class="string">&#x27;/&#x27;</span>.join(pt_file.split(<span class="string">&#x27;/&#x27;</span>)[:-<span class="number">1</span>])</span><br><span class="line">    onnx_file = os.path.join(save_path, <span class="string">&#x27;best_ptq.onnx&#x27;</span>)</span><br><span class="line">    dynamic = <span class="built_in">dict</span>()</span><br><span class="line">    dynamic[<span class="string">&#x27;images&#x27;</span>] = &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>&#125;</span><br><span class="line">    dynamic[<span class="string">&#x27;num_dets&#x27;</span>] = &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>&#125;</span><br><span class="line">    dynamic[<span class="string">&#x27;det_boxes&#x27;</span>] = &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>&#125;</span><br><span class="line">    torch.onnx.export(</span><br><span class="line">        q_model,</span><br><span class="line">        dummy_input,</span><br><span class="line">        onnx_file,</span><br><span class="line">        verbose=<span class="literal">False</span>,</span><br><span class="line">        opset_version=<span class="number">13</span>,</span><br><span class="line">        do_constant_folding=<span class="literal">False</span>,</span><br><span class="line">        input_names=input_names,</span><br><span class="line">        output_names=output_names,</span><br><span class="line">        dynamic_axes=dynamic)</span><br></pre></td></tr></table></figure>
<p>上面的代码，生成的 ONNX 模型是已经量化过的。以下是代码中的量化过程：</p>
<ol>
<li><strong>导入 PyTorch Quantization 库</strong>：<ul>
<li>通过 <code>import pytorch_quantization</code> 以及其他相关模块的导入，使用了 PyTorch Quantization 库中的功能。</li>
</ul>
</li>
<li><strong>量化模型</strong>：<ul>
<li>在加载模型后，执行了量化操作。在 <code>__main__</code> 中，通过 <code>collect_stats</code> 和 <code>compute_amax</code> 函数执行了量化统计和计算最大值的操作。这是典型的 QAT（Quantization Aware Training）过程，其中使用校准数据集来估计量化参数。</li>
<li>在执行 <code>compute_amax</code> 函数时，传递了 <code>method=&quot;entropy&quot;</code> 参数，这表明使用的是熵方法来计算量化参数。</li>
<li>最后，通过 <code>torch.onnx.export</code> 函数将量化后的模型导出为 ONNX 格式。</li>
</ul>
</li>
</ol>
</li>
</ul>
<h1 id="3-QAT"><a href="#3-QAT" class="headerlink" title="3 QAT"></a>3 QAT</h1><p>实际上是使用<code>pytorch-quantization</code> PyTorch 中进行量化（Quantization）的库，支持PTQ和QAT的量化。这里给出就是QAT的例子</p>
<blockquote>
<p>注：在pytorch中执行导出的onnx将产生一个明确量化的模型，属于显式量化</p>
</blockquote>
<ul>
<li><strong>操作流程：</strong>安装pytorch_quantization库-&gt;加载训练数据-&gt;加载模型（在加载模型之前，启用quant_modules.initialize() 以保证原始模型层替换为量化层）-&gt;训练-&gt;导出onnx;</li>
<li><strong>优点：</strong>1. 模型量化参数重新训练，训练较好时，精度下降较少； 2. 通过导出的onnx能够看到每层量化的过程；2. onnx导出为tensort  engine时可以采用trtexec(注：命令行需加–int8，需要fp16和int8混合精度时，再添加–fp16)，比较简单；3.训练过程可在任意设备中进行；</li>
<li><strong>缺点：</strong>1.导出onnx时，显存占用非常大；2.最终精度取决于训练好坏；3. QAT训练shape需与推理shape一致才能获得好的推理结果；4. 导出onnx时需采用真实的图片输入作为输入设置</li>
<li>操作示例参看<a href="https://link.zhihu.com/?target=https%3A//github.com/Susan19900316/yolov5_tensorrt_int8/blob/master/pytorch_yolov5_qat.py">yolov5_pytorch_qat.py</a>感知训练，参看<a href="https://link.zhihu.com/?target=https%3A//github.com/Susan19900316/yolov5_tensorrt_int8/blob/master/onnx2trt_ptq.py">export_onnx_qat.py</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pytorch_quantization</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization <span class="keyword">import</span> nn <span class="keyword">as</span> quant_nn</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pytorch_quantization.__version__)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> models.experimental <span class="keyword">import</span> End2End</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    pt_file = <span class="string">&#x27;runs/train/exp/weights/best.pt&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型加载</span></span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">    ckpt = torch.load(pt_file, map_location=<span class="string">&#x27;cpu&#x27;</span>)  <span class="comment"># load checkpoint to CPU to avoid CUDA memory leak</span></span><br><span class="line">    q_model = ckpt[<span class="string">&#x27;model&#x27;</span>]</span><br><span class="line">    q_model.<span class="built_in">eval</span>()</span><br><span class="line">    q_model = End2End(q_model).cuda().<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set static member of TensorQuantizer to use Pytorch’s own fake quantization functions</span></span><br><span class="line">    quant_nn.TensorQuantizer.use_fb_fake_quant = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Exporting to ONNX</span></span><br><span class="line">    <span class="comment"># dummy_input = torch.randn(26, 3, 384, 1280, device=&#x27;cuda&#x27;)</span></span><br><span class="line">    im = np.load(<span class="string">&#x27;im.npy&#x27;</span>) <span class="comment"># 重要：真实图像</span></span><br><span class="line">    dummy_input = torch.from_numpy(im).cuda()</span><br><span class="line">    dummy_input = dummy_input.<span class="built_in">float</span>()</span><br><span class="line">    dummy_input = dummy_input / <span class="number">255</span></span><br><span class="line">    input_names = [<span class="string">&quot;images&quot;</span>]</span><br><span class="line">    output_names = [<span class="string">&#x27;num_dets&#x27;</span>, <span class="string">&#x27;det_boxes&#x27;</span>]</span><br><span class="line">    save_path = <span class="string">&#x27;/&#x27;</span>.join(pt_file.split(<span class="string">&#x27;/&#x27;</span>)[:-<span class="number">1</span>])</span><br><span class="line">    onnx_file = os.path.join(save_path, <span class="string">&#x27;best_nms_dynamic_qat.onnx&#x27;</span>)</span><br><span class="line">    dynamic = &#123;<span class="string">&#x27;images&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>&#125;&#125;</span><br><span class="line">    dynamic[<span class="string">&#x27;num_dets&#x27;</span>] = &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>&#125;</span><br><span class="line">    dynamic[<span class="string">&#x27;det_boxes&#x27;</span>] = &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>&#125;</span><br><span class="line">    torch.onnx.export(</span><br><span class="line">        q_model,</span><br><span class="line">        dummy_input,</span><br><span class="line">        onnx_file,</span><br><span class="line">        verbose=<span class="literal">False</span>,</span><br><span class="line">        opset_version=<span class="number">13</span>,</span><br><span class="line">        do_constant_folding=<span class="literal">False</span>,</span><br><span class="line">        input_names=input_names,</span><br><span class="line">        output_names=output_names,</span><br><span class="line">        dynamic_axes=dynamic)</span><br></pre></td></tr></table></figure>
<h1 id="附录："><a href="#附录：" class="headerlink" title="附录："></a>附录：</h1><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/648877516">tensorrt官方int8量化方法汇总</a> </li>
<li>参考代码：python版本 onnx转int8 trtengine <a target="_blank" rel="noopener" href="https://github.com/aiLiwensong/Pytorch2TensorRT/tree/master">https://github.com/aiLiwensong/Pytorch2TensorRT/tree/master</a></li>
<li>参考代码 <strong><a target="_blank" rel="noopener" href="https://github.com/SakodaShintaro/Miacis">Miacis</a></strong>  </li>
<li>参考代码v<strong><a target="_blank" rel="noopener" href="https://github.com/kalfazed/tensorrt_starter">tensorrt_starter</a></strong></li>
<li><strong>强烈推荐参考代码</strong> <strong><a target="_blank" rel="noopener" href="https://github.com/kalfazed/tensorrt_starter">tensorrt_starter</a></strong>   </li>
<li><a target="_blank" rel="noopener" href="https://github.com/leo-drive/avte.autoware_universe/blob/4542112f80ddbfe089cefe1dfffa20470399eed3/perception/tensorrt_yolox/include/tensorrt_yolox/calibrator.hpp#L166">c++实现所有量化方法代码</a></li>
<li>强烈推荐知乎的一个文章，包含PTQ和QAT的流程和python代码实现<strong><a target="_blank" rel="noopener" href="https://github.com/Susan19900316/yolov5_tensorrt_int8">yolov5_tensorrt_int8</a></strong>     </li>
<li>参考代码 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/TaipKang/p/15542329.html">https://www.cnblogs.com/TaipKang/p/15542329.html</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/index.html#pytorch-quantization-s-documentation">pytorch-quantization’s documentation</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>奔跑的IC
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://example.com/TensorRT/TensorRT%20INT8%E9%87%8F%E5%8C%96%E4%BB%A3%E7%A0%81/" title="TensorRT INT8量化代码">http://example.com/TensorRT/TensorRT INT8量化代码/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/git/" rel="tag"><i class="fa fa-tag"></i> git</a>
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C</a>
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C++</a>
              <a href="/tags/GPU/" rel="tag"><i class="fa fa-tag"></i> GPU</a>
              <a href="/tags/Tensorrt/" rel="tag"><i class="fa fa-tag"></i> Tensorrt</a>
              <a href="/tags/DeepLearning/" rel="tag"><i class="fa fa-tag"></i> DeepLearning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/TensorRT/Calibration%20file/" rel="prev" title="Calibration file">
                  <i class="fa fa-chevron-left"></i> Calibration file
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT-plugin/" rel="next" title="TensorRT-plugin">
                  TensorRT-plugin <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">奔跑的IC</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"zmurder","repo":"zmurder.github.io","client_id":"cf2343f27b6c29efe0bc","client_secret":"3268a1fa92706c7358d5421f88f76a0f7ada3188","admin_user":"zmurder","distraction_free_mode":true,"proxy":"https://strong-caramel-969805.netlify.app/github_access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"6802c20a9679d288c21868ae9cb3d98e"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
