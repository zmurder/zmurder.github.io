<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
  <meta name="msvalidate.01" content="7EC20DBC74B004C2782077570E15C280">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
    <meta name="description" content="1 Plugin简介网络模型就是很多层组成的，tensorRT基本上比较经典的层比如，卷积，反卷积，全连接，RNN，softmax等，在tensorRT中都是有对应的实现方式的，tensorRT是可以直接解析的。但是由于现在深度学习技术发展日新月异，各种不同结构的自定义层（比如：STN）层出不穷，所以tensorRT是不可能全部支持当前存在的所有层的。那对于这些自定义的层该怎么办？">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorRT-plugin">
<meta property="og:url" content="http://example.com/TensorRT/TensorRT-plugin/index.html">
<meta property="og:site_name" content="奔跑的IC">
<meta property="og:description" content="1 Plugin简介网络模型就是很多层组成的，tensorRT基本上比较经典的层比如，卷积，反卷积，全连接，RNN，softmax等，在tensorRT中都是有对应的实现方式的，tensorRT是可以直接解析的。但是由于现在深度学习技术发展日新月异，各种不同结构的自定义层（比如：STN）层出不穷，所以tensorRT是不可能全部支持当前存在的所有层的。那对于这些自定义的层该怎么办？">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/download.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/download-16632326579123.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/download-16632326579134.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/download-16632327107909.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/download-166323271079010.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/download-166323271079011.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220915170932921.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220915171418072.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220915191025034.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220916110508870.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220916110619782.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220919133000899.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220919133120820.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220927114645642.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220919103455265.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220919114720622.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220919114755156.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220919114927636.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220919114942792.png">
<meta property="og:image" content="http://example.com/home/huolin/.config/Typora/typora-user-images/image-20220928141043140.png">
<meta property="og:image" content="http://example.com/home/huolin/.config/Typora/typora-user-images/image-20220928141703758.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220929191047186.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220929191752527.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220929193123478.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/post-processed-gn-layer-3.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220929194124761.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220929194013225.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220929194516628.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20221130152004319.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20220929195248286.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20221014150954138.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT-plugin/image-20221014152058430.png">
<meta property="article:published_time" content="2024-12-01T10:13:45.317Z">
<meta property="article:modified_time" content="2024-12-01T10:13:45.317Z">
<meta property="article:author" content="奔跑的IC">
<meta property="article:tag" content="git">
<meta property="article:tag" content="C">
<meta property="article:tag" content="C++">
<meta property="article:tag" content="GPU">
<meta property="article:tag" content="Tensorrt">
<meta property="article:tag" content="DeepLearning">
<meta property="article:tag" content="Typora">
<meta property="article:tag" content="Plugin">
<meta property="article:tag" content="cmake">
<meta property="article:tag" content="卷积神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/TensorRT/TensorRT-plugin/download.png">


<link rel="canonical" href="http://example.com/TensorRT/TensorRT-plugin/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://example.com/TensorRT/TensorRT-plugin/","path":"TensorRT/TensorRT-plugin/","title":"TensorRT-plugin"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>TensorRT-plugin | 奔跑的IC</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">奔跑的IC</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-reorder fa-fw"></i>文章列表</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-Plugin%E7%AE%80%E4%BB%8B"><span class="nav-text">1 Plugin简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E7%AE%97%E5%AD%90"><span class="nav-text">1.1 算子</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-1-%E7%AE%97%E5%AD%90%E5%90%8D%E7%A7%B0%EF%BC%88Name%EF%BC%89"><span class="nav-text">1.1.1 算子名称（Name）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-2-%E7%AE%97%E5%AD%90%E7%B1%BB%E5%9E%8B%EF%BC%88Type%EF%BC%89"><span class="nav-text">1.1.2 算子类型（Type）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-3-%E5%BC%A0%E9%87%8F%EF%BC%88Tensor%EF%BC%89"><span class="nav-text">1.1.3 张量（Tensor）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-4-%E6%95%B0%E6%8D%AE%E6%8E%92%E5%B8%83%E6%A0%BC%E5%BC%8F%EF%BC%88Format%EF%BC%89"><span class="nav-text">1.1.4 数据排布格式（Format）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-5-%E5%BD%A2%E7%8A%B6%EF%BC%88Shape%EF%BC%89"><span class="nav-text">1.1.5 形状（Shape）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-6-%E8%BD%B4%EF%BC%88axis%EF%BC%89"><span class="nav-text">1.1.6 轴（axis）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-7-%E6%9D%83%E9%87%8D%EF%BC%88weights%EF%BC%89"><span class="nav-text">1.1.7 权重（weights）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-8-%E5%81%8F%E5%B7%AE%EF%BC%88bias%EF%BC%89"><span class="nav-text">1.1.8 偏差（bias）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E5%AE%98%E6%96%B9%E4%B8%AD%E6%96%87"><span class="nav-text">2 官方中文</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E4%BD%BF%E7%94%A8c-API%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="nav-text">2.1 使用c++ API添加自定义层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-1-%E7%A4%BA%E4%BE%8B-%E4%BD%BF%E7%94%A8c-%E6%B7%BB%E5%8A%A0%E6%94%AF%E6%8C%81%E5%8A%A8%E6%80%81%E5%BD%A2%E7%8A%B6%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="nav-text">2.1.1 示例:使用c++添加支持动态形状的自定义层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-2-%E7%A4%BA%E4%BE%8B%EF%BC%9A%E4%BD%BF%E7%94%A8C-%E6%B7%BB%E5%8A%A0INT8-I-O%E6%94%AF%E6%8C%81%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="nav-text">2.1.2 示例：使用C ++添加INT8 I&#x2F;O支持的自定义层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E5%9C%A8%E4%BD%BF%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%A7%A3%E6%9E%90%E5%99%A8%EF%BC%88Parser%EF%BC%89%E6%97%B6%E4%BD%BF%E7%94%A8%E7%94%A8%E6%88%B7%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="nav-text">2.2 在使用模型解析器（Parser）时使用用户自定义层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-Plugin-API"><span class="nav-text">2.4 Plugin API</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-1-IPluginV2-API%E6%8F%8F%E8%BF%B0"><span class="nav-text">2.4.1 IPluginV2 API描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-2-IPluginCreator-API%E6%8F%8F%E8%BF%B0"><span class="nav-text">2.4.2. IPluginCreator API描述</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-Plugin%E7%BC%96%E5%86%99%E6%B5%81%E7%A8%8B"><span class="nav-text">3 Plugin编写流程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E8%B0%83%E7%94%A8%E5%85%B3%E7%B3%BB"><span class="nav-text">3.1 调用关系</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-1-build-engine%E6%97%B6"><span class="nav-text">3.1.1 build  engine时</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-2-%E5%8A%A0%E8%BD%BD%E5%92%8C%E6%8E%A8%E7%90%86-engine%E6%97%B6"><span class="nav-text">3.1.2 加载和推理  engine时</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-PluginCreator"><span class="nav-text">3.2 PluginCreator</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-1-getPluginName"><span class="nav-text">3.2.1 getPluginName</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-2-getPluginVersion"><span class="nav-text">3.2.2 getPluginVersion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-3-getFieldNames"><span class="nav-text">3.2.3 getFieldNames</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-3-createPlugin"><span class="nav-text">3.2.3 createPlugin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-4-deserializePlugin"><span class="nav-text">3.2.4 deserializePlugin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-5-set-getPluginNamespace"><span class="nav-text">3.2.5 set&#x2F;getPluginNamespace</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-Plugin%E7%B1%BB%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90"><span class="nav-text">3.3 Plugin类代码分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-1-%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0"><span class="nav-text">3.3.1 构造函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-2-clone"><span class="nav-text">3.3.2 clone</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-3-getNbOutputs"><span class="nav-text">3.3.3 getNbOutputs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-4-getOutputDataType"><span class="nav-text">3.3.4 getOutputDataType</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-5-getOutputDimensions"><span class="nav-text">3.3.5 getOutputDimensions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-6-supportsFormatCombination"><span class="nav-text">3.3.6 supportsFormatCombination</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-7-configurePlugin"><span class="nav-text">3.3.7 configurePlugin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-8-getWorkspaceSize"><span class="nav-text">3.3.8 getWorkspaceSize</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-9-enqueue"><span class="nav-text">3.3.9 enqueue</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-10-destroy"><span class="nav-text">3.3.10 destroy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-11-initialize"><span class="nav-text">3.3.11 initialize</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-12-terminate"><span class="nav-text">3.3.12 terminate</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-13-getSerializationSize"><span class="nav-text">3.3.13 getSerializationSize</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-14-serialize"><span class="nav-text">3.3.14 serialize</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-15-attachToContext"><span class="nav-text">3.3.15 attachToContext</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-%E7%AE%80%E5%8D%95%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-text">3.4 简单的例子</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-1-%E5%A4%B4%E6%96%87%E4%BB%B6"><span class="nav-text">3.4.1 头文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-2-%E6%BA%90%E6%96%87%E4%BB%B6"><span class="nav-text">3.4.2 源文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-3-%E6%B5%8B%E8%AF%95%E8%BF%90%E8%A1%8C"><span class="nav-text">3.4.3 测试运行</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-%E5%AE%98%E6%96%B9%E4%BE%8B%E5%AD%90"><span class="nav-text">4 官方例子</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-sampleFasterRCNN"><span class="nav-text">4.1 sampleFasterRCNN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-1-%E8%BE%93%E5%85%A5%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-text">4.1.1 输入预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-2-%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C"><span class="nav-text">4.1.2 定义网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-3-%E7%BC%96%E8%AF%91engine"><span class="nav-text">4.1.3 编译engine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-4-%E8%BF%90%E8%A1%8Cengine"><span class="nav-text">4.1.4 运行engine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-5-%E9%AA%8C%E8%AF%81%E8%BE%93%E5%87%BA"><span class="nav-text">4.1.5 验证输出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-6-int8%E7%B2%BE%E5%BA%A6%E9%A2%84%E6%B5%8B"><span class="nav-text">4.1.6 int8精度预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-7-Tensorrt-API%E5%B1%82%E5%92%8COPS"><span class="nav-text">4.1.7 Tensorrt API层和OPS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-8-%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="nav-text">4.1.8 数据准备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-9-%E8%BF%90%E8%A1%8C%E7%A4%BA%E4%BE%8B"><span class="nav-text">4.1.9 运行示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-10-%E7%90%86%E8%A7%A3%E7%9A%84%E5%87%A0%E4%B8%AA%E7%82%B9%EF%BC%9A"><span class="nav-text">4.1.10 理解的几个点：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-11-%E6%B7%BB%E5%8A%A0%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84plugin%EF%BC%8C%E4%BD%BFtrtexec%E5%8F%AF%E4%BB%A5%E8%AF%86%E5%88%AB"><span class="nav-text">4.1.11 添加自定义的plugin，使trtexec可以识别</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-%E5%AE%98%E6%96%B9%E5%BA%93%E7%BC%96%E8%AF%91"><span class="nav-text">5 官方库编译</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-%E5%AE%98%E6%96%B9plugin%E7%BC%96%E8%AF%91"><span class="nav-text">5.1 官方plugin编译</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-1-PC%E7%AB%AF%E7%BC%96%E8%AF%91"><span class="nav-text">5.1.1 PC端编译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-2-%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91"><span class="nav-text">5.1.2 交叉编译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-3-%E5%8F%88%E4%B8%80%E7%82%B9%E7%90%86%E8%A7%A3"><span class="nav-text">5.1.3 又一点理解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5-%E6%B3%A8%E5%86%8Cplugin"><span class="nav-text">第一步 注册plugin</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E5%B0%86plugin%E5%8A%A0%E5%85%A5%E7%BD%91%E7%BB%9C"><span class="nav-text">第二步 将plugin加入网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%87%A0%E4%B8%AA%E6%B3%A8%E6%84%8F%E7%9A%84%E7%82%B9"><span class="nav-text">几个注意的点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#plugin%E7%9A%84%E5%90%8D%E7%A7%B0"><span class="nav-text">plugin的名称</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#plugin%E5%8F%82%E6%95%B0"><span class="nav-text">plugin参数</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E4%B8%AD%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-text">使用中的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-trtexec-dynamic-batch-size"><span class="nav-text">1 trtexec dynamic batch size</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95%EF%BC%9A"><span class="nav-text">附录：</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RPN"><span class="nav-text">RPN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ROI"><span class="nav-text">ROI</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="奔跑的IC"
      src="/images/zyd.gif">
  <p class="site-author-name" itemprop="name">奔跑的IC</p>
  <div class="site-description" itemprop="description">死磕牛角的IT农民工</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">149</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/TensorRT/TensorRT-plugin/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zyd.gif">
      <meta itemprop="name" content="奔跑的IC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="奔跑的IC">
      <meta itemprop="description" content="死磕牛角的IT农民工">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="TensorRT-plugin | 奔跑的IC">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          TensorRT-plugin
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-01 18:13:45" itemprop="dateCreated datePublished" datetime="2024-12-01T18:13:45+08:00">2024-12-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TensorRT/" itemprop="url" rel="index"><span itemprop="name">TensorRT</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="1-Plugin简介"><a href="#1-Plugin简介" class="headerlink" title="1 Plugin简介"></a>1 Plugin简介</h1><p>网络模型就是很多层组成的，tensorRT基本上比较经典的层比如，卷积，反卷积，全连接，RNN，softmax等，在tensorRT中都是有对应的实现方式的，tensorRT是可以直接解析的。但是由于现在深度学习技术发展日新月异，各种不同结构的自定义层（比如：STN）层出不穷，所以tensorRT是不可能全部支持当前存在的所有层的。那对于这些自定义的层该怎么办？</p>
<p><strong>tensorRT中有一个 Plugin 层，这个层提供了 API 可以由用户自己定义tensorRT不支持的层。</strong></p>
<p><strong>可以看出，编写plugin实际就是编写一个自定义的算子（因为TensorRT支持的算子有限）。</strong>顺便介绍一下算子</p>
<h2 id="1-1-算子"><a href="#1-1-算子" class="headerlink" title="1.1 算子"></a>1.1 算子</h2><p>参考网址 <a target="_blank" rel="noopener" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100180768/9dd5739">https://support.huawei.com/enterprise/zh/doc/EDOC1100180768/9dd5739</a></p>
<p>深度学习算法由一个个计算单元组成，我们称这些计算单元为算子（Operator，简称Op）。在网络模型中，算子对应层中的计算逻辑，例如：卷积层（Convolution Layer）是一个算子；全连接层（Fully-connected Layer， FC layer）中的权值求和过程，是一个算子。</p>
<h3 id="1-1-1-算子名称（Name）"><a href="#1-1-1-算子名称（Name）" class="headerlink" title="1.1.1 算子名称（Name）"></a>1.1.1 <strong>算子名称（Name）</strong></h3><p>算子的name，用于标志网络中的某个算子，同一网络中算子的名称需要保持唯一。如下图所示conv1，pool1，conv2都是此网络中的算子名称，其中conv1与conv2算子的类型为Convolution，表示分别做一次卷积运算。</p>
<p>图1-1  网络拓扑示例<br><img src="/TensorRT/TensorRT-plugin/download.png" class="" title="img"></p>
<h3 id="1-1-2-算子类型（Type）"><a href="#1-1-2-算子类型（Type）" class="headerlink" title="1.1.2 算子类型（Type）"></a>1.1.2 <strong>算子类型（Type）</strong></h3><p>算子的Type，代表算子的类型，例如卷积算子的类型为Convolution，在一个网络中同一类型的算子可能存在多个。</p>
<h3 id="1-1-3-张量（Tensor）"><a href="#1-1-3-张量（Tensor）" class="headerlink" title="1.1.3 张量（Tensor）"></a>1.1.3 <strong>张量（Tensor）</strong></h3><p>Tensor是算子中的数据，包括输入数据与输出数据，TensorDesc（Tensor描述符）是对输入数据与输出数据的描述，TensorDesc数据结构包含如下属性如<a target="_blank" rel="noopener" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100180768/9dd5739#tb6e37f72bced414fa0adde77dc48447f">表1-1</a>所示。</p>
<p>表1-1  TensorDesc属性解释</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>定义</th>
</tr>
</thead>
<tbody>
<tr>
<td>名称（name）</td>
<td>用于对Tensor进行索引，不同Tensor的name需要保持唯一。</td>
</tr>
<tr>
<td>形状（shape）</td>
<td>Tensor的形状，比如（10,）或者（1024，1024）或者（2，3，4）等。详细介绍请参见<a target="_blank" rel="noopener" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100180768/9dd5739#section1546519496196"> 形状（shape）</a>。 默认值：无 形式：(i1, i2,…in)，其中i1到in均为正整数</td>
</tr>
<tr>
<td>数据类型（dtype）</td>
<td>功能描述：指定Tensor对象的数据类型。 默认值：无 取值范围：float16, float32, int8, int16, int32, uint8, uint16, bool。</td>
</tr>
<tr>
<td>数据排布格式（format）</td>
<td>详细请参见<a target="_blank" rel="noopener" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100180768/9dd5739#section125241018101614"> 数据排布格式（format）</a>。</td>
</tr>
</tbody>
</table>
</div>
<h3 id="1-1-4-数据排布格式（Format）"><a href="#1-1-4-数据排布格式（Format）" class="headerlink" title="1.1.4 数据排布格式（Format）"></a>1.1.4 <strong>数据排布格式（Format）</strong></h3><p>在深度学习框架中，多维数据通过多维数组存储，比如卷积神经网络的特征图用四维数组保存，四个维度分别为批量大小（Batch, N）、特征图高度（Height, H）、特征图宽度（Width, W）以及特征图通道（Channels, C）。</p>
<p>由于数据只能线性存储，因为这四个维度有对应的顺序。不同深度学习框架会按照不同的顺序存储特征图数据，比如Caffe，排列顺序为[Batch, Channels, Height, Width]，即NCHW。Tensorflow中，排列顺序为[Batch, Height, Width,  Channels]，即NHWC。</p>
<p>如<a target="_blank" rel="noopener" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100180768/9dd5739#f01701bda7d07409aa921940ae9897c13">图1-2</a>所示，以一张格式为RGB的图片为例，NCHW中，C排列在外层，实际存储的是“RRRGGGBBB”，即同一通道的所有像素值顺序存储在一起；而NHWC中C排列在最内层，实际存储的则是“RGBRGBRGB”，即多个通道的同一位置的像素值顺序存储在一起。</p>
<p>图1-2  NCHW和NHWC<br><img src="/TensorRT/TensorRT-plugin/download-16632326579123.png" class="" title="img"></p>
<h3 id="1-1-5-形状（Shape）"><a href="#1-1-5-形状（Shape）" class="headerlink" title="1.1.5 形状（Shape）"></a>1.1.5 <strong>形状（Shape）</strong></h3><p>张量的形状，以(D0, D1, … ,Dn-1)的形式表示，D0到Dn是任意的正整数。</p>
<p>如形状(3,4)表示第一维有3个元素，第二维有4个元素，(3,4)表示一个3行4列的矩阵数组。</p>
<p>在形状的小括号中有多少个数字，就代表这个张量是多少维的张量。形状的第一个元素要看张量最外层的中括号中有几个元素，形状的第二个元素要看张量中从左边开始数第二个中括号中有几个元素，依此类推。例如：</p>
<p>表1-2  张量的形状举例</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>张量</th>
<th>形状</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>(0,)</td>
</tr>
<tr>
<td>[1,2,3]</td>
<td>(3,)</td>
</tr>
<tr>
<td>[[1,2],[3,4]]</td>
<td>(2,2)</td>
</tr>
<tr>
<td>[[[1,2],[3,4]], [[5,6],[7,8]]]</td>
<td>(2,2,2)</td>
</tr>
</tbody>
</table>
</div>
<p>物理含义我们应该怎么理解呢？假设我们有这样一个shape=(4, 20, 20, 3)。</p>
<p>假设有一些照片，每个像素点都由红/绿/蓝3色组成，即shape里面3的含义，照片的宽和高都是20，也就是20*20=400个像素，总共有4张的照片，这就是shape=(4, 20, 20, 3)的物理含义。</p>
<p>图1-3  示意图：<br><img src="/TensorRT/TensorRT-plugin/download-16632326579134.png" class="" title="img"></p>
<p>如果体现在编程上，可以简单把shape理解为操作Tensor的各层循环，比如我们要对shape=(4, 20, 20, 3)的A tensor进行操作，循环语句如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">produce A &#123;</span><br><span class="line">  <span class="keyword">for</span> (i, <span class="number">0</span>, <span class="number">4</span>) &#123;</span><br><span class="line">    <span class="keyword">for</span> (j, <span class="number">0</span>, <span class="number">20</span>) &#123;</span><br><span class="line">      <span class="keyword">for</span> (p, <span class="number">0</span>, <span class="number">20</span>) &#123;</span><br><span class="line">        <span class="keyword">for</span> (q, <span class="number">0</span>, <span class="number">3</span>) &#123;</span><br><span class="line">          A[((((((i*<span class="number">20</span>) + j)*<span class="number">20</span>) + p)*<span class="number">3</span>) + q)] = a_tensor[((((((i*<span class="number">20</span>) + j)*<span class="number">20</span>) + p)*<span class="number">3</span>) + q)]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-1-6-轴（axis）"><a href="#1-1-6-轴（axis）" class="headerlink" title="1.1.6 轴（axis）"></a>1.1.6 <strong>轴（axis）</strong></h3><p>轴是相对Shape来说的，轴代表张量的shape的下标，比如张量a是一个5行6列的二维数组，即shape是(5,6)，则axis=0表示是张量中的第一维，即行。axis=1表示是张量中的第二维，即列。</p>
<p>例如张量数据[[[1,2],[3,4]],  [[5,6],[7,8]]]，Shape为(2,2,2)，则轴0代表第一个维度的数据即[[1,2],[3,4]]与[[5,6],[7,8]]这两个矩阵，轴1代表第二个维度的数据即[1,2]、[3,4]、[5,6]、[7,8]这四个数组，轴2代表第三个维度的数据即1，2，3，4，5，6，7，8这八个数。</p>
<p>轴axis可以为负数，此时表示是倒数第axis个维度。</p>
<p>N维Tensor的轴有：0 , 1, 2,……，N-1。</p>
<p>图1-4  轴示意图：<br><img src="/TensorRT/TensorRT-plugin/download-16632327107909.png" class="" title="img"></p>
<h3 id="1-1-7-权重（weights）"><a href="#1-1-7-权重（weights）" class="headerlink" title="1.1.7 权重（weights）"></a>1.1.7 <strong>权重（weights）</strong></h3><p>当输入数据进入计算单元时，会乘以一个权重。例如，如果一个算子有两个输入，则每个输入会分配一个关联权重，一般将认为较重要数据赋予较高的权重，不重要的数据赋予较小的权重，为零的权重则表示特定的特征是无需关注的。</p>
<p>如<a target="_blank" rel="noopener" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100180768/9dd5739#zh-cn_topic_0000001073361370_f231ebde3733e42b181faee9678387dec">图1-5</a>所示，假设输入数据为X1，与其相关联的权重为W1，那么在通过计算单元后，数据变为了X1*W1。</p>
<p>图1-5  权重计算示例<br><img src="/TensorRT/TensorRT-plugin/download-166323271079010.png" class="" title="img"></p>
<h3 id="1-1-8-偏差（bias）"><a href="#1-1-8-偏差（bias）" class="headerlink" title="1.1.8 偏差（bias）"></a>1.1.8 <strong>偏差（bias）</strong></h3><p>偏差是除了权重之外，另一个被应用于输入数据的线性分量。它被加到权重与输入数据相乘的结果中，用于改变权重与输入相乘所得结果的范围。</p>
<p>如<a target="_blank" rel="noopener" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100180768/9dd5739#zh-cn_topic_0000001073361370_ff87eef1acc86457686ad23b592357cee">图1-6</a>所示，假设输入数据为X1，与其相关联的权重为W1，偏差为B1，那么在通过计算单元后，数据变为了X1*W1+B1。</p>
<p>图1-6  偏差计算示例<br><img src="/TensorRT/TensorRT-plugin/download-166323271079011.png" class="" title="img"></p>
<h1 id="2-官方中文"><a href="#2-官方中文" class="headerlink" title="2 官方中文"></a>2 官方中文</h1><p>官方参考<a target="_blank" rel="noopener" href="https://developer.nvidia.com/zh-cn/blog/tensorrt-custom-layer-cn/#:~:text=%E8%A6%81%E5%9C%A8%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E4%B8%AD%E4%BD%BF%E7%94%A8%20TensorRT%20%E6%8F%92%E4%BB%B6%EF%BC%8C%E5%BF%85%E9%A1%BB%E5%8A%A0%E8%BD%BD,libnvinfer_plugin.so%20%E5%BA%93%EF%BC%8C%E5%B9%B6%E4%B8%94%E5%BF%85%E9%A1%BB%E9%80%9A%E8%BF%87%E5%9C%A8%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E4%BB%A3%E7%A0%81%E4%B8%AD%E8%B0%83%E7%94%A8%20initLibNvInferPlugins%20%E6%9D%A5%E6%B3%A8%E5%86%8C%E6%89%80%E6%9C%89%E6%8F%92%E4%BB%B6%E3%80%82">中文</a> <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#extending">英文</a></p>
<p>这里参考英文作了修改</p>
<p>NVIDIA TensorRT 支持多种类型的层，其功能不断扩展；但是，在某些情况下，支持的层不能满足模型的特定需求。<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#layers">官方已经支持的层</a></p>
<p>您可以通过实现自定义层，通常称为插件（plug-ins）来扩展 TensorRT。</p>
<p>要在您的应用程序中使用Tensorrt插件，必须加载libnvinfer_plugin.so（windows中是nvinfer_plugin.dll）库。所有插件必须通过在您的应用程序代码中调用Initlibnvinferplugins来注册。有关这些插件的更多信息，请参阅<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/_nv_infer_plugin_8h.html">NvInferPlugin.h</a> 文件以获取参考。</p>
<p>如果这些插件无法满足您的需求，则可以编写并添加自己的需求。</p>
<h2 id="2-1-使用c-API添加自定义层"><a href="#2-1-使用c-API添加自定义层" class="headerlink" title="2.1 使用c++ API添加自定义层"></a>2.1 使用c++ API添加自定义层</h2><p>您可以通过从 TensorRT 的插件基类之一派生来实现自定义层。</p>
<p>您必须从插件的基类之一派生插件类。在支持具有不同类型/格式的输入/输出或具有动态形状的网络方面，它们具有不同的表达能力。下表总结了基类，按从最不具表现力到最具表现力的顺序排列。</p>
<p>注意：如果插件用于一般用途，请提供 FP32 实现，以使其能够在任何网络上正常运行。</p>
<p>Table 3. Base classes, ordered from least expressive to most expressive</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>Introduced in TensorRT version?</th>
<th>Mixed input/output formats/types</th>
<th>Dynamic shapes?</th>
<th>Supports implicit/explicit batch mode?</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/classnvinfer1_1_1_i_plugin_v2_ext.html">IPluginV2Ext</a></td>
<td>5.1</td>
<td>Limited</td>
<td>No</td>
<td>Implicit batch mode only</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/classnvinfer1_1_1_i_plugin_v2_i_o_ext.html">IPluginV2IOExt</a></td>
<td>6.0.1</td>
<td>General</td>
<td>No</td>
<td>Implicit batch mode only</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/classnvinfer1_1_1_i_plugin_v2_dynamic_ext.html">IPluginV2DynamicExt</a></td>
<td>6.0.1</td>
<td>General</td>
<td>Yes</td>
<td>Explicit batch mode only</td>
</tr>
</tbody>
</table>
</div>
<p>为了在网络中使用插件，您必须首先将其注册到 TensorRT 的 <code>PluginRegistry</code> （ <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_plugin_registry.html">C++</a> 、 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/Plugin/IPluginRegistry.html">Python</a> ）。不是直接注册插件，而是为插件注册一个工厂类的实例，派生自 <code>PluginCreator</code> ( <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_plugin_creator.html">C++</a> , <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/Plugin/IPluginCreator.html">Python</a> )。插件创建者类还提供有关插件的其他信息：它的名称、版本和插件字段参数。</p>
<p>您可以通过两种方式注册注册表：</p>
<ul>
<li>TensorRT提供了一个宏REGISTER_TENSORRT_PLUGIN，它静态地将插件创建者注册到注册表中。注意REGISTER_TENSORRT_PLUGIN总是在默认的命名空间()下注册创建者。</li>
<li>通过创建自己的入口点，类似于Initlibnvinferplugins并在插件注册表上调用registercreator，动态注册。。这比静态注册更可取，因为它可能提供更低的内存占用，并允许在唯一的名称空间下注册插件。这确保在不同插件库之间的构建期间不会发生名称冲突。查看官方例子sampleFasterRCNN。里面就是动态注册后使用param解析器来识别插件的。</li>
</ul>
<p>注意：</p>
<ul>
<li>要在应用程序中使用 TensorRT 插件，必须加载 <code>libnvinfer_plugin.so</code>库，并且必须通过在应用程序代码中调用 <code>initLibNvInferPlugins</code>来注册所有插件。</li>
<li>如果您有自己的插件库，则可以包含一个类似的入口点，以便在唯一命名空间下的注册表中注册所有插件。这确保了在构建期间跨不同插件库的插件名称没有冲突。</li>
</ul>
<p>调用iPluginCreator :: createplugin（）返回Ipluginv2类型的插件对象。您可以使用AddPluginv2（）将插件添加到Tensorrt网络中，该网络可以使用给定的插件创建网络层。</p>
<p>例如，可以向网络添加一个插件层，如下所示（<strong>注意这里是使用API搭建的网络层，如果是使用param解析器时，注册过应该就可以使用了</strong>）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Look up the plugin in the registry</span></span><br><span class="line"><span class="keyword">auto</span> creator = <span class="built_in">getPluginRegistry</span>()-&gt;<span class="built_in">getPluginCreator</span>(pluginName, pluginVersion);</span><br><span class="line"><span class="type">const</span> PluginFieldCollection* pluginFC = creator-&gt;<span class="built_in">getFieldNames</span>();</span><br><span class="line"><span class="comment">// Populate the fields parameters for the plugin layer </span></span><br><span class="line"><span class="comment">// PluginFieldCollection *pluginData = parseAndFillFields(pluginFC, layerFields); </span></span><br><span class="line"><span class="comment">// Create the plugin object using the layerName and the plugin meta data</span></span><br><span class="line">IPluginV2 *pluginObj = creator-&gt;<span class="built_in">createPlugin</span>(layerName, pluginData);</span><br><span class="line"><span class="comment">// Add the plugin to the TensorRT network </span></span><br><span class="line"><span class="keyword">auto</span> layer = network.<span class="built_in">addPluginV2</span>(&amp;inputs[<span class="number">0</span>], <span class="built_in">int</span>(inputs.<span class="built_in">size</span>()), pluginObj);</span><br><span class="line">… (build rest of the network <span class="keyword">and</span> serialize engine)</span><br><span class="line"><span class="comment">// Destroy the plugin object</span></span><br><span class="line">pluginObj-&gt;<span class="built_in">destroy</span>()</span><br></pre></td></tr></table></figure>
<p>注意:前面描述的createPlugin方法在堆上创建一个新的插件对象，并返回指向该对象的指针。确保如前面所示销毁pluginObj，以避免内存泄漏。</p>
<p>在序列化期间，TensorRT引擎在内部存储所有IPluginV2类型插件的插件类型、插件版本和名称空间(如果存在的话)。在反序列化期间，TensorRT从插件注册表中查找插件创建者并调用IPluginCreator::deserializePlugin()。当引擎被删除时，在引擎构建过程中创建的插件对象的克隆将被引擎通过调用IPluginV2::destroy()方法销毁。您有责任确保所创建的插件对象在添加到网络后被释放。</p>
<p>注意：</p>
<ul>
<li>请勿序列化所有插件参数：仅在运行时插件正常运行所需的内容。可以省略构建时间参数。</li>
<li>以相同的顺序序列化和反序列化插件参数。在反序列化期间，检查插件参数是否初始化为默认值或反序列化值。未初始化的参数会导致未定义的行为。</li>
</ul>
<h3 id="2-1-1-示例-使用c-添加支持动态形状的自定义层"><a href="#2-1-1-示例-使用c-添加支持动态形状的自定义层" class="headerlink" title="2.1.1 示例:使用c++添加支持动态形状的自定义层"></a>2.1.1 示例:使用c++添加支持动态形状的自定义层</h3><p>要支持动态形状，您的插件必须派生自IPluginV2DynamicExt。</p>
<p>BarPlugin是一个具有两个输入和两个输出的插件</p>
<ul>
<li>第一个输出是第二个输入的副本。</li>
<li>第二个输出是两个输入的串联，沿着第一个维度，所有类型/格式必须相同，并且是线性格式。</li>
</ul>
<p>BarPlugin必须按照以下方法派生</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BarPlugin</span> : <span class="keyword">public</span> IPluginV2DynamicExt</span><br><span class="line">&#123;</span><br><span class="line">	...<span class="keyword">override</span> <span class="keyword">virtual</span> methods inherited from IPluginV2DynamicExt.</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>受动态形状影响的四种方法是：</p>
<ul>
<li>getOutputDimensions</li>
<li>supportsFormatCombination</li>
<li>configurePlugin</li>
<li>enqueue</li>
</ul>
<p>getOutputDimensions的重载根据输入维度返回输出维度的符号表达式。您可以使用使用传递到getOutputDimensions的IExprBuilder从输入的表达式构建表达式。在本例中，无需为情况1构建新的表达式，因为第二个输出的维度与第一个输入的维度相同。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">DimsExprs <span class="title">BarPlugin::getOutputDimensions</span><span class="params">(<span class="type">int</span> outputIndex, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> DimsExprs* inputs, <span class="type">int</span> nbInputs, </span></span></span><br><span class="line"><span class="params"><span class="function">    IExprBuilder&amp; exprBuilder)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (outputIndex)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">0</span>: </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// First dimension of output is sum of input </span></span><br><span class="line">        <span class="comment">// first dimensions.</span></span><br><span class="line">        <span class="function">DimsExprs <span class="title">output</span><span class="params">(inputs[<span class="number">0</span>])</span></span>;</span><br><span class="line">        output.d[<span class="number">0</span>] = </span><br><span class="line">            exprBuilder.<span class="built_in">operation</span>(DimensionOperation::kSUM, </span><br><span class="line">                inputs[<span class="number">0</span>].d[<span class="number">0</span>], inputs[<span class="number">1</span>].d[<span class="number">0</span>]);</span><br><span class="line">	   <span class="keyword">return</span> output;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> inputs[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">         <span class="keyword">throw</span> std::<span class="built_in">invalid_argument</span>(“invalid output”);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>supportsFormatCombination</code>的覆盖必须指示是否允许格式组合。接口将输入/输出统一索引为“<code>connections</code>”，从第一个输入的 0 开始，然后依次为其余输入，然后为输出编号。在示例中，输入是 <code>connections</code> 0 和 1，输出是 <code>connections</code> 2 和 3。</p>
<p>TensorRT 使用 <code>supportsFormatCombination</code>来询问给定的格式/类型组合是否适用于连接，给定的格式/类型用于索引较少的连接。因此，覆盖可以假设较少索引的连接已经过审查，并专注于与索引 <code>pos</code>的连接。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">BarPlugin::supportsFormatCombination</span><span class="params">(<span class="type">int</span> pos, <span class="type">const</span> PluginTensorDesc* inOut, <span class="type">int</span> nbInputs, <span class="type">int</span> nbOutputs)</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">assert</span>(<span class="number">0</span> &lt;= pos &amp;&amp; pos &lt; <span class="number">4</span>);</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span>* in = inOut;</span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span>* out = inOut + nbInputs;</span><br><span class="line">    <span class="keyword">switch</span> (pos)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">0</span>: <span class="keyword">return</span> in[<span class="number">0</span>].format == TensorFormat::kLINEAR;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>: <span class="keyword">return</span> in[<span class="number">1</span>].type == in[<span class="number">0</span>].type &amp;&amp;</span><br><span class="line">                   in[<span class="number">1</span>].format == TensorFormat::kLINEAR;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">2</span>: <span class="keyword">return</span> out[<span class="number">0</span>].type == in[<span class="number">0</span>].type &amp;&amp;</span><br><span class="line">                   out[<span class="number">0</span>].format == TensorFormat::kLINEAR;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">3</span>: <span class="keyword">return</span> out[<span class="number">1</span>].type == in[<span class="number">0</span>].type &amp;&amp;</span><br><span class="line">                   out[<span class="number">1</span>].format == TensorFormat::kLINEAR;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">throw</span> std::<span class="built_in">invalid_argument</span>(“invalid connection number”);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里的局部变量 <code>in</code>和 <code>out</code>允许通过输入或输出编号而不是连接编号检查 <code>inOut</code> 。</p>
<p>重要提示：覆盖检查索引小于 <code>pos</code>的连接的格式/类型，但绝不能检查索引大于 <code>pos</code>的连接的格式/类型。该示例使用case 3来检查连接 3 和连接 0，而不是使用case 0来检查连接 0 和连接 3。</p>
<p>TensorRT 使用 <code>configurePlugin</code>在运行时设置插件。这个插件不需要 <code>configurePlugin</code>来做任何事情，所以它是一个空操作：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">BarPlugin::configurePlugin</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> DynamicPluginTensorDesc* in, <span class="type">int</span> nbInputs, </span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> DynamicPluginTensorDesc* out, <span class="type">int</span> nbOutputs)</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果插件需要知道它可能遇到的最小或最大尺寸，它可以检查字段 <code>DynamicPluginTensorDesc::min</code>或 <code>DynamicPluginTensorDesc::max</code>的任何输入或输出。格式和构建时维度信息可以在 <code>DynamicPluginTensorDesc::desc</code>中找到。任何运行时维度都显示为 <code>-1</code>。实际维度提供给 <code>BarPlugin::enqueue</code> 。</p>
<p>最后，重写 <code>BarPlugin::enqueue</code>必须完成这项工作。由于形状是动态的，因此 <code>enqueue</code> 会收到一个 <code>PluginTensorDesc</code> ，它描述了每个输入和输出的实际尺寸、类型和格式。</p>
<h3 id="2-1-2-示例：使用C-添加INT8-I-O支持的自定义层"><a href="#2-1-2-示例：使用C-添加INT8-I-O支持的自定义层" class="headerlink" title="2.1.2 示例：使用C ++添加INT8 I/O支持的自定义层"></a>2.1.2 示例：使用C ++添加INT8 I/O支持的自定义层</h3><p>PoolPlugin是一个插件，可演示如何扩展自定义式层的INT8 I/O。派生如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PoolPlugin</span> : <span class="keyword">public</span> IPluginV2IOExt</span><br><span class="line">&#123;</span><br><span class="line">    ...<span class="keyword">override</span> <span class="keyword">virtual</span> methods inherited from IPluginV2IOExt.</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>大多数纯虚拟方法都是插件的共同点。影响INT8 I/O的主要方法是：</p>
<ul>
<li>supportsFormatCombination</li>
<li>configurePlugin</li>
<li>enqueue</li>
</ul>
<p><code>supportsFormatCombination</code>的重载必须指示允许哪个 INT8 I/O 组合。此接口的用法类似于示例2.2。在本例中，支持的 I/O 张量格式为线性 <code>CHW</code>，数据类型为 <code>FP32</code>、<code>FP16</code> 或 <code>INT8</code>，但 I/O 张量必须具有相同的数据类型。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">PoolPlugin::supportsFormatCombination</span><span class="params">(<span class="type">int</span> pos, <span class="type">const</span> PluginTensorDesc* inOut, <span class="type">int</span> nbInputs, <span class="type">int</span> nbOutputs)</span> <span class="type">const</span> <span class="keyword">override</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">assert</span>(nbInputs == <span class="number">1</span> &amp;&amp; nbOutputs == <span class="number">1</span> &amp;&amp; pos &lt; nbInputs + nbOutputs);</span><br><span class="line">    <span class="type">bool</span> condition = inOut[pos].format == TensorFormat::kLINEAR;</span><br><span class="line">    condition &amp;= ((inOut[pos].type == DataType::kFLOAT) ||</span><br><span class="line">                  (inOut[pos].type == DataType::kHALF) ||</span><br><span class="line">                  (inOut[pos].type == DataType::kINT8));</span><br><span class="line">    condition &amp;= inOut[pos].type == inOut[<span class="number">0</span>].type;</span><br><span class="line">    <span class="keyword">return</span> condition;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>如果 INT8 校准必须与具有 INT8 I/O 插件的网络一起使用，则该插件必须支持 FP32 I/O，因为它被 FP32 校准图使用。</li>
<li>如果不支持 FP32 I/O 变体或未使用 INT8 校准，则必须明确设置所有必需的 INT8 I/O 张量尺度。</li>
<li>校准无法确定插件内部张量的动态范围。对量化数据进行操作的插件必须为内部张量计算自己的动态范围。</li>
</ul>
<p>TensorRT 调用 <code>configurePlugin</code>方法通过 <code>PluginTensorDesc</code>将信息传递给插件，这些信息存储为成员变量，序列化和反序列化。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">PoolPlugin::configurePlugin</span><span class="params">(<span class="type">const</span> PluginTensorDesc* in, <span class="type">int</span> nbInput, <span class="type">const</span> PluginTensorDesc* out, <span class="type">int</span> nbOutput)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    mPoolingParams.mC = mInputDims.d[<span class="number">0</span>];</span><br><span class="line">    mPoolingParams.mH = mInputDims.d[<span class="number">1</span>];</span><br><span class="line">    mPoolingParams.mW = mInputDims.d[<span class="number">2</span>];</span><br><span class="line">    mPoolingParams.mP = mOutputDims.d[<span class="number">1</span>];</span><br><span class="line">    mPoolingParams.mQ = mOutputDims.d[<span class="number">2</span>];</span><br><span class="line">    mInHostScale = in[<span class="number">0</span>].scale &gt;= <span class="number">0.0F</span> ? in[<span class="number">0</span>].scale : <span class="number">-1.0F</span>;</span><br><span class="line">    mOutHostScale = out[<span class="number">0</span>].scale &gt;= <span class="number">0.0F</span> ? out[<span class="number">0</span>].scale : <span class="number">-1.0F</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个张量的int8 i/o量表可以从publIntenSordesc :: scale获得。</p>
<p> 最后，重写 <code>UffPoolPluginV2::enqueue</code>必须完成这项工作。它包括一组核心算法，可在运行时通过使用实际批量大小、输入、输出、cuDNN 流和配置的信息来执行自定义层。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">PoolPlugin::enqueue</span><span class="params">(<span class="type">int</span> batchSize, <span class="type">const</span> <span class="type">void</span>* <span class="type">const</span>* inputs, <span class="type">void</span>** outputs, <span class="type">void</span>* workspace, cudaStream_t stream)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudnnPoolingForward</span>(mCudnn, mPoolingDesc, &amp;kONE, mSrcDescriptor, input, &amp;kZERO, mDstDescriptor, output));</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="2-2-在使用模型解析器（Parser）时使用用户自定义层"><a href="#2-2-在使用模型解析器（Parser）时使用用户自定义层" class="headerlink" title="2.2 在使用模型解析器（Parser）时使用用户自定义层"></a>2.2 在使用模型解析器（Parser）时使用用户自定义层</h2><p>ONNX解析器<strong>自动尝试将无法识别的节点作为插件导入</strong>。如果在插件注册表中找到与节点具有相同op_type（个人理解是输出输出的数据类型相同就关联起来了）的插件，解析器将节点的属性作为插件字段参数转发给插件创建者，以便创建插件。默认情况下，解析器使用1作为插件版本和“”作为插件名称空间。通过在相应的ONNX节点中设置插件版本和插件命名空间字符串属性，可以覆盖此行为。</p>
<h2 id="2-4-Plugin-API"><a href="#2-4-Plugin-API" class="headerlink" title="2.4 Plugin API"></a>2.4 Plugin API</h2><p>所有新的插件都应该从IPluginCreator和使用c++  API添加自定义层中描述的一个插件基类派生类。另外，新的插件还应该调用REGISTER TENSORRT  PLUGIN(…)宏来将插件注册到TENSORRT插件注册表中，或者创建一个等同于initLibNvInferPlugins()的init函数。</p>
<h3 id="2-4-1-IPluginV2-API描述"><a href="#2-4-1-IPluginV2-API描述" class="headerlink" title="2.4.1 IPluginV2 API描述"></a>2.4.1 IPluginV2 API描述</h3><p>下面的部分描述IPluginV2类的功能。为了将插件层连接到相邻层并设置输入和输出数据结构，构建器通过调用以下插件方法检查输出的数量及其维度。</p>
<p><strong><code>getNbOutputs</code></strong></p>
<p>用于指定输出张量的数量。</p>
<p><strong><code>getOutputDimensions</code></strong></p>
<p>用于将输出的维度指定为输入维度的函数。</p>
<p><strong><code>supportsFormat</code></strong></p>
<p>用于检查插件是否支持给定的数据格式。</p>
<p><strong><code>getOutputDataType</code></strong></p>
<p>用于获取给定索引处输出的数据类型。返回的数据类型必须具有插件支持的格式。</p>
<p>插件层可以支持四种数据格式，例如：</p>
<ul>
<li>NCHW单精度 (FP32)、半精度 (FP16) 和整型 (INT32) 张量</li>
<li>NC / 2HW2和NHWC8半精度 (FP16) 张量</li>
</ul>
<p>格式由 <code>PluginFormatType</code>枚举。</p>
<p><strong><code>getWorkspaceSize</code></strong></p>
<p>除了输入和输出张量之外，不计算所有数据并且需要内存空间的插件可以使用 <code>getWorkspaceSize</code>方法指定额外的内存需求，该方法由构建器调用以确定和预分配暂存空间。</p>
<p>在构建和推理期间，可能会多次配置和执行插件层。在构建时，为了发现最佳配置，层被配置、初始化、执行和终止。为插件选择最佳格式后，再次配置插件，然后在推理应用程序的生命周期内初始化一次并执行多次，最后在引擎销毁时终止。</p>
<p>这些步骤由构建器和引擎使用以下插件方法控制：</p>
<p><strong><code>configurePlugin</code>(配置插件)</strong></p>
<p>传达输入和输出的数量、所有输入和输出的维度和数据类型、所有输入和输出的广播信息、选择的插件格式和最大批量大小。此时，插件设置其内部状态并为给定配置选择最合适的算法和数据结构。</p>
<p><strong><code>initialize</code>(初始化)</strong></p>
<p>此时配置是已知的，并且正在创建推理引擎，因此插件可以设置其内部数据结构并准备执行。</p>
<p><strong><code>enqueue</code>(排队)</strong></p>
<p>封装插件的实际算法和内核调用，并提供运行时批处理大小、指向输入、输出和暂存空间的指针，以及用于内核执行的CUDA流。</p>
<p><strong><code>terminate</code>(终止)</strong></p>
<p>引擎上下文被销毁，插件持有的所有资源必须被释放。</p>
<p><strong><code>clone</code>(克隆)</strong></p>
<p>每次创建包含此插件层的新构建器、网络或引擎时都会调用它。它必须返回一个带有正确参数的新插件对象。</p>
<p><strong><code>destroy</code>(销毁)</strong></p>
<p>用于销毁插件对象和/或每次创建新插件对象时分配的其他内存。每当构建器或网络或引擎被破坏时都会调用它。</p>
<p><strong><code>set/getPluginNamespace</code></strong></p>
<p>该方法用于设置该插件对象所属的库命名空间（默认可以是“”）。来自同一个插件库的所有插件对象都应该具有相同的命名空间。</p>
<p><code>IPluginV2Ext</code>支持可以处理广播输入和输出的插件。此功能需要实现以下方法：</p>
<p><strong><code>canBroadcastInputAcrossBatch</code></strong></p>
<p>对每个输入调用此方法，其张量在批次中进行语义广播。如果 <code>canBroadcastInputAcrossBatch</code>返回true （意味着插件可以支持广播），则 TensorRT 不会复制输入张量。插件应该在批处理中共享一个副本。如果它返回false ，则 TensorRT 会复制输入张量，使其看起来像一个非广播张量。</p>
<p><strong><code>isOutputBroadcastAcrossBatch</code></strong></p>
<p>这为每个输出索引调用。该插件应在给定索引处返回 true 输出，并在整个批次中广播。</p>
<p><strong><code>IPluginV2IOExt</code></strong> 这由构建器在 <code>initialize()</code>之前调用。它为层提供了基于 I/O PluginTensorDesc和最大批量大小进行算法选择的机会。</p>
<p>注意：基于 <code>IPluginV2</code>的插件在引擎级别共享，而不是在执行上下文级别共享，因此这些可能被多个线程同时使用的插件需要以线程安全的方式管理它们的资源。创建 <code>ExecutionContext</code>时会克隆基于 <code>IPluginV2Ext</code>和派生接口的插件，因此这不是必需的。</p>
<h3 id="2-4-2-IPluginCreator-API描述"><a href="#2-4-2-IPluginCreator-API描述" class="headerlink" title="2.4.2. IPluginCreator API描述"></a>2.4.2. IPluginCreator API描述</h3><p><code>IPluginCreator</code>类中的以下方法用于从插件注册表中查找和创建适当的插件：</p>
<p><strong><code>getPluginName</code></strong></p>
<p>这将返回插件名称，并且应该与IPluginExt::getPluginType的返回值匹配。</p>
<p><strong><code>getPluginVersion</code></strong></p>
<p>返回插件版本。对于所有内部 TensorRT 插件，默认为1 。</p>
<p><strong><code>getFieldNames</code></strong></p>
<p>要成功创建插件，需要了解插件的所有字段参数。此方法返回 <code>PluginFieldCollection</code>结构，其中填充了 <code>PluginField</code>条目以反映字段名称和 <code>PluginFieldType</code> （数据应指向 <code>nullptr</code> ）。</p>
<p><strong><code>createPlugin</code></strong></p>
<p>此方法用于使用 <code>PluginFieldCollection</code>参数创建插件。应填充 <code>PluginField</code>条目的数据字段以指向每个插件字段条目的实际数据。</p>
<p>注意：传递给 <code>createPlugin</code>函数的数据应该由调用者分配，并在程序被销毁时最终由调用者释放。 <code>createPlugin</code>函数返回的插件对象的所有权被传递给调用者，并且也必须被销毁。</p>
<p><strong><code>deserializePlugin</code></strong></p>
<p>此方法由 TensorRT 引擎根据插件名称和版本在内部调用。它应该返回要用于推理的插件对象。在该函数中创建的插件对象在引擎被销毁时被 TensorRT 引擎销毁。</p>
<p><strong><code>set/getPluginNamespace</code></strong> 该方法用于设置此创建者实例所属的命名空间（默认可以是“”）。</p>
<h1 id="3-Plugin编写流程"><a href="#3-Plugin编写流程" class="headerlink" title="3 Plugin编写流程"></a>3 Plugin编写流程</h1><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/297002406">博客参考</a></p>
<p>上面的博客参考了官方的Plugin对应的，这个例子的文件在我本机的 <code>/home/huolin/github/TensorRT/plugin/normalizePlugin/</code> 路经下</p>
<p>TODO 有的下面有cu cpp h文件 但是有的只有cpp和h文件，不知道为什么</p>
<img src="/TensorRT/TensorRT-plugin/image-20220915170932921.png" class="" title="image-20220915170932921">
<p>官方的例子有些变化，有点头大，这里就以下面的例子为参考</p>
<p>就是开源的文档trt-samples-for-hackathon-cn-master/cookbook/05-Plugin</p>
<img src="/TensorRT/TensorRT-plugin/image-20220915171418072.png" class="" title="image-20220915171418072">
<p>步骤：</p>
<ol>
<li>继承 IPluginV2DynamicExt 类实现一个Plugin 类</li>
<li>继承 IPluginCreator 类实现一个 PluginCreator 类</li>
<li>实现用于计算的 CUDA C++ kernel</li>
<li>将 Plugin 编译为 .so 保存</li>
<li>在 TenorRT 中加载和使用 Plugin</li>
</ol>
<img src="/TensorRT/TensorRT-plugin/image-20220915191025034.png" class="" title="image-20220915191025034">
<h2 id="3-1-调用关系"><a href="#3-1-调用关系" class="headerlink" title="3.1 调用关系"></a>3.1 调用关系</h2><ol>
<li>调用IPluginCreator::createPlugin()返回一个类型为IPluginV2的插件对象。</li>
<li>在反序列化期间，TensorRT从插件注册表中查找插件创建者并调用IPluginCreator::deserializePlugin()。</li>
</ol>
<h3 id="3-1-1-build-engine时"><a href="#3-1-1-build-engine时" class="headerlink" title="3.1.1 build  engine时"></a>3.1.1 build  engine时</h3><ol>
<li><p>initLibNvInferPlugins 调用 addPluginCreator 注册plugin ;调用一次</p>
<ul>
<li><p>addPluginCreator 里面参照官方模板调用</p>
<p>IPluginCreator::setPluginNamespace<br>IPluginCreator::getPluginNamespace<br>IPluginCreator::getPluginName<br>IPluginCreator::getPluginVersion</p>
</li>
<li>IPluginCreator::getFieldNames</li>
<li>IPluginCreator::createPlugin：里面new了一个 <code>IPluginV2</code> ;调用一次</li>
<li>Plugin::getNbOutputs;调用多次</li>
<li>Plugin::getOutputDataType;调用多次</li>
<li>Plugin::clone：里面new了一个 <code>IPluginV2</code>和上面的不同;调用多次</li>
<li>Plugin::destroy</li>
<li>析构IPluginV2</li>
<li>Plugin::getOutputDataType;调用多次</li>
<li>Plugin::getOutputDimensions;调用多次</li>
<li>readCalibrationCache</li>
<li>Plugin::supportsFormatCombination</li>
<li>Plugin::clone</li>
<li>Plugin::setPluginNamespace</li>
<li>Plugin::configurePlugin</li>
<li>Plugin::getWorkspaceSize</li>
<li>Plugin::destroy</li>
<li>析构IPluginV2</li>
<li>Plugin::initialize</li>
<li>Plugin::destroy</li>
<li>析构IPluginV2</li>
<li>Plugin::getPluginType</li>
<li>Plugin::getPluginVersion</li>
<li>Plugin::getPluginNamespace</li>
<li>Plugin::getSerializationSize</li>
<li>Plugin::serialize</li>
<li>Plugin::getSerializationSize</li>
<li>Plugin::getPluginType</li>
<li>Plugin::getPluginVersion</li>
<li>Plugin::getPluginNamespace</li>
<li>Plugin::getSerializationSize</li>
<li>Plugin::serialize</li>
<li>Plugin::getSerializationSize</li>
<li>Plugin::terminate</li>
<li>Plugin::destroy</li>
<li>~Plugin</li>
<li>Plugin::destroy</li>
<li>~Plugin</li>
</ul>
</li>
</ol>
<h3 id="3-1-2-加载和推理-engine时"><a href="#3-1-2-加载和推理-engine时" class="headerlink" title="3.1.2 加载和推理  engine时"></a>3.1.2 加载和推理  engine时</h3><ol>
<li><p>initLibNvInferPlugins 调用 addPluginCreator 注册plugin ;调用一次</p>
<ul>
<li><p>addPluginCreator 里面参照官方模板调用</p>
<p>IPluginCreator::setPluginNamespace<br>IPluginCreator::getPluginNamespace<br>IPluginCreator::getPluginName<br>IPluginCreator::getPluginVersion</p>
</li>
</ul>
</li>
<li><p>应该是deserializeCudaEngine调用了deserializePlugin;调用一次</p>
<ul>
<li>deserializePlugin里面new了一个 <code>IPluginV2</code>. It should return the plugin object to be used for                                    inference。还调用了setPluginNamespace</li>
</ul>
</li>
<li>initialize:  TensorRT 引擎构建过程中被调用;调用一次</li>
<li>clone: 每次创建包含此插件层的新构建器、网络或引擎时都会调用此函数。它必须返回一个具有正确参数的新插件对象。new了一个 <code>IPluginV2</code>,和上面的deserializePlugin调用的不是一个new;调用一次</li>
<li>attachToContext;调用一次</li>
<li>configurePlugin;调用一次</li>
<li>enqueue;推理一次调用一次。频繁的调用。</li>
</ol>
<h2 id="3-2-PluginCreator"><a href="#3-2-PluginCreator" class="headerlink" title="3.2 PluginCreator"></a>3.2 PluginCreator</h2><p>功能比较简单，套用参考的结构就行。</p>
<p>IPluginCreator 类中的以下方法用于从插件注册表查找并创建适当的插件：</p>
<h3 id="3-2-1-getPluginName"><a href="#3-2-1-getPluginName" class="headerlink" title="3.2.1 getPluginName"></a>3.2.1 getPluginName</h3><p>这将返回插件名称，并且应与 <code>IPluginExt::getPluginType</code>的返回值匹配。</p>
<h3 id="3-2-2-getPluginVersion"><a href="#3-2-2-getPluginVersion" class="headerlink" title="3.2.2 getPluginVersion"></a>3.2.2 getPluginVersion</h3><p>返回插件版本。对于所有内部 TensorRT 插件，该值默认为 1。</p>
<h3 id="3-2-3-getFieldNames"><a href="#3-2-3-getFieldNames" class="headerlink" title="3.2.3 getFieldNames"></a>3.2.3 getFieldNames</h3><p>要成功创建插件，需要了解插件的所有字段参数。此方法返回 PluginFieldCollection 结构，其中填充了 PluginField 条目以反映字段名称和 PluginFieldType（数据应指向 nullptr）。</p>
<p>从上面的描述可以看出PluginCreator最重要的两个函数就是</p>
<h3 id="3-2-3-createPlugin"><a href="#3-2-3-createPlugin" class="headerlink" title="3.2.3 createPlugin"></a>3.2.3 createPlugin</h3><p>此方法用于使用 PluginFieldCollection 参数创建插件。应填充 PluginField 条目的数据字段以指向每个插件字段条目的实际数据。（从模型读取参数）</p>
<p>其中new了一个 <code>IPluginV2</code>这个例子中就是 <code>AddScalarPlugin</code></p>
<ul>
<li><p><code>deserializePlugin</code></p>
<p>其中new了一个 <code>IPluginV2</code>这个例子中就是 <code>AddScalarPlugin</code>，但是<strong>构造函数不同</strong></p>
</li>
<li><p>还有一个重要的注册宏REGISTER_TENSORRT_PLUGIN 用于注册PluginCreator</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">REGISTER_TENSORRT_PLUGIN</span>(AddScalarPluginCreator);<span class="comment">//注册Creator</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="3-2-4-deserializePlugin"><a href="#3-2-4-deserializePlugin" class="headerlink" title="3.2.4 deserializePlugin"></a>3.2.4 deserializePlugin</h3><p>该方法由 TensorRT 引擎根据插件名称和版本在内部调用。它应该返回用于推理的插件对象。当TensorRT引擎被销毁时，在此函数中创建的插件对象也会被TensorRT引擎销毁。</p>
<h3 id="3-2-5-set-getPluginNamespace"><a href="#3-2-5-set-getPluginNamespace" class="headerlink" title="3.2.5 set/getPluginNamespace"></a>3.2.5 set/getPluginNamespace</h3><p>该方法用于设置该创建者实例所属的命名空间（默认可以是“”）。</p>
<p>提取PluginCreator相关的代码</p>
<p>头文件中</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//step 2 继承IPluginCreator实现一个PluginCreator类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AddScalarPluginCreator</span> : <span class="keyword">public</span> IPluginCreator</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">static</span> PluginFieldCollection    fc_;</span><br><span class="line">    <span class="type">static</span> std::vector&lt;PluginField&gt; attr_;</span><br><span class="line">    std::string                     namespace_;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">AddScalarPluginCreator</span>();</span><br><span class="line">    ~<span class="built_in">AddScalarPluginCreator</span>();</span><br><span class="line">    <span class="function"><span class="type">const</span> <span class="type">char</span> *                 <span class="title">getPluginName</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> <span class="type">char</span> *                 <span class="title">getPluginVersion</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> PluginFieldCollection *<span class="title">getFieldNames</span><span class="params">()</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function">IPluginV2 *                  <span class="title">createPlugin</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *name, <span class="type">const</span> PluginFieldCollection *fc)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function">IPluginV2 *                  <span class="title">deserializePlugin</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *name, <span class="type">const</span> <span class="type">void</span> *serialData, <span class="type">size_t</span> serialLength)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>                         <span class="title">setPluginNamespace</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *pluginNamespace)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> <span class="type">char</span> *                 <span class="title">getPluginNamespace</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>cpp文件中</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// class AddScalarPluginCreator</span></span><br><span class="line">PluginFieldCollection    AddScalarPluginCreator::fc_ &#123;&#125;;</span><br><span class="line">std::vector&lt;PluginField&gt; AddScalarPluginCreator::attr_;</span><br><span class="line"></span><br><span class="line">AddScalarPluginCreator::<span class="built_in">AddScalarPluginCreator</span>()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    attr_.<span class="built_in">clear</span>();</span><br><span class="line">    attr_.<span class="built_in">emplace_back</span>(<span class="built_in">PluginField</span>(<span class="string">&quot;scalar&quot;</span>, <span class="literal">nullptr</span>, PluginFieldType::kFLOAT32, <span class="number">1</span>));</span><br><span class="line">    fc_.nbFields = attr_.<span class="built_in">size</span>();</span><br><span class="line">    fc_.fields   = attr_.<span class="built_in">data</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">AddScalarPluginCreator::~<span class="built_in">AddScalarPluginCreator</span>()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 最重要的两个成员函数，分别用于“接受参数创建 Plugin” 和 “去序列化创建 Plugin”</span></span><br><span class="line"><span class="function">IPluginV2 *<span class="title">AddScalarPluginCreator::createPlugin</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *name, <span class="type">const</span> PluginFieldCollection *fc)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="type">float</span>                          scalar = <span class="number">0</span>;</span><br><span class="line">    std::map&lt;std::string, <span class="type">float</span> *&gt; parameterMap &#123;&#123;<span class="string">&quot;scalar&quot;</span>, &amp;scalar&#125;&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; fc-&gt;nbFields; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (parameterMap.<span class="built_in">find</span>(fc-&gt;fields[i].name) != parameterMap.<span class="built_in">end</span>())</span><br><span class="line">        &#123;</span><br><span class="line">            *parameterMap[fc-&gt;fields[i].name] = *<span class="built_in">reinterpret_cast</span>&lt;<span class="type">const</span> <span class="type">float</span> *&gt;(fc-&gt;fields[i].data);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">//new了plugin</span></span><br><span class="line">    AddScalarPlugin *pObj = <span class="keyword">new</span> <span class="built_in">AddScalarPlugin</span>(name, scalar);</span><br><span class="line">    pObj-&gt;<span class="built_in">setPluginNamespace</span>(namespace_.<span class="built_in">c_str</span>());</span><br><span class="line">    <span class="keyword">return</span> pObj;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//第二个重要的函数</span></span><br><span class="line"><span class="function">IPluginV2 *<span class="title">AddScalarPluginCreator::deserializePlugin</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *name, <span class="type">const</span> <span class="type">void</span> *serialData, <span class="type">size_t</span> serialLength)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">	<span class="comment">//new了plugin 和上面的不一样，是两个构造函数</span></span><br><span class="line">    AddScalarPlugin *pObj = <span class="keyword">new</span> <span class="built_in">AddScalarPlugin</span>(name, serialData, serialLength);</span><br><span class="line">    pObj-&gt;<span class="built_in">setPluginNamespace</span>(namespace_.<span class="built_in">c_str</span>());</span><br><span class="line">    <span class="keyword">return</span> pObj;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPluginCreator::setPluginNamespace</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *pluginNamespace)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    namespace_ = pluginNamespace;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">AddScalarPluginCreator::getPluginNamespace</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> namespace_.<span class="built_in">c_str</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">AddScalarPluginCreator::getPluginName</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> PLUGIN_NAME;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">AddScalarPluginCreator::getPluginVersion</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> PLUGIN_VERSION;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> PluginFieldCollection *<span class="title">AddScalarPluginCreator::getFieldNames</span><span class="params">()</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> &amp;fc_;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">REGISTER_TENSORRT_PLUGIN</span>(AddScalarPluginCreator);<span class="comment">//注册Creator</span></span><br><span class="line"></span><br><span class="line">&#125; <span class="comment">// namespace nvinfer1</span></span><br></pre></td></tr></table></figure>
<h2 id="3-3-Plugin类代码分析"><a href="#3-3-Plugin类代码分析" class="headerlink" title="3.3 Plugin类代码分析"></a>3.3 Plugin类代码分析</h2><p>上面的PluginCreator两次new了plugin，貌似就是注册和对接使用，功能实现都是这里的plugin的代码</p>
<p>头文件</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cookbookHelper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">char</span> *PLUGIN_NAME &#123;<span class="string">&quot;AddScalar&quot;</span>&#125;;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">char</span> *PLUGIN_VERSION &#123;<span class="string">&quot;1&quot;</span>&#125;;</span><br><span class="line">&#125; <span class="comment">// namespace</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> nvinfer1</span><br><span class="line">&#123;<span class="comment">// step1 继承IPluginV2DynamicExt实现一个Plugin类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AddScalarPlugin</span> : <span class="keyword">public</span> IPluginV2DynamicExt</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">const</span> std::string name_;</span><br><span class="line">    std::string       namespace_;</span><br><span class="line">    <span class="keyword">struct</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">float</span> scalar;</span><br><span class="line">    &#125; m_;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">AddScalarPlugin</span>() = <span class="keyword">delete</span>;</span><br><span class="line">    <span class="built_in">AddScalarPlugin</span>(<span class="type">const</span> std::string &amp;name, <span class="type">float</span> scalar);</span><br><span class="line">    <span class="built_in">AddScalarPlugin</span>(<span class="type">const</span> std::string &amp;name, <span class="type">const</span> <span class="type">void</span> *buffer, <span class="type">size_t</span> length);</span><br><span class="line">    ~<span class="built_in">AddScalarPlugin</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Method inherited from IPluginV2</span></span><br><span class="line">    <span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">getPluginType</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">getPluginVersion</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">int32_t</span>     <span class="title">getNbOutputs</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">int32_t</span>     <span class="title">initialize</span><span class="params">()</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>        <span class="title">terminate</span><span class="params">()</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">size_t</span>      <span class="title">getSerializationSize</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>        <span class="title">serialize</span><span class="params">(<span class="type">void</span> *buffer)</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>        <span class="title">destroy</span><span class="params">()</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>        <span class="title">setPluginNamespace</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *pluginNamespace)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">getPluginNamespace</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Method inherited from IPluginV2Ext</span></span><br><span class="line">    <span class="function">DataType <span class="title">getOutputDataType</span><span class="params">(<span class="type">int32_t</span> index, DataType <span class="type">const</span> *inputTypes, <span class="type">int32_t</span> nbInputs)</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>     <span class="title">attachToContext</span><span class="params">(cudnnContext *contextCudnn, cublasContext *contextCublas, IGpuAllocator *gpuAllocator)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>     <span class="title">detachFromContext</span><span class="params">()</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Method inherited from IPluginV2DynamicExt</span></span><br><span class="line">    <span class="function">IPluginV2DynamicExt *<span class="title">clone</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function">DimsExprs            <span class="title">getOutputDimensions</span><span class="params">(<span class="type">int32_t</span> outputIndex, <span class="type">const</span> DimsExprs *inputs, <span class="type">int32_t</span> nbInputs, IExprBuilder &amp;exprBuilder)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">bool</span>                 <span class="title">supportsFormatCombination</span><span class="params">(<span class="type">int32_t</span> pos, <span class="type">const</span> PluginTensorDesc *inOut, <span class="type">int32_t</span> nbInputs, <span class="type">int32_t</span> nbOutputs)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>                 <span class="title">configurePlugin</span><span class="params">(<span class="type">const</span> DynamicPluginTensorDesc *in, <span class="type">int32_t</span> nbInputs, <span class="type">const</span> DynamicPluginTensorDesc *out, <span class="type">int32_t</span> nbOutputs)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">size_t</span>               <span class="title">getWorkspaceSize</span><span class="params">(<span class="type">const</span> PluginTensorDesc *inputs, <span class="type">int32_t</span> nbInputs, <span class="type">const</span> PluginTensorDesc *outputs, <span class="type">int32_t</span> nbOutputs)</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">int32_t</span>              <span class="title">enqueue</span><span class="params">(<span class="type">const</span> PluginTensorDesc *inputDesc, <span class="type">const</span> PluginTensorDesc *outputDesc, <span class="type">const</span> <span class="type">void</span> *<span class="type">const</span> *inputs, <span class="type">void</span> *<span class="type">const</span> *outputs, <span class="type">void</span> *workspace, cudaStream_t stream)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>cpp文件</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;AddScalarPlugin.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//step 3 实现计算cuda的核函数</span></span><br><span class="line"><span class="comment">// 用于计算的 kernel</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">addScalarKernel</span><span class="params">(<span class="type">const</span> <span class="type">float</span> *input, <span class="type">float</span> *output, <span class="type">const</span> <span class="type">float</span> scalar, <span class="type">const</span> <span class="type">int</span> nElement)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (index &gt;= nElement)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> _1      = input[index];</span><br><span class="line">    <span class="type">float</span> _2      = _1 + scalar;</span><br><span class="line">    output[index] = _2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> nvinfer1</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// 这里各成员函数按照被调用顺序或重要程度顺序排列</span></span><br><span class="line"><span class="comment">// class AddScalarPlugin</span></span><br><span class="line">AddScalarPlugin::<span class="built_in">AddScalarPlugin</span>(<span class="type">const</span> std::string &amp;name, <span class="type">float</span> scalar):</span><br><span class="line">    <span class="built_in">name_</span>(name)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    m_.scalar = scalar;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">AddScalarPlugin::<span class="built_in">AddScalarPlugin</span>(<span class="type">const</span> std::string &amp;name, <span class="type">const</span> <span class="type">void</span> *buffer, <span class="type">size_t</span> length):</span><br><span class="line">    <span class="built_in">name_</span>(name)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="built_in">memcpy</span>(&amp;m_, buffer, <span class="built_in">sizeof</span>(m_));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">AddScalarPlugin::~<span class="built_in">AddScalarPlugin</span>()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">IPluginV2DynamicExt *<span class="title">AddScalarPlugin::clone</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">auto</span> p = <span class="keyword">new</span> <span class="built_in">AddScalarPlugin</span>(name_, &amp;m_, <span class="built_in">sizeof</span>(m_));</span><br><span class="line">    p-&gt;<span class="built_in">setPluginNamespace</span>(namespace_.<span class="built_in">c_str</span>());</span><br><span class="line">    <span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int32_t</span> <span class="title">AddScalarPlugin::getNbOutputs</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">DataType <span class="title">AddScalarPlugin::getOutputDataType</span><span class="params">(<span class="type">int32_t</span> index, DataType <span class="type">const</span> *inputTypes, <span class="type">int32_t</span> nbInputs)</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> inputTypes[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">DimsExprs <span class="title">AddScalarPlugin::getOutputDimensions</span><span class="params">(<span class="type">int32_t</span> outputIndex, <span class="type">const</span> DimsExprs *inputs, <span class="type">int32_t</span> nbInputs, IExprBuilder &amp;exprBuilder)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> inputs[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">AddScalarPlugin::supportsFormatCombination</span><span class="params">(<span class="type">int32_t</span> pos, <span class="type">const</span> PluginTensorDesc *inOut, <span class="type">int32_t</span> nbInputs, <span class="type">int32_t</span> nbOutputs)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> DEBUG</span></span><br><span class="line">    <span class="type">bool</span> res;</span><br><span class="line">    <span class="keyword">switch</span> (pos)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">        res = inOut[<span class="number">0</span>].type == DataType::kFLOAT &amp;&amp; inOut[<span class="number">0</span>].format == TensorFormat::kLINEAR;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">        res = inOut[<span class="number">1</span>].format == inOut[<span class="number">0</span>].format &amp;&amp; inOut[<span class="number">1</span>].type == inOut[<span class="number">0</span>].type;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>: <span class="comment">// should NOT be here!</span></span><br><span class="line">        res = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;\tpos=&quot;</span> &lt;&lt; pos &lt;&lt; <span class="string">&quot;,res=&quot;</span> &lt;&lt; res &lt;&lt; <span class="string">&quot;-&gt;[&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nbInputs + nbOutputs; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">getFormatString</span>(inOut[i].format) &lt;&lt; <span class="string">&quot;,&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;],[&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nbInputs + nbOutputs; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">dataTypeToString</span>(inOut[i].type) &lt;&lt; <span class="string">&quot;,&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;]&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    <span class="keyword">switch</span> (pos)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> inOut[<span class="number">0</span>].type == DataType::kFLOAT &amp;&amp; inOut[<span class="number">0</span>].format == TensorFormat::kLINEAR;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> inOut[<span class="number">1</span>].type == inOut[<span class="number">0</span>].type &amp;&amp; inOut[<span class="number">1</span>].format == inOut[<span class="number">0</span>].format;</span><br><span class="line">    <span class="keyword">default</span>: <span class="comment">// should NOT be here!</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::configurePlugin</span><span class="params">(<span class="type">const</span> DynamicPluginTensorDesc *in, <span class="type">int32_t</span> nbInputs, <span class="type">const</span> DynamicPluginTensorDesc *out, <span class="type">int32_t</span> nbOutputs)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">size_t</span> <span class="title">AddScalarPlugin::getWorkspaceSize</span><span class="params">(<span class="type">const</span> PluginTensorDesc *inputs, <span class="type">int32_t</span> nbInputs, <span class="type">const</span> PluginTensorDesc *outputs, <span class="type">int32_t</span> nbOutputs)</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int32_t</span> <span class="title">AddScalarPlugin::enqueue</span><span class="params">(<span class="type">const</span> PluginTensorDesc *inputDesc, <span class="type">const</span> PluginTensorDesc *outputDesc, <span class="type">const</span> <span class="type">void</span> *<span class="type">const</span> *inputs, <span class="type">void</span> *<span class="type">const</span> *outputs, <span class="type">void</span> *workspace, cudaStream_t stream)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="type">int</span> nElement = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; inputDesc[<span class="number">0</span>].dims.nbDims; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        nElement *= inputDesc[<span class="number">0</span>].dims.d[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(CEIL_DIVIDE(nElement, <span class="number">256</span>), <span class="number">1</span>, <span class="number">1</span>)</span>, <span class="title">block</span><span class="params">(<span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">    addScalarKernel&lt;&lt;&lt;grid, block, <span class="number">0</span>, stream&gt;&gt;&gt;(<span class="built_in">reinterpret_cast</span>&lt;<span class="type">const</span> <span class="type">float</span> *&gt;(inputs[<span class="number">0</span>]), <span class="built_in">reinterpret_cast</span>&lt;<span class="type">float</span> *&gt;(outputs[<span class="number">0</span>]), m_.scalar, nElement);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::destroy</span><span class="params">()</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">delete</span> <span class="keyword">this</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int32_t</span> <span class="title">AddScalarPlugin::initialize</span><span class="params">()</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::terminate</span><span class="params">()</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">size_t</span> <span class="title">AddScalarPlugin::getSerializationSize</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sizeof</span>(m_);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::serialize</span><span class="params">(<span class="type">void</span> *buffer)</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="built_in">memcpy</span>(buffer, &amp;m_, <span class="built_in">sizeof</span>(m_));</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::setPluginNamespace</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *pluginNamespace)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    namespace_ = pluginNamespace;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">AddScalarPlugin::getPluginNamespace</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> namespace_.<span class="built_in">c_str</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">AddScalarPlugin::getPluginType</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> PLUGIN_NAME;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">AddScalarPlugin::getPluginVersion</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> PLUGIN_VERSION;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::attachToContext</span><span class="params">(cudnnContext *contextCudnn, cublasContext *contextCublas, IGpuAllocator *gpuAllocator)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::detachFromContext</span><span class="params">()</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-1-构造函数"><a href="#3-3-1-构造函数" class="headerlink" title="3.3.1 构造函数"></a>3.3.1 构造函数</h3><p>构造函数一般设置为三个。</p>
<p>第一个用于在parse阶段，<code>PluginCreator</code>用于创建该插件时调用的构造函数，需要传递权重信息以及参数。对应上面Create的 <code>createPlugin</code>函数</p>
<p>第二个用于在 <code>clone</code>阶段，复制这个plugin时会用到的构造函数。这个例子没有</p>
<p>第三个用于在 <code>deserialize</code>阶段，用于将序列化好的权重和参数传入该plugin并创建。对应上面Create的 <code>deserializePlugin</code></p>
<p>这里的两个构造函数就是</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">AddScalarPlugin</span>(<span class="type">const</span> std::string &amp;name, <span class="type">float</span> scalar);</span><br><span class="line"><span class="built_in">AddScalarPlugin</span>(<span class="type">const</span> std::string &amp;name, <span class="type">const</span> <span class="type">void</span> *buffer, <span class="type">size_t</span> length);</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>析构函数则需要执行 <code>terminate</code>，<code>terminate</code>函数就是释放这个op之前开辟的一些显存空间:但是这里没有</p>
<p>例如</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MyCustomPlugin::~<span class="built_in">MyCustomPlugin</span>()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">terminate</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-2-clone"><a href="#3-3-2-clone" class="headerlink" title="3.3.2 clone"></a>3.3.2 clone</h3><p>这玩意儿干嘛的，顾名思义，就是克隆嘛，将这个 <code>plugin</code>对象克隆一份给TensorRT的builder、network或者engine。</p>
<p>（创建多个 context ，可以与源对象共享本 engine 的资源）克隆通常发生在 TensorRT 引擎构建时，引擎需要在不同的执行上下文（execution context）中使用相同的插件。为了确保插件的正确性和一致性，TensorRT 会调用插件的 <code>clone</code> 函数来创建一个插件的副本，该副本将在不同的执行上下文中使用。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">IPluginV2DynamicExt *<span class="title">AddScalarPlugin::clone</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">auto</span> p = <span class="keyword">new</span> <span class="built_in">AddScalarPlugin</span>(name_, &amp;m_, <span class="built_in">sizeof</span>(m_));</span><br><span class="line">    p-&gt;<span class="built_in">setPluginNamespace</span>(namespace_.<span class="built_in">c_str</span>());</span><br><span class="line">    <span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-3-getNbOutputs"><a href="#3-3-3-getNbOutputs" class="headerlink" title="3.3.3 getNbOutputs"></a>3.3.3 getNbOutputs</h3><p>插件op返回多少个Tensor，比如 <code>AddScalarPlugin</code>这个操作只输出一个Tensor(也就是一个output)，所以直接 <code>return 1</code>：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int32_t</span> <span class="title">AddScalarPlugin::getNbOutputs</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-4-getOutputDataType"><a href="#3-3-4-getOutputDataType" class="headerlink" title="3.3.4 getOutputDataType"></a>3.3.4 <strong>getOutputDataType</strong></h3><p>返回结果的类型，一般来说我们插件op返回结果类型与输入类型一致：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">DataType <span class="title">AddScalarPlugin::getOutputDataType</span><span class="params">(<span class="type">int32_t</span> index, DataType <span class="type">const</span> *inputTypes, <span class="type">int32_t</span> nbInputs)</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> inputTypes[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-5-getOutputDimensions"><a href="#3-3-5-getOutputDimensions" class="headerlink" title="3.3.5 getOutputDimensions"></a>3.3.5 <strong>getOutputDimensions</strong></h3><ul>
<li>向 TensorRT 报告每个输出张量的形状。</li>
<li>TensorRT支持Dynamic-shape的时候，batch这一维度必须是explicit的，也就是说，TensorRT处理的维度从以往的三维[3,-1,-1]变成了[1,3,-1,-1]。最新的onnx-tensorrt也必须设置explicit的batchsize，而且这个batch维度在 <code>getOutputDimensions</code>中是可以获取到的。</li>
</ul>
<p>在旧版的IPluginV2类中，getOutputDimensions的定义如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">virtual</span> Dims <span class="title">getOutputDimensions</span><span class="params">(<span class="type">int</span> index, <span class="type">const</span> Dims* inputs, <span class="type">int</span> nbInputDims)</span> TRTNOEXCEPT </span>= <span class="number">0</span>;</span><br></pre></td></tr></table></figure>
<p>而在新版的IPluginV2DynamicExt类中定义如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">virtual</span> DimsExprs <span class="title">getOutputDimensions</span><span class="params">(<span class="type">int</span> outputIndex, <span class="type">const</span> DimsExprs* inputs, <span class="type">int</span> nbInputs, IExprBuilder&amp; exprBuilder)</span> </span>= <span class="number">0</span>;</span><br></pre></td></tr></table></figure>
<p>我们要做的就是在这个成员函数中根据输入维度推理出模型的输出维度，需要注意的是，虽然说输出维度 是由输入维度决定，但这个<strong>输出维度其实“内定”的</strong>(也就是在计算之前就算出来了)。如果咱的插件op的输出维度需要通过实际运行计算得到，那么这个函数就无法满足咱了。</p>
<p>这里的例中</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">DimsExprs <span class="title">AddScalarPlugin::getOutputDimensions</span><span class="params">(<span class="type">int32_t</span> outputIndex, <span class="type">const</span> DimsExprs *inputs, <span class="type">int32_t</span> nbInputs, IExprBuilder &amp;exprBuilder)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> inputs[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-6-supportsFormatCombination"><a href="#3-3-6-supportsFormatCombination" class="headerlink" title="3.3.6 supportsFormatCombination"></a>3.3.6 <strong>supportsFormatCombination</strong></h3><p>TensorRT调用此方法以判断pos索引的输入/输出是否支持 <code>inOut[pos].format</code>和 <code>inOut[pos].type</code>指定的格式/数据类型。</p>
<p>如果插件支持 <code>inOut[pos]</code>处的格式/数据类型，则返回true。 如果 <code>是否支持</code>取决于其他的输入/输出格式/数据类型，则插件可以使其结果取决于 <code>inOut[0..pos-1]</code>中的格式/数据类型，该格式/数据类型将设置为插件支持的值。 这个函数不需要检查 <code>inOut[pos + 1..nbInputs + nbOutputs-1]</code>，pos的决定必须仅基于 <code>inOut[0..pos]</code>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">AddScalarPlugin::supportsFormatCombination</span><span class="params">(<span class="type">int32_t</span> pos, <span class="type">const</span> PluginTensorDesc *inOut, <span class="type">int32_t</span> nbInputs, <span class="type">int32_t</span> nbOutputs)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> DEBUG</span></span><br><span class="line">    <span class="type">bool</span> res;</span><br><span class="line">    <span class="keyword">switch</span> (pos)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">        res = inOut[<span class="number">0</span>].type == DataType::kFLOAT &amp;&amp; inOut[<span class="number">0</span>].format == TensorFormat::kLINEAR;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">        res = inOut[<span class="number">1</span>].format == inOut[<span class="number">0</span>].format &amp;&amp; inOut[<span class="number">1</span>].type == inOut[<span class="number">0</span>].type;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>: <span class="comment">// should NOT be here!</span></span><br><span class="line">        res = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;\tpos=&quot;</span> &lt;&lt; pos &lt;&lt; <span class="string">&quot;,res=&quot;</span> &lt;&lt; res &lt;&lt; <span class="string">&quot;-&gt;[&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nbInputs + nbOutputs; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">getFormatString</span>(inOut[i].format) &lt;&lt; <span class="string">&quot;,&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;],[&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nbInputs + nbOutputs; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">dataTypeToString</span>(inOut[i].type) &lt;&lt; <span class="string">&quot;,&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;]&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    <span class="keyword">switch</span> (pos)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> inOut[<span class="number">0</span>].type == DataType::kFLOAT &amp;&amp; inOut[<span class="number">0</span>].format == TensorFormat::kLINEAR;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> inOut[<span class="number">1</span>].type == inOut[<span class="number">0</span>].type &amp;&amp; inOut[<span class="number">1</span>].format == inOut[<span class="number">0</span>].format;</span><br><span class="line">    <span class="keyword">default</span>: <span class="comment">// should NOT be here!</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-7-configurePlugin"><a href="#3-3-7-configurePlugin" class="headerlink" title="3.3.7 configurePlugin"></a>3.3.7 <strong>configurePlugin</strong></h3><ul>
<li>在推理前将调用该成员函数。在创建执行引擎时被调用。这个函数的目的是配置插件，为其提供有关网络、TensorRT 构建配置等信息。</li>
<li>Dynamic Shape 模式中，每当输入数据形状发生变化（调用 context.set_binding_shape）时，该成员函数被调用</li>
<li>构建期调用时 in/out 张量形状中含有 -1</li>
<li>运行期调用时 in/out 张量形状为真实绑定的形状</li>
</ul>
<p>配置这个插件op，判断输入和输出类型数量是否正确。官方还提到通过这个配置信息可以告知TensorRT去选择合适的算法(algorithm)去调优这个模型。</p>
<p>但自动调优目前还没有尝试过，我们一般自己写的plugin执行代码都是定死的，所谓的调优步骤可能更多地针对官方的op。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::configurePlugin</span><span class="params">(<span class="type">const</span> DynamicPluginTensorDesc *in, <span class="type">int32_t</span> nbInputs, <span class="type">const</span> DynamicPluginTensorDesc *out, <span class="type">int32_t</span> nbOutputs)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-8-getWorkspaceSize"><a href="#3-3-8-getWorkspaceSize" class="headerlink" title="3.3.8 getWorkspaceSize"></a>3.3.8 <strong>getWorkspaceSize</strong></h3><ul>
<li>向 TensorRT 报告中间计算结果的存储空间</li>
<li>由 TensorRT 管理，参与显存优化</li>
</ul>
<p>这个函数需要返回这个插件op需要中间显存变量的实际数据大小(bytesize)，这个是通过TensorRT的接口去获取，是比较规范的方式。</p>
<p>我们需要在这里确定这个op需要多大的显存空间去运行，在实际运行的时候就可以直接使用TensorRT开辟好的空间而不是自己去申请显存空间。</p>
<p>例如：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">size_t</span> <span class="title">MyCustomPlugin::getWorkspaceSize</span><span class="params">(<span class="type">const</span> nvinfer1::PluginTensorDesc* inputs, <span class="type">int</span> nbInputs, <span class="type">const</span> nvinfer1::PluginTensorDesc* outputs, <span class="type">int</span> nbOutputs)</span> <span class="type">const</span> </span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="comment">// 计算这个op前向过程中你认为需要的中间显存数量</span></span><br><span class="line">    <span class="type">size_t</span> need_num;</span><br><span class="line">    <span class="keyword">return</span> need_num * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>例子使用的是直接返回0，TODO 为什么？</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">size_t</span> <span class="title">AddScalarPlugin::getWorkspaceSize</span><span class="params">(<span class="type">const</span> PluginTensorDesc *inputs, <span class="type">int32_t</span> nbInputs, <span class="type">const</span> PluginTensorDesc *outputs, <span class="type">int32_t</span> nbOutputs)</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-9-enqueue"><a href="#3-3-9-enqueue" class="headerlink" title="3.3.9 enqueue"></a>3.3.9 <strong>enqueue</strong></h3><ul>
<li>调用 CUDA C++ kernel 计算的地方</li>
<li>可以根据输入张量的不同形状、数据类型等条件选择不同 kernel 执行计算</li>
<li>不要在 enqueue 中使用 cudaMalloc* 等函数</li>
</ul>
<p>实际插件op的执行函数，我们自己实现的cuda操作就放到这里(当然C++写的op也可以放进来，不过因为是CPU执行，速度就比较慢了)，与往常一样接受输入 <code>inputs</code>产生输出 <code>outputs</code>，传给相应的指针就可以。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int32_t</span> <span class="title">AddScalarPlugin::enqueue</span><span class="params">(<span class="type">const</span> PluginTensorDesc *inputDesc, <span class="type">const</span> PluginTensorDesc *outputDesc, <span class="type">const</span> <span class="type">void</span> *<span class="type">const</span> *inputs, <span class="type">void</span> *<span class="type">const</span> *outputs, <span class="type">void</span> *workspace, cudaStream_t stream)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="type">int</span> nElement = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; inputDesc[<span class="number">0</span>].dims.nbDims; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        nElement *= inputDesc[<span class="number">0</span>].dims.d[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(CEIL_DIVIDE(nElement, <span class="number">256</span>), <span class="number">1</span>, <span class="number">1</span>)</span>, <span class="title">block</span><span class="params">(<span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">    addScalarKernel&lt;&lt;&lt;grid, block, <span class="number">0</span>, stream&gt;&gt;&gt;(<span class="built_in">reinterpret_cast</span>&lt;<span class="type">const</span> <span class="type">float</span> *&gt;(inputs[<span class="number">0</span>]), <span class="built_in">reinterpret_cast</span>&lt;<span class="type">float</span> *&gt;(outputs[<span class="number">0</span>]), m_.scalar, nElement);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-10-destroy"><a href="#3-3-10-destroy" class="headerlink" title="3.3.10 destroy"></a>3.3.10 destroy</h3><p>当 context/engine 销毁时被调用</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::destroy</span><span class="params">()</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">delete</span> <span class="keyword">this</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-11-initialize"><a href="#3-3-11-initialize" class="headerlink" title="3.3.11 initialize"></a>3.3.11 <strong>initialize</strong></h3><p>初始化函数，在这个插件准备开始run之前执行。（engine 创建时被调用，用于初始化 Plugin 层）</p>
<p>主要初始化一些提前开辟空间的参数，一般是一些cuda操作需要的参数(例如conv操作需要执行卷积操作，我们就需要提前开辟weight和bias的显存)，假如我们的算子需要这些参数，则在这里需要提前开辟显存。</p>
<p>需要注意的是，如果插件算子需要开辟比较大的显存空间，<strong>不建议自己去申请显存空间，可以使用Tensorrt官方接口传过来的workspace指针来获取显存空间</strong>。因为如果这个插件被一个网络调用了很多次，而这个插件op需要开辟很多显存空间，那么TensorRT在构建network的时候会根据这个插件被调用的次数开辟很多显存，很容易导致显存溢出。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int32_t</span> <span class="title">AddScalarPlugin::initialize</span><span class="params">()</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-12-terminate"><a href="#3-3-12-terminate" class="headerlink" title="3.3.12 terminate"></a>3.3.12 terminate</h3><p>engine 销毁时被调用，用于释放 initialize 函数申请的资源</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::terminate</span><span class="params">()</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-13-getSerializationSize"><a href="#3-3-13-getSerializationSize" class="headerlink" title="3.3.13 getSerializationSize"></a>3.3.13 <strong>getSerializationSize</strong></h3><p>（报告序列化需要的空间大小，单位 Byte）</p>
<p>返回序列化时需要写多少字节到buffer中。 TODO m_ 是什么意思？</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">size_t</span> <span class="title">AddScalarPlugin::getSerializationSize</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sizeof</span>(m_);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-14-serialize"><a href="#3-3-14-serialize" class="headerlink" title="3.3.14 serialize"></a>3.3.14 serialize</h3><p>将Plugin 数据序列化到给定的 buffer 中</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::serialize</span><span class="params">(<span class="type">void</span> *buffer)</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="built_in">memcpy</span>(buffer, &amp;m_, <span class="built_in">sizeof</span>(m_));</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-3-15-attachToContext"><a href="#3-3-15-attachToContext" class="headerlink" title="3.3.15 attachToContext"></a>3.3.15 <strong>attachToContext</strong></h3><p>（申请使用 context 独占的 cudnn 或 cublas 资源）</p>
<p>创建执行上下文（execution context）时被调用。这个函数的目的是将插件附加（attach）到执行上下文，以便在推理时使用。</p>
<p>如果这个op使用到了一些其他东西，例如 <code>cublas handle</code>，可以直接借助TensorRT内部提供的 <code>cublas handle</code>:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::attachToContext</span><span class="params">(cudnnContext *contextCudnn, cublasContext *contextCublas, IGpuAllocator *gpuAllocator)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-4-简单的例子"><a href="#3-4-简单的例子" class="headerlink" title="3.4 简单的例子"></a>3.4 简单的例子</h2><p>就是cookbook/05-Plugin/usePluginV2DynamicExt/的代码示例</p>
<p>下图是运行过的文件结构</p>
<img src="/TensorRT/TensorRT-plugin/image-20220916110508870.png" class="" title="image-20220916110508870">
<h3 id="3-4-1-头文件"><a href="#3-4-1-头文件" class="headerlink" title="3.4.1 头文件"></a>3.4.1 头文件</h3><p>AddScalarPlugin.h</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Copyright (c) 2021-2022, NVIDIA CORPORATION. All rights reserved.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment"> * you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment"> * You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cookbookHelper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">char</span> *PLUGIN_NAME &#123;<span class="string">&quot;AddScalar&quot;</span>&#125;;</span><br><span class="line"><span class="type">static</span> <span class="type">const</span> <span class="type">char</span> *PLUGIN_VERSION &#123;<span class="string">&quot;1&quot;</span>&#125;;</span><br><span class="line">&#125; <span class="comment">// namespace</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> nvinfer1</span><br><span class="line">&#123;<span class="comment">// step1 继承IPluginV2DynamicExt实现一个Plugin类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AddScalarPlugin</span> : <span class="keyword">public</span> IPluginV2DynamicExt</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">const</span> std::string name_;</span><br><span class="line">    std::string       namespace_;</span><br><span class="line">    <span class="keyword">struct</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">float</span> scalar;</span><br><span class="line">    &#125; m_;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">AddScalarPlugin</span>() = <span class="keyword">delete</span>;</span><br><span class="line">    <span class="built_in">AddScalarPlugin</span>(<span class="type">const</span> std::string &amp;name, <span class="type">float</span> scalar);</span><br><span class="line">    <span class="built_in">AddScalarPlugin</span>(<span class="type">const</span> std::string &amp;name, <span class="type">const</span> <span class="type">void</span> *buffer, <span class="type">size_t</span> length);</span><br><span class="line">    ~<span class="built_in">AddScalarPlugin</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Method inherited from IPluginV2</span></span><br><span class="line">    <span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">getPluginType</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">getPluginVersion</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">int32_t</span>     <span class="title">getNbOutputs</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">int32_t</span>     <span class="title">initialize</span><span class="params">()</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>        <span class="title">terminate</span><span class="params">()</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">size_t</span>      <span class="title">getSerializationSize</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>        <span class="title">serialize</span><span class="params">(<span class="type">void</span> *buffer)</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>        <span class="title">destroy</span><span class="params">()</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>        <span class="title">setPluginNamespace</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *pluginNamespace)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">getPluginNamespace</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Method inherited from IPluginV2Ext</span></span><br><span class="line">    <span class="function">DataType <span class="title">getOutputDataType</span><span class="params">(<span class="type">int32_t</span> index, DataType <span class="type">const</span> *inputTypes, <span class="type">int32_t</span> nbInputs)</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>     <span class="title">attachToContext</span><span class="params">(cudnnContext *contextCudnn, cublasContext *contextCublas, IGpuAllocator *gpuAllocator)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>     <span class="title">detachFromContext</span><span class="params">()</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Method inherited from IPluginV2DynamicExt</span></span><br><span class="line">    <span class="function">IPluginV2DynamicExt *<span class="title">clone</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function">DimsExprs            <span class="title">getOutputDimensions</span><span class="params">(<span class="type">int32_t</span> outputIndex, <span class="type">const</span> DimsExprs *inputs, <span class="type">int32_t</span> nbInputs, IExprBuilder &amp;exprBuilder)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">bool</span>                 <span class="title">supportsFormatCombination</span><span class="params">(<span class="type">int32_t</span> pos, <span class="type">const</span> PluginTensorDesc *inOut, <span class="type">int32_t</span> nbInputs, <span class="type">int32_t</span> nbOutputs)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>                 <span class="title">configurePlugin</span><span class="params">(<span class="type">const</span> DynamicPluginTensorDesc *in, <span class="type">int32_t</span> nbInputs, <span class="type">const</span> DynamicPluginTensorDesc *out, <span class="type">int32_t</span> nbOutputs)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">size_t</span>               <span class="title">getWorkspaceSize</span><span class="params">(<span class="type">const</span> PluginTensorDesc *inputs, <span class="type">int32_t</span> nbInputs, <span class="type">const</span> PluginTensorDesc *outputs, <span class="type">int32_t</span> nbOutputs)</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">int32_t</span>              <span class="title">enqueue</span><span class="params">(<span class="type">const</span> PluginTensorDesc *inputDesc, <span class="type">const</span> PluginTensorDesc *outputDesc, <span class="type">const</span> <span class="type">void</span> *<span class="type">const</span> *inputs, <span class="type">void</span> *<span class="type">const</span> *outputs, <span class="type">void</span> *workspace, cudaStream_t stream)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">//step 2 继承IPluginCreator实现一个PluginCreator类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AddScalarPluginCreator</span> : <span class="keyword">public</span> IPluginCreator</span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="type">static</span> PluginFieldCollection    fc_;</span><br><span class="line">    <span class="type">static</span> std::vector&lt;PluginField&gt; attr_;</span><br><span class="line">    std::string                     namespace_;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">AddScalarPluginCreator</span>();</span><br><span class="line">    ~<span class="built_in">AddScalarPluginCreator</span>();</span><br><span class="line">    <span class="function"><span class="type">const</span> <span class="type">char</span> *                 <span class="title">getPluginName</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> <span class="type">char</span> *                 <span class="title">getPluginVersion</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> PluginFieldCollection *<span class="title">getFieldNames</span><span class="params">()</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function">IPluginV2 *                  <span class="title">createPlugin</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *name, <span class="type">const</span> PluginFieldCollection *fc)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function">IPluginV2 *                  <span class="title">deserializePlugin</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *name, <span class="type">const</span> <span class="type">void</span> *serialData, <span class="type">size_t</span> serialLength)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">void</span>                         <span class="title">setPluginNamespace</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *pluginNamespace)</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">    <span class="function"><span class="type">const</span> <span class="type">char</span> *                 <span class="title">getPluginNamespace</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span> <span class="keyword">override</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">&#125; <span class="comment">// namespace nvinfer1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="3-4-2-源文件"><a href="#3-4-2-源文件" class="headerlink" title="3.4.2 源文件"></a>3.4.2 源文件</h3><p> AddScalarPlugin.cu</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Copyright (c) 2021-2022, NVIDIA CORPORATION. All rights reserved.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment"> * you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment"> * You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;AddScalarPlugin.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//step 3 实现计算cuda的核函数</span></span><br><span class="line"><span class="comment">// 用于计算的 kernel</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">addScalarKernel</span><span class="params">(<span class="type">const</span> <span class="type">float</span> *input, <span class="type">float</span> *output, <span class="type">const</span> <span class="type">float</span> scalar, <span class="type">const</span> <span class="type">int</span> nElement)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> index = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span> (index &gt;= nElement)</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> _1      = input[index];</span><br><span class="line">    <span class="type">float</span> _2      = _1 + scalar;</span><br><span class="line">    output[index] = _2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> nvinfer1</span><br><span class="line">&#123;</span><br><span class="line"><span class="comment">// 这里各成员函数按照被调用顺序或重要程度顺序排列</span></span><br><span class="line"><span class="comment">// class AddScalarPlugin</span></span><br><span class="line">AddScalarPlugin::<span class="built_in">AddScalarPlugin</span>(<span class="type">const</span> std::string &amp;name, <span class="type">float</span> scalar):</span><br><span class="line">    <span class="built_in">name_</span>(name)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    m_.scalar = scalar;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">AddScalarPlugin::<span class="built_in">AddScalarPlugin</span>(<span class="type">const</span> std::string &amp;name, <span class="type">const</span> <span class="type">void</span> *buffer, <span class="type">size_t</span> length):</span><br><span class="line">    <span class="built_in">name_</span>(name)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="built_in">memcpy</span>(&amp;m_, buffer, <span class="built_in">sizeof</span>(m_));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">AddScalarPlugin::~<span class="built_in">AddScalarPlugin</span>()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">IPluginV2DynamicExt *<span class="title">AddScalarPlugin::clone</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">auto</span> p = <span class="keyword">new</span> <span class="built_in">AddScalarPlugin</span>(name_, &amp;m_, <span class="built_in">sizeof</span>(m_));</span><br><span class="line">    p-&gt;<span class="built_in">setPluginNamespace</span>(namespace_.<span class="built_in">c_str</span>());</span><br><span class="line">    <span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int32_t</span> <span class="title">AddScalarPlugin::getNbOutputs</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">DataType <span class="title">AddScalarPlugin::getOutputDataType</span><span class="params">(<span class="type">int32_t</span> index, DataType <span class="type">const</span> *inputTypes, <span class="type">int32_t</span> nbInputs)</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> inputTypes[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">DimsExprs <span class="title">AddScalarPlugin::getOutputDimensions</span><span class="params">(<span class="type">int32_t</span> outputIndex, <span class="type">const</span> DimsExprs *inputs, <span class="type">int32_t</span> nbInputs, IExprBuilder &amp;exprBuilder)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> inputs[<span class="number">0</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">AddScalarPlugin::supportsFormatCombination</span><span class="params">(<span class="type">int32_t</span> pos, <span class="type">const</span> PluginTensorDesc *inOut, <span class="type">int32_t</span> nbInputs, <span class="type">int32_t</span> nbOutputs)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> DEBUG</span></span><br><span class="line">    <span class="type">bool</span> res;</span><br><span class="line">    <span class="keyword">switch</span> (pos)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">        res = inOut[<span class="number">0</span>].type == DataType::kFLOAT &amp;&amp; inOut[<span class="number">0</span>].format == TensorFormat::kLINEAR;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">        res = inOut[<span class="number">1</span>].format == inOut[<span class="number">0</span>].format &amp;&amp; inOut[<span class="number">1</span>].type == inOut[<span class="number">0</span>].type;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">default</span>: <span class="comment">// should NOT be here!</span></span><br><span class="line">        res = <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;\tpos=&quot;</span> &lt;&lt; pos &lt;&lt; <span class="string">&quot;,res=&quot;</span> &lt;&lt; res &lt;&lt; <span class="string">&quot;-&gt;[&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nbInputs + nbOutputs; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">getFormatString</span>(inOut[i].format) &lt;&lt; <span class="string">&quot;,&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;],[&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nbInputs + nbOutputs; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">dataTypeToString</span>(inOut[i].type) &lt;&lt; <span class="string">&quot;,&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;]&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    <span class="keyword">switch</span> (pos)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> inOut[<span class="number">0</span>].type == DataType::kFLOAT &amp;&amp; inOut[<span class="number">0</span>].format == TensorFormat::kLINEAR;</span><br><span class="line">    <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> inOut[<span class="number">1</span>].type == inOut[<span class="number">0</span>].type &amp;&amp; inOut[<span class="number">1</span>].format == inOut[<span class="number">0</span>].format;</span><br><span class="line">    <span class="keyword">default</span>: <span class="comment">// should NOT be here!</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::configurePlugin</span><span class="params">(<span class="type">const</span> DynamicPluginTensorDesc *in, <span class="type">int32_t</span> nbInputs, <span class="type">const</span> DynamicPluginTensorDesc *out, <span class="type">int32_t</span> nbOutputs)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">size_t</span> <span class="title">AddScalarPlugin::getWorkspaceSize</span><span class="params">(<span class="type">const</span> PluginTensorDesc *inputs, <span class="type">int32_t</span> nbInputs, <span class="type">const</span> PluginTensorDesc *outputs, <span class="type">int32_t</span> nbOutputs)</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int32_t</span> <span class="title">AddScalarPlugin::enqueue</span><span class="params">(<span class="type">const</span> PluginTensorDesc *inputDesc, <span class="type">const</span> PluginTensorDesc *outputDesc, <span class="type">const</span> <span class="type">void</span> *<span class="type">const</span> *inputs, <span class="type">void</span> *<span class="type">const</span> *outputs, <span class="type">void</span> *workspace, cudaStream_t stream)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="type">int</span> nElement = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; inputDesc[<span class="number">0</span>].dims.nbDims; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        nElement *= inputDesc[<span class="number">0</span>].dims.d[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(CEIL_DIVIDE(nElement, <span class="number">256</span>), <span class="number">1</span>, <span class="number">1</span>)</span>, <span class="title">block</span><span class="params">(<span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">    addScalarKernel&lt;&lt;&lt;grid, block, <span class="number">0</span>, stream&gt;&gt;&gt;(<span class="built_in">reinterpret_cast</span>&lt;<span class="type">const</span> <span class="type">float</span> *&gt;(inputs[<span class="number">0</span>]), <span class="built_in">reinterpret_cast</span>&lt;<span class="type">float</span> *&gt;(outputs[<span class="number">0</span>]), m_.scalar, nElement);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::destroy</span><span class="params">()</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">delete</span> <span class="keyword">this</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int32_t</span> <span class="title">AddScalarPlugin::initialize</span><span class="params">()</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::terminate</span><span class="params">()</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">size_t</span> <span class="title">AddScalarPlugin::getSerializationSize</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sizeof</span>(m_);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::serialize</span><span class="params">(<span class="type">void</span> *buffer)</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="built_in">memcpy</span>(buffer, &amp;m_, <span class="built_in">sizeof</span>(m_));</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::setPluginNamespace</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *pluginNamespace)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    namespace_ = pluginNamespace;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">AddScalarPlugin::getPluginNamespace</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> namespace_.<span class="built_in">c_str</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">AddScalarPlugin::getPluginType</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> PLUGIN_NAME;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">AddScalarPlugin::getPluginVersion</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> PLUGIN_VERSION;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::attachToContext</span><span class="params">(cudnnContext *contextCudnn, cublasContext *contextCublas, IGpuAllocator *gpuAllocator)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPlugin::detachFromContext</span><span class="params">()</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// class AddScalarPluginCreator</span></span><br><span class="line">PluginFieldCollection    AddScalarPluginCreator::fc_ &#123;&#125;;</span><br><span class="line">std::vector&lt;PluginField&gt; AddScalarPluginCreator::attr_;</span><br><span class="line"></span><br><span class="line">AddScalarPluginCreator::<span class="built_in">AddScalarPluginCreator</span>()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    attr_.<span class="built_in">clear</span>();</span><br><span class="line">    attr_.<span class="built_in">emplace_back</span>(<span class="built_in">PluginField</span>(<span class="string">&quot;scalar&quot;</span>, <span class="literal">nullptr</span>, PluginFieldType::kFLOAT32, <span class="number">1</span>));</span><br><span class="line">    fc_.nbFields = attr_.<span class="built_in">size</span>();</span><br><span class="line">    fc_.fields   = attr_.<span class="built_in">data</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">AddScalarPluginCreator::~<span class="built_in">AddScalarPluginCreator</span>()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 最重要的两个成员函数，分别用于“接受参数创建 Plugin” 和 “去序列化创建 Plugin”</span></span><br><span class="line"><span class="function">IPluginV2 *<span class="title">AddScalarPluginCreator::createPlugin</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *name, <span class="type">const</span> PluginFieldCollection *fc)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="type">float</span>                          scalar = <span class="number">0</span>;</span><br><span class="line">    std::map&lt;std::string, <span class="type">float</span> *&gt; parameterMap &#123;&#123;<span class="string">&quot;scalar&quot;</span>, &amp;scalar&#125;&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; fc-&gt;nbFields; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (parameterMap.<span class="built_in">find</span>(fc-&gt;fields[i].name) != parameterMap.<span class="built_in">end</span>())</span><br><span class="line">        &#123;</span><br><span class="line">            *parameterMap[fc-&gt;fields[i].name] = *<span class="built_in">reinterpret_cast</span>&lt;<span class="type">const</span> <span class="type">float</span> *&gt;(fc-&gt;fields[i].data);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">//new了plugin </span></span><br><span class="line">    AddScalarPlugin *pObj = <span class="keyword">new</span> <span class="built_in">AddScalarPlugin</span>(name, scalar);</span><br><span class="line">    pObj-&gt;<span class="built_in">setPluginNamespace</span>(namespace_.<span class="built_in">c_str</span>());</span><br><span class="line">    <span class="keyword">return</span> pObj;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//第二个重要的函数 把序列化的 buffer 传给 Plugin 的构造函数</span></span><br><span class="line"><span class="function">IPluginV2 *<span class="title">AddScalarPluginCreator::deserializePlugin</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *name, <span class="type">const</span> <span class="type">void</span> *serialData, <span class="type">size_t</span> serialLength)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">	<span class="comment">//new了plugin 和上面的不一样，是两个构造函数 从 buffer 中读取数据并完成 Plugin 构造</span></span><br><span class="line">    AddScalarPlugin *pObj = <span class="keyword">new</span> <span class="built_in">AddScalarPlugin</span>(name, serialData, serialLength);</span><br><span class="line">    pObj-&gt;<span class="built_in">setPluginNamespace</span>(namespace_.<span class="built_in">c_str</span>());</span><br><span class="line">    <span class="keyword">return</span> pObj;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">AddScalarPluginCreator::setPluginNamespace</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *pluginNamespace)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    namespace_ = pluginNamespace;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">AddScalarPluginCreator::getPluginNamespace</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> namespace_.<span class="built_in">c_str</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">AddScalarPluginCreator::getPluginName</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> PLUGIN_NAME;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span> *<span class="title">AddScalarPluginCreator::getPluginVersion</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> PLUGIN_VERSION;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> PluginFieldCollection *<span class="title">AddScalarPluginCreator::getFieldNames</span><span class="params">()</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">WHERE_AM_I</span>();</span><br><span class="line">    <span class="keyword">return</span> &amp;fc_;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">REGISTER_TENSORRT_PLUGIN</span>(AddScalarPluginCreator);<span class="comment">//静态注册</span></span><br><span class="line"></span><br><span class="line">&#125; <span class="comment">// namespace nvinfer1</span></span><br></pre></td></tr></table></figure>
<h3 id="3-4-3-测试运行"><a href="#3-4-3-测试运行" class="headerlink" title="3.4.3 测试运行"></a>3.4.3 测试运行</h3><p>上面完成了Plugin的代码编写，makefile中将上面的plugin编译为.so文件，方便后面构建网络时添加使用。</p>
<p>engine的构建使用python代码编写了一个简单的网络，就一个输入层和一个插件层。</p>
<p>engine的构建流程参考 <code>TensorRT例子</code>教程。</p>
<p>需要了解的就是里面调用了 <code>create_plugin</code>也就是插件中PluginCreator的createPlugin</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Copyright (c) 2021-2022, NVIDIA CORPORATION. All rights reserved.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment"># you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment"># You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> ctypes</span><br><span class="line"><span class="keyword">from</span> cuda <span class="keyword">import</span> cudart</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br><span class="line"></span><br><span class="line">soFile = <span class="string">&quot;./AddScalarPlugin.so&quot;</span><span class="comment">#python通过加载这个so来实现</span></span><br><span class="line">np.random.seed(<span class="number">97</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">printArrayInfomation</span>(<span class="params">x, info=<span class="string">&quot;&quot;</span>, n=<span class="number">5</span></span>):</span><br><span class="line">    <span class="built_in">print</span>( <span class="string">&#x27;%s:%s,SumAbs=%.5e,Var=%.5f,Max=%.5f,Min=%.5f,SAD=%.5f&#x27;</span>%( \</span><br><span class="line">        info,<span class="built_in">str</span>(x.shape),np.<span class="built_in">sum</span>(<span class="built_in">abs</span>(x)),np.var(x),np.<span class="built_in">max</span>(x),np.<span class="built_in">min</span>(x),np.<span class="built_in">sum</span>(np.<span class="built_in">abs</span>(np.diff(x.reshape(-<span class="number">1</span>)))) ))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>, x.reshape(-<span class="number">1</span>)[:n], x.reshape(-<span class="number">1</span>)[-n:])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check</span>(<span class="params">a, b, weak=<span class="literal">False</span>, checkEpsilon=<span class="number">1e-5</span></span>):</span><br><span class="line">    <span class="keyword">if</span> weak:</span><br><span class="line">        res = np.<span class="built_in">all</span>(np.<span class="built_in">abs</span>(a - b) &lt; checkEpsilon)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        res = np.<span class="built_in">all</span>(a == b)</span><br><span class="line">    diff0 = np.<span class="built_in">max</span>(np.<span class="built_in">abs</span>(a - b))</span><br><span class="line">    diff1 = np.<span class="built_in">max</span>(np.<span class="built_in">abs</span>(a - b) / (np.<span class="built_in">abs</span>(b) + checkEpsilon))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;check:%s, absDiff=%f, relDiff=%f&quot;</span> % (res, diff0, diff1))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">addScalarCPU</span>(<span class="params">inputH, scalar</span>):</span><br><span class="line">    <span class="keyword">return</span> [inputH[<span class="number">0</span>] + scalar]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getAddScalarPlugin</span>(<span class="params">scalar</span>):</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> trt.get_plugin_registry().plugin_creator_list:</span><br><span class="line">        <span class="comment">#print(c.name)</span></span><br><span class="line">        <span class="keyword">if</span> c.name == <span class="string">&quot;AddScalar&quot;</span>:</span><br><span class="line">            parameterList = []</span><br><span class="line">            parameterList.append(trt.PluginField(<span class="string">&quot;scalar&quot;</span>, np.float32(scalar), trt.PluginFieldType.FLOAT32))</span><br><span class="line">            <span class="keyword">return</span> c.create_plugin(c.name, trt.PluginFieldCollection(parameterList))<span class="comment">#这里调用了Create的函数，也就是plugin代码中的create_plugin</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">shape, scalar</span>):</span><br><span class="line">    testCase = <span class="string">&quot;&lt;shape=%s,scalar=%f&gt;&quot;</span> % (shape, scalar)</span><br><span class="line">    trtFile = <span class="string">&quot;./model-Dim%s.plan&quot;</span> % <span class="built_in">str</span>(<span class="built_in">len</span>(shape))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test %s&quot;</span> % testCase)</span><br><span class="line">    logger = trt.Logger(trt.Logger.ERROR)</span><br><span class="line">    trt.init_libnvinfer_plugins(logger, <span class="string">&#x27;&#x27;</span>)<span class="comment">#类似与c++的接口initLibNvInferPlugins注册plugin</span></span><br><span class="line">    ctypes.cdll.LoadLibrary(soFile)</span><br><span class="line">    <span class="keyword">if</span> os.path.isfile(trtFile):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(trtFile, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            engine = trt.Runtime(logger).deserialize_cuda_engine(f.read())</span><br><span class="line">        <span class="keyword">if</span> engine == <span class="literal">None</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Failed loading engine!&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Succeeded loading engine!&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        builder = trt.Builder(logger)</span><br><span class="line">        network = builder.create_network(<span class="number">1</span> &lt;&lt; <span class="built_in">int</span>(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))</span><br><span class="line">        profile = builder.create_optimization_profile()</span><br><span class="line">        config = builder.create_builder_config()</span><br><span class="line">        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, <span class="number">6</span> &lt;&lt; <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">        inputT0 = network.add_input(<span class="string">&quot;inputT0&quot;</span>, trt.float32, [-<span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> shape])</span><br><span class="line">        profile.set_shape(inputT0.name, [<span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> shape], [<span class="number">8</span> <span class="keyword">for</span> i <span class="keyword">in</span> shape], [<span class="number">32</span> <span class="keyword">for</span> i <span class="keyword">in</span> shape])</span><br><span class="line">        config.add_optimization_profile(profile)</span><br><span class="line"></span><br><span class="line">        pluginLayer = network.add_plugin_v2([inputT0], getAddScalarPlugin(scalar))<span class="comment">#从这里加载自定义的插件</span></span><br><span class="line">        network.mark_output(pluginLayer.get_output(<span class="number">0</span>))</span><br><span class="line">        engineString = builder.build_serialized_network(network, config)</span><br><span class="line">        <span class="keyword">if</span> engineString == <span class="literal">None</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Failed building engine!&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Succeeded building engine!&quot;</span>)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(trtFile, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(engineString)</span><br><span class="line">        engine = trt.Runtime(logger).deserialize_cuda_engine(engineString)</span><br><span class="line"></span><br><span class="line">    context = engine.create_execution_context()</span><br><span class="line">    context.set_binding_shape(<span class="number">0</span>, shape)</span><br><span class="line">    <span class="comment">#print(&quot;Binding all? %s&quot;%([&quot;No&quot;,&quot;Yes&quot;][int(context.all_binding_shapes_specified)]))</span></span><br><span class="line">    nInput = np.<span class="built_in">sum</span>([engine.binding_is_input(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(engine.num_bindings)])</span><br><span class="line">    nOutput = engine.num_bindings - nInput</span><br><span class="line">    <span class="comment">#for i in range(nInput):</span></span><br><span class="line">    <span class="comment">#    print(&quot;Bind[%2d]:i[%2d]-&gt;&quot; % (i, i), engine.get_binding_dtype(i), engine.get_binding_shape(i), context.get_binding_shape(i), engine.get_binding_name(i))</span></span><br><span class="line">    <span class="comment">#for i in range(nInput, nInput + nOutput):</span></span><br><span class="line">    <span class="comment">#    print(&quot;Bind[%2d]:o[%2d]-&gt;&quot; % (i, i - nInput), engine.get_binding_dtype(i), engine.get_binding_shape(i), context.get_binding_shape(i), engine.get_binding_name(i))</span></span><br><span class="line"></span><br><span class="line">    bufferH = []</span><br><span class="line">    bufferH.append(np.arange(np.prod(shape), dtype=np.float32).reshape(shape))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nOutput):</span><br><span class="line">        bufferH.append(np.empty(context.get_binding_shape(nInput + i), dtype=trt.nptype(engine.get_binding_dtype(nInput + i))))</span><br><span class="line">    bufferD = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(engine.num_bindings):</span><br><span class="line">        bufferD.append(cudart.cudaMalloc(bufferH[i].nbytes)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput):</span><br><span class="line">        cudart.cudaMemcpy(bufferD[i], np.ascontiguousarray(bufferH[i].reshape(-<span class="number">1</span>)).ctypes.data, bufferH[i].nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice)</span><br><span class="line"></span><br><span class="line">    context.execute_v2(bufferD)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nOutput):</span><br><span class="line">        cudart.cudaMemcpy(bufferH[nInput + i].ctypes.data, bufferD[nInput + i], bufferH[nInput + i].nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost)</span><br><span class="line"></span><br><span class="line">    outputCPU = addScalarCPU(bufferH[:nInput], scalar)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    for i in range(nInput):</span></span><br><span class="line"><span class="string">        printArrayInfomation(bufferH[i])</span></span><br><span class="line"><span class="string">    for i in range(nOutput):</span></span><br><span class="line"><span class="string">        printArrayInfomation(bufferH[nInput+i])</span></span><br><span class="line"><span class="string">    for i in range(nOutput):</span></span><br><span class="line"><span class="string">        printArrayInfomation(outputCPU[i])</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    check(bufferH[nInput:][<span class="number">0</span>], outputCPU[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> buffer <span class="keyword">in</span> bufferD:</span><br><span class="line">        cudart.cudaFree(buffer)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test %s finish!\n&quot;</span> % testCase)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    os.system(<span class="string">&quot;rm -rf ./*.plan&quot;</span>)</span><br><span class="line">    np.set_printoptions(precision=<span class="number">3</span>, linewidth=<span class="number">100</span>, suppress=<span class="literal">True</span>)</span><br><span class="line">    run([<span class="number">32</span>], <span class="number">1</span>)</span><br><span class="line">    run([<span class="number">32</span>, <span class="number">32</span>], <span class="number">1</span>)</span><br><span class="line">    run([<span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>], <span class="number">1</span>)</span><br><span class="line">    run([<span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Test all finish!&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT-plugin/image-20220916110619782.png" class="" title="image-20220916110619782">
<h1 id="4-官方例子"><a href="#4-官方例子" class="headerlink" title="4 官方例子"></a>4 官方例子</h1><h2 id="4-1-sampleFasterRCNN"><a href="#4-1-sampleFasterRCNN" class="headerlink" title="4.1 sampleFasterRCNN"></a>4.1 sampleFasterRCNN</h2><p>这个示例sampleFasterRCNN使用TensorRT插件（TensorRT plugins），执行推理，并实现一个<strong>融合自定义层</strong>，用于Faster R-CNN模型的端到端推理。构建了进一步优化的基础，例如使用INT8校准、用户训练网络等。</p>
<p>Fast R-CNN比它的前辈(RCNN, Fast R-CNN)更快更准确，因为它允许端到端推理，不需要独立的区域提议算法(如Fast R-CNN中的选择性搜索)或分类方法(如RCNN中的支持向量机)。</p>
<p>sampleFasterRCNN示例使用了来自TensorRT插件库的插件，包含了<strong>Faster  R-CNN的区域建议网络(RPN)和ROIPooling层的融合实现</strong>。这些特殊的层来自Faster  R-CNN的论文，<strong>它们作为一个名为RPNROIPlugin的插件一起实现。该插件在TensorRT插件注册表中注册，名称为RPROI_TRT。</strong>如下图</p>
<p>VGG16_faster_rcnn_final.caffemodel模型文件结构如下</p>
<img src="/TensorRT/TensorRT-plugin/image-20220919133000899.png" class="" title="image-20220919133000899">
<p>加载的faster_rcnn_test_iplugin.prototxt文件如下</p>
<img src="/TensorRT/TensorRT-plugin/image-20220919133120820.png" class="" title="image-20220919133120820">
<p>步骤如下：</p>
<ol>
<li>输入预处理</li>
<li>定义网络（插件的动态加载）</li>
<li>编译engine</li>
<li>运行engine</li>
<li>验证输出</li>
</ol>
<h3 id="4-1-1-输入预处理"><a href="#4-1-1-输入预处理" class="headerlink" title="4.1.1 输入预处理"></a>4.1.1 输入预处理</h3><p>更快的R-CNN采用3通道375x500图像作为输入。由于TensorRT不依赖于任何计算机视觉库，图像以每个像素的二进制R、G和B值表示。格式是<strong>像素映射位图PPM</strong>)，这是一种netpbm彩色图像格式。在这种格式中，每个像素的<strong>R、G和B</strong>值通常用整数字节(0-255)表示，它们一个像素一个像素地存储在一起。</p>
<p>然而，Faster R- cnn的作者已经训练了网络，使第一卷积层以<strong>B、G和R</strong>顺序看到图像数据。因此，当将PPM映像放入网络输入缓冲区时，您需要<strong>反转顺序</strong>。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span>* data = <span class="keyword">new</span> <span class="type">float</span>[N*INPUT_C*INPUT_H*INPUT_W];</span><br><span class="line"><span class="comment">// pixel mean used by the Faster R-CNN&#x27;s author</span></span><br><span class="line"><span class="type">float</span> pixelMean[<span class="number">3</span>]&#123; <span class="number">102.9801f</span>, <span class="number">115.9465f</span>, <span class="number">122.7717f</span> &#125;; <span class="comment">// also in BGR order</span></span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>, volImg = INPUT_C*INPUT_H*INPUT_W; i &lt; N; ++i)</span><br><span class="line">&#123;</span><br><span class="line">	<span class="keyword">for</span> (<span class="type">int</span> c = <span class="number">0</span>; c &lt; INPUT_C; ++c)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="comment">// the color image to input should be in BGR order</span></span><br><span class="line">		<span class="keyword">for</span> (<span class="type">unsigned</span> j = <span class="number">0</span>, volChl = INPUT_H*INPUT_W; j &lt; volChl; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            data[i*volImg + c*volChl + j] =  <span class="built_in">float</span>(ppms[i].buffer[j*INPUT_C + <span class="number">2</span> - c]) - pixelMean[c];</span><br><span class="line">        &#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>有一个简单的PPM读取函数称为readPPMFile。</p>
<p>此外，在示例中还有一个名为writePPMFileWithBBox的函数，该函数用一像素宽的红线在图像中绘制给定的边界框。</p>
<p>为了获得PPM图像，您可以很容易地使用命令行工具，例如ImageMagick来执行从JPEG图像的调整大小和转换。</p>
<h3 id="4-1-2-定义网络"><a href="#4-1-2-定义网络" class="headerlink" title="4.1.2 定义网络"></a>4.1.2 定义网络</h3><p>该网络是在原始文件prototxt中定义的，该文件位于data/faster-rcnn目录中。prototxt文件与Faster R-CNN使用的文件非常相似，只是将RPN和ROI池层融合并用名为RPROIFused的自定义层代替。</p>
<p><strong>这个示例使用插件注册将插件添加到网络中。Caffe解析器根据Caffe原文本文件中指定的层名将插件对象添加到网络中，例如RPROI</strong>。</p>
<img src="/TensorRT/TensorRT-plugin/image-20220927114645642.png" class="" title="image-20220927114645642">
<h3 id="4-1-3-编译engine"><a href="#4-1-3-编译engine" class="headerlink" title="4.1.3 编译engine"></a>4.1.3 编译engine</h3><p>参考之前的内容</p>
<h3 id="4-1-4-运行engine"><a href="#4-1-4-运行engine" class="headerlink" title="4.1.4 运行engine"></a>4.1.4 运行engine</h3><p>在sampleFasterRCNN.cpp中，推理函数有两个输入</p>
<ul>
<li>data是图像输入，</li>
<li>imInfo是图像信息数组，用于存储批处理的每张图像的行数、列数和缩放。</li>
</ul>
<p>4个输出</p>
<ul>
<li>bbox pred是高度、宽度和中心坐标的预测偏移量。</li>
<li>cls_prob问题是与每个边界框的每个对象类相关的概率。</li>
<li>rois是每个边界框的高度、宽度和中心坐标。</li>
<li>count不再使用了，不考虑（如果输出没有与nmsMaxOut对齐，count输出用于指定产生的NMS边界框的数量）</li>
</ul>
<h3 id="4-1-5-验证输出"><a href="#4-1-5-验证输出" class="headerlink" title="4.1.5 验证输出"></a>4.1.5 验证输出</h3><p>Faster R-CNN网络的输出需要进行后处理，以便获得人类可解释的结果。</p>
<ul>
<li>首先，因为边界框现在由对中心、高度和宽度的偏移量表示，<strong>所以需要通过除以imInfo(图像信息)中定义的比例，将它们缩小到原始图像空间</strong>。</li>
<li>确保对边界框应用逆变换，并剪辑得到的坐标，使它们不超出图像边界。</li>
<li>最后，采用非最大抑制算法去除重叠预测。后处理代码是在CPU内定义的，因为它们既不是计算密集型的，也不是内存密集型的。</li>
<li>在完成上述所有工作之后，根据类号、置信度评分(概率)和四个坐标，可以使用边界框。它们使用writePPMFileWithBBox函数在输出PPM图像中绘制。</li>
</ul>
<h3 id="4-1-6-int8精度预测"><a href="#4-1-6-int8精度预测" class="headerlink" title="4.1.6 int8精度预测"></a>4.1.6 int8精度预测</h3><p>RPNROIPlugin有四个输入(bbox confidence、bbox offset、feature map和image info)和两个输出(feature map和rois（Region of Interest感兴趣区域）)。此插件feature map 支持FP32/INT8 I/O，其他都是fp32的输入输出。int8精度需要Tensor的动态文件在目录data/faster-rcnn下。该文件中的每一行都包含一个张量名称(与层名称相同)和一个动态范围值。动态范围值表示张量边界值的abs。</p>
<h3 id="4-1-7-Tensorrt-API层和OPS"><a href="#4-1-7-Tensorrt-API层和OPS" class="headerlink" title="4.1.7 Tensorrt API层和OPS"></a>4.1.7 Tensorrt API层和OPS</h3><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#activation-layer">Activation layer</a> 激活层 实现基于元素的激活功能。kRELU的激活层。</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#convolution-layer">Convolution layer</a> 卷积层计算二维(通道、高度和宽度)卷积，有或没有偏置。</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#fullyconnected-layer">FullyConnected layer</a> 全连通层实现矩阵-向量乘积，有或没有偏差。</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#plugin-layer">Plugin (RPROI) layer</a> 插件层是用户定义的</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#pooling-layer">Pooling layer</a> 池化层池化层实现通道内的池化。支持的池类型是最大、平均和最大-平均混合。</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#shuffle-layer">Shuffle layer</a> Shuffle层实现了张量的重塑和转置算子。</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#identity-layer">Identity Layer</a> 标识层实现标识操作。</p>
<h3 id="4-1-8-数据准备"><a href="#4-1-8-数据准备" class="headerlink" title="4.1.8 数据准备"></a>4.1.8 数据准备</h3><ol>
<li>设置$TRT_DATADIR为数据存放的目录</li>
<li><p>下载<a target="_blank" rel="noopener" href="https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz">faster_rcnn_models.tgz</a>数据集</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export TRT_DATADIR=/usr/src/tensorrt/data</span><br><span class="line">mkdir -p $TRT_DATADIR/faster-rcnn</span><br><span class="line">wget --no-check-certificate https://dl.dropboxusercontent.com/s/o6ii098bu51d139/faster_rcnn_models.tgz?dl=0 -O $TRT_DATADIR/faster-rcnn/faster-rcnn.tgz</span><br></pre></td></tr></table></figure></li>
<li><p>解压数据到data/faster-rcnn目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf $TRT_DATADIR/faster-rcnn/faster-rcnn.tgz -C $TRT_DATADIR/faster-rcnn --strip-components=1 --exclude=ZF_*</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>这里看到对应的文件已经有了。不需要再下载了。(还是需要下载，否则报错没有model文件)</p>
<p>没有下载时文件如下</p>
<img src="/TensorRT/TensorRT-plugin/image-20220919103455265.png" class="" title="image-20220919103455265">
<p>下载并解压后多了模型文件</p>
<img src="/TensorRT/TensorRT-plugin/image-20220919114720622.png" class="" title="image-20220919114720622">
<h3 id="4-1-9-运行示例"><a href="#4-1-9-运行示例" class="headerlink" title="4.1.9 运行示例"></a>4.1.9 运行示例</h3><ol>
<li><p>编译</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> sampleFasterRCNN/</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">make -j4</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> ../../bin</span></span><br></pre></td></tr></table></figure></li>
<li><p>运行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sample_fasterRCNN --datadir=/usr/src/tensorrt/data/faster-rcnn</span><br></pre></td></tr></table></figure></li>
<li><p>验证示例是否成功运行。如果示例成功运行，您应该会看到类似以下的输出</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Sample output</span><br><span class="line">[I] Detected car in 000456.ppm with confidence 99.0063%  (Result stored in car-0.990063.ppm).</span><br><span class="line">[I] Detected person in 000456.ppm with confidence 97.4725%  (Result stored in person-0.974725.ppm).</span><br><span class="line">[I] Detected cat in 000542.ppm with confidence 99.1191%  (Result stored in cat-0.991191.ppm).</span><br><span class="line">[I] Detected dog in 001150.ppm with confidence 99.9603%  (Result stored in dog-0.999603.ppm).</span><br><span class="line">[I] Detected dog in 001763.ppm with confidence 99.7705%  (Result stored in dog-0.997705.ppm).</span><br><span class="line">[I] Detected horse in 004545.ppm with confidence 99.467%  (Result stored in horse-0.994670.ppm).</span><br><span class="line">&amp;&amp;&amp;&amp; PASSED TensorRT.sample_fasterRCNN # ./build/x86_64-linux/sample_fasterRCNN</span><br></pre></td></tr></table></figure>
<p>此输出显示示例成功运行PASSED</p>
<img src="/TensorRT/TensorRT-plugin/image-20220919114755156.png" class="" title="image-20220919114755156">
<p>输出的照片在bin目录下</p>
<img src="/TensorRT/TensorRT-plugin/image-20220919114927636.png" class="" title="image-20220919114927636">
<img src="/TensorRT/TensorRT-plugin/image-20220919114942792.png" class="" title="image-20220919114942792">
</li>
</ol>
<h3 id="4-1-10-理解的几个点："><a href="#4-1-10-理解的几个点：" class="headerlink" title="4.1.10 理解的几个点："></a>4.1.10 理解的几个点：</h3><ol>
<li>模型的结构由faster_rcnn_test_iplugin.prototxt定义，不是由VGG16_faster_rcnn_final.caffemodel文件定义的。只是使用了VGG16_faster_rcnn_final.caffemodel中的训练过的参数。因为VGG16_faster_rcnn_final.caffemodel文件中有RPN和ROI层，这两层在faster_rcnn_test_iplugin.prototxt中定义为一个自定义层RPROI_TRT。</li>
<li>虽然有自定义的层，但是在samples/sampleFasterRCNN/sampleFasterRCNN.cpp中engine的编译方式和sampleMNIST相比什么变化。应该是调用parse时直接就解析了，包括解析了自定义层。<strong>因为使用的时parser解析器来解析的，所以不需要调用addPluginV2函数（addPluginV2函数应该是自定义网络结构使用plugin时使用的）</strong></li>
<li>应该是编译成了动态库，在下面网络使用的时候调用了，库文件在/usr/lib/x86_64-linux-gnu/libnvinfer_plugin_static.a</li>
<li><p>为什么自定义层也能被解析？就是因为在文件中sampleFasterRCNN.cpp中在build和infer之前调用了注册。在faster_rcnn_test_iplugin.prototxt网路结构描述文件中有一层的名称（名称对应不上，应该是因为2.2节的原因才关联起来的）是这样也就使自定义插件和模型关联起来了。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">initLibNvInferPlugins</span>(&amp;sample::gLogger, <span class="string">&quot;&quot;</span>);<span class="comment">//动态注册plugin Creator</span></span><br></pre></td></tr></table></figure>
<p>这个函数实现了plugin的动态注册，在plugin/InferPlugin.cpp中实现</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> CreatorType&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">initializePlugin</span><span class="params">(<span class="type">void</span>* logger, <span class="type">const</span> <span class="type">char</span>* libNamespace)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    PluginCreatorRegistry::<span class="built_in">getInstance</span>().<span class="built_in">addPluginCreator</span>&lt;CreatorType&gt;(logger, libNamespace);<span class="comment">//这里面注册</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125; <span class="comment">// namespace plugin</span></span><br><span class="line">&#125; <span class="comment">// namespace nvinfer1</span></span><br><span class="line"><span class="comment">// New Plugin APIs</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">initLibNvInferPlugins</span><span class="params">(<span class="type">void</span>* logger, <span class="type">const</span> <span class="type">char</span>* libNamespace)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::BatchTilePluginCreator&gt;(logger, libNamespace);</span><br><span class="line">		.....</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::RPROIPluginCreator&gt;(logger, libNamespace);<span class="comment">//这里就是我们的插件 RPROIPluginCreator 就是我们自定义的creator</span></span><br><span class="line">		.....</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="comment">// extern &quot;C&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> CreatorType&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">addPluginCreator</span><span class="params">(<span class="type">void</span>* logger, <span class="type">const</span> <span class="type">char</span>* libNamespace)</span><span class="comment">//注册</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Make accesses to the plugin creator registry thread safe</span></span><br><span class="line">    <span class="function">std::lock_guard&lt;std::mutex&gt; <span class="title">lock</span><span class="params">(mRegistryLock)</span></span>;</span><br><span class="line"></span><br><span class="line">    std::string errorMsg;</span><br><span class="line">    std::string verboseMsg;</span><br><span class="line"></span><br><span class="line">    std::unique_ptr&lt;CreatorType&gt; pluginCreator&#123;<span class="keyword">new</span> CreatorType&#123;&#125;&#125;;</span><br><span class="line">    pluginCreator-&gt;<span class="built_in">setPluginNamespace</span>(libNamespace);</span><br><span class="line"></span><br><span class="line">    nvinfer1::plugin::gLogger = <span class="built_in">static_cast</span>&lt;nvinfer1::ILogger*&gt;(logger);</span><br><span class="line">    std::string pluginType = std::string&#123;pluginCreator-&gt;<span class="built_in">getPluginNamespace</span>()&#125;</span><br><span class="line">        + <span class="string">&quot;::&quot;</span> + std::string&#123;pluginCreator-&gt;<span class="built_in">getPluginName</span>()&#125; + <span class="string">&quot; version &quot;</span></span><br><span class="line">        + std::string&#123;pluginCreator-&gt;<span class="built_in">getPluginVersion</span>()&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (mRegistryList.<span class="built_in">find</span>(pluginType) == mRegistryList.<span class="built_in">end</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">bool</span> status = <span class="built_in">getPluginRegistry</span>()-&gt;<span class="built_in">registerCreator</span>(*pluginCreator, libNamespace);<span class="comment">//注册</span></span><br><span class="line">        <span class="keyword">if</span> (status)</span><br><span class="line">        &#123;</span><br><span class="line">            mRegistry.<span class="built_in">push</span>(std::<span class="built_in">move</span>(pluginCreator));</span><br><span class="line">            mRegistryList.<span class="built_in">insert</span>(pluginType);</span><br><span class="line">            verboseMsg = <span class="string">&quot;Registered plugin creator - &quot;</span> + pluginType;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            errorMsg = <span class="string">&quot;Could not register plugin creator -  &quot;</span> + pluginType;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        verboseMsg = <span class="string">&quot;Plugin creator already registered - &quot;</span> + pluginType;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (logger)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (!errorMsg.<span class="built_in">empty</span>())</span><br><span class="line">        &#123;</span><br><span class="line">            nvinfer1::plugin::gLogger-&gt;<span class="built_in">log</span>(ILogger::Severity::kERROR, errorMsg.<span class="built_in">c_str</span>());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (!verboseMsg.<span class="built_in">empty</span>())</span><br><span class="line">        &#123;</span><br><span class="line">            nvinfer1::plugin::gLogger-&gt;<span class="built_in">log</span>(ILogger::Severity::kVERBOSE, verboseMsg.<span class="built_in">c_str</span>());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>把我们自定义的crteator放在下面方便理解：plugin/nvFasterRCNN/nvFasterRCNNPlugin.cpp</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Detach the plugin object from its execution context.</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">RPROIPlugin::detachFromContext</span><span class="params">()</span> <span class="keyword">noexcept</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">RPROIPluginCreator::<span class="built_in">RPROIPluginCreator</span>()</span><br><span class="line">&#123;</span><br><span class="line">    mPluginAttributes.<span class="built_in">clear</span>();</span><br><span class="line">    mPluginAttributes.<span class="built_in">emplace_back</span>(<span class="built_in">PluginField</span>(<span class="string">&quot;poolingH&quot;</span>, <span class="literal">nullptr</span>, PluginFieldType::kINT32, <span class="number">1</span>));</span><br><span class="line">    mPluginAttributes.<span class="built_in">emplace_back</span>(<span class="built_in">PluginField</span>(<span class="string">&quot;poolingW&quot;</span>, <span class="literal">nullptr</span>, PluginFieldType::kINT32, <span class="number">1</span>));</span><br><span class="line">    mPluginAttributes.<span class="built_in">emplace_back</span>(<span class="built_in">PluginField</span>(<span class="string">&quot;featureStride&quot;</span>, <span class="literal">nullptr</span>, PluginFieldType::kINT32, <span class="number">1</span>));</span><br><span class="line">    mPluginAttributes.<span class="built_in">emplace_back</span>(<span class="built_in">PluginField</span>(<span class="string">&quot;preNmsTop&quot;</span>, <span class="literal">nullptr</span>, PluginFieldType::kINT32, <span class="number">1</span>));</span><br><span class="line">    mPluginAttributes.<span class="built_in">emplace_back</span>(<span class="built_in">PluginField</span>(<span class="string">&quot;nmsMaxOut&quot;</span>, <span class="literal">nullptr</span>, PluginFieldType::kINT32, <span class="number">1</span>));</span><br><span class="line">    mPluginAttributes.<span class="built_in">emplace_back</span>(<span class="built_in">PluginField</span>(<span class="string">&quot;anchorsRatioCount&quot;</span>, <span class="literal">nullptr</span>, PluginFieldType::kINT32, <span class="number">1</span>));</span><br><span class="line">    mPluginAttributes.<span class="built_in">emplace_back</span>(<span class="built_in">PluginField</span>(<span class="string">&quot;anchorsScaleCount&quot;</span>, <span class="literal">nullptr</span>, PluginFieldType::kINT32, <span class="number">1</span>));</span><br><span class="line">    mPluginAttributes.<span class="built_in">emplace_back</span>(<span class="built_in">PluginField</span>(<span class="string">&quot;iouThreshold&quot;</span>, <span class="literal">nullptr</span>, PluginFieldType::kFLOAT32, <span class="number">1</span>));</span><br><span class="line">    mPluginAttributes.<span class="built_in">emplace_back</span>(<span class="built_in">PluginField</span>(<span class="string">&quot;minBoxSize&quot;</span>, <span class="literal">nullptr</span>, PluginFieldType::kFLOAT32, <span class="number">1</span>));</span><br><span class="line">    mPluginAttributes.<span class="built_in">emplace_back</span>(<span class="built_in">PluginField</span>(<span class="string">&quot;spatialScale&quot;</span>, <span class="literal">nullptr</span>, PluginFieldType::kFLOAT32, <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO Do we need to pass the size attribute here for float arrarys, we</span></span><br><span class="line">    <span class="comment">// dont have that information at this point.</span></span><br><span class="line">    mPluginAttributes.<span class="built_in">emplace_back</span>(<span class="built_in">PluginField</span>(<span class="string">&quot;anchorsRatios&quot;</span>, <span class="literal">nullptr</span>, PluginFieldType::kFLOAT32, <span class="number">1</span>));</span><br><span class="line">    mPluginAttributes.<span class="built_in">emplace_back</span>(<span class="built_in">PluginField</span>(<span class="string">&quot;anchorsScales&quot;</span>, <span class="literal">nullptr</span>, PluginFieldType::kFLOAT32, <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    mFC.nbFields = mPluginAttributes.<span class="built_in">size</span>();</span><br><span class="line">    mFC.fields = mPluginAttributes.<span class="built_in">data</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">RPROIPluginCreator::~<span class="built_in">RPROIPluginCreator</span>()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// Free allocated memory (if any) here</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span>* <span class="title">RPROIPluginCreator::getPluginName</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> RPROI_PLUGIN_NAME;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> <span class="type">char</span>* <span class="title">RPROIPluginCreator::getPluginVersion</span><span class="params">()</span> <span class="type">const</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> RPROI_PLUGIN_VERSION;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">const</span> PluginFieldCollection* <span class="title">RPROIPluginCreator::getFieldNames</span><span class="params">()</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> &amp;mFC;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">IPluginV2Ext* <span class="title">RPROIPluginCreator::createPlugin</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* name, <span class="type">const</span> PluginFieldCollection* fc)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> PluginField* fields = fc-&gt;fields;</span><br><span class="line">    <span class="type">int</span> nbFields = fc-&gt;nbFields;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nbFields; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">const</span> <span class="type">char</span>* attrName = fields[i].name;</span><br><span class="line">        <span class="keyword">if</span> (!<span class="built_in">strcmp</span>(attrName, <span class="string">&quot;poolingH&quot;</span>))</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">ASSERT</span>(fields[i].type == PluginFieldType::kINT32);</span><br><span class="line">            params.poolingH = *(<span class="built_in">static_cast</span>&lt;<span class="type">const</span> <span class="type">int</span>*&gt;(fields[i].data));</span><br><span class="line">        &#125;</span><br><span class="line">		.....</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// This object will be deleted when the network is destroyed, which will</span></span><br><span class="line">    <span class="comment">// call RPROIPlugin::terminate()</span></span><br><span class="line">    RPROIPlugin* plugin = <span class="keyword">new</span> <span class="built_in">RPROIPlugin</span>(params, anchorsRatios.<span class="built_in">data</span>(), anchorsScales.<span class="built_in">data</span>());</span><br><span class="line">    plugin-&gt;<span class="built_in">setPluginNamespace</span>(mNamespace.<span class="built_in">c_str</span>());</span><br><span class="line">    <span class="keyword">return</span> plugin;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">IPluginV2Ext* <span class="title">RPROIPluginCreator::deserializePlugin</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* name, <span class="type">const</span> <span class="type">void</span>* serialData, <span class="type">size_t</span> serialLength)</span> <span class="keyword">noexcept</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// This object will be deleted when the network is destroyed, which will</span></span><br><span class="line">    <span class="comment">// call RPROIPlugin::terminate()</span></span><br><span class="line">    RPROIPlugin* plugin = <span class="keyword">new</span> <span class="built_in">RPROIPlugin</span>(serialData, serialLength);</span><br><span class="line">    plugin-&gt;<span class="built_in">setPluginNamespace</span>(mNamespace.<span class="built_in">c_str</span>());</span><br><span class="line">    <span class="keyword">return</span> plugin;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="4-1-11-添加自定义的plugin，使trtexec可以识别"><a href="#4-1-11-添加自定义的plugin，使trtexec可以识别" class="headerlink" title="4.1.11 添加自定义的plugin，使trtexec可以识别"></a>4.1.11 添加自定义的plugin，使trtexec可以识别</h3><p>如何将自己的 <code>plugin</code>编译到库中，使得 <code>trtexec</code>可以识别自定义层呢？</p>
<p>参考编写一个自己的 <code>plugin</code>代码比如上面4.1.10的 <code>RPROIPlugin</code>层，然后添加到 <code>initLibNvInferPlugins</code>函数中。编译成库，替代自带的库 <code>libnvinfer_plugin.so</code>就可以了。</p>
<h1 id="5-官方库编译"><a href="#5-官方库编译" class="headerlink" title="5 官方库编译"></a>5 官方库编译</h1><h2 id="5-1-官方plugin编译"><a href="#5-1-官方plugin编译" class="headerlink" title="5.1 官方plugin编译"></a>5.1 官方plugin编译</h2><p>源码<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT">官网地址</a>，这里使用命令下载，否则第一步就报错。这里也是按照官方的方法进行编译的</p>
<p><img src="/home/huolin/.config/Typora/typora-user-images/image-20220928141043140.png" alt="image-20220928141043140"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">下载源码</span></span><br><span class="line">git clone -b master https://github.com/nvidia/TensorRT TensorRT</span><br><span class="line">cd TensorRT</span><br><span class="line">git checkout -b branch8.4.1 8.4.1 #因为我电脑安装的TensorRT版本是8.4.1，为了方便检出了对应的版本进行编译 这时候cudnn也会切换到对应的版本，有一个坑就是我又安装了cuda的11.6版本（原为11.4），也就是tensorRT cuda cudnn的版本要匹配。</span><br><span class="line">git submodule update --init --recursive</span><br></pre></td></tr></table></figure>
<p>这里源码的路径就是TensorRT</p>
<h3 id="5-1-1-PC端编译"><a href="#5-1-1-PC端编译" class="headerlink" title="5.1.1 PC端编译"></a>5.1.1 PC端编译</h3><p>在源码目录内创建build目录，因为这里只是编译plugin因此将sameple和parsers都关闭了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p build &amp;&amp; cd build</span><br><span class="line">cmake -DBUILD_PARSERS=OFF -DBUILD_SAMPLES=OFF -DTRT_OUT_DIR=`pwd`/out ../</span><br><span class="line">make -j$(nproc)</span><br></pre></td></tr></table></figure>
<p>编译完成后在build/out目录下会有对应的plugin的动态库文件。</p>
<p><img src="/home/huolin/.config/Typora/typora-user-images/image-20220928141703758.png" alt="image-20220928141703758"></p>
<p>自带的库为</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/usr/lib/x86_64-linux-gnu/libnvinfer_plugin.so.8.4.1</span><br><span class="line">/usr/lib/x86_64-linux-gnu/libnvinfer_plugin.so</span><br><span class="line">/usr/lib/x86_64-linux-gnu/libnvinfer_plugin.so.8</span><br><span class="line">/usr/lib/x86_64-linux-gnu/libnvinfer_plugin_static.a</span><br></pre></td></tr></table></figure>
<h3 id="5-1-2-交叉编译"><a href="#5-1-2-交叉编译" class="headerlink" title="5.1.2 交叉编译"></a>5.1.2 交叉编译</h3><h3 id="5-1-3-又一点理解"><a href="#5-1-3-又一点理解" class="headerlink" title="5.1.3 又一点理解"></a>5.1.3 又一点理解</h3><p>plugin和TensorRT如何结合起来呢？</p>
<p>一个自定义的plugin如果希望tensorRT识别并使用与构建engine需要两步</p>
<ul>
<li>注册plugin</li>
<li>将plugin加入网络</li>
</ul>
<p>有三种方法来实现：</p>
<ol>
<li>使用c++ 的API，对应上面的2.1节。</li>
<li>使用python的API</li>
<li>使用ONNX parser自动解析。（后面看到实际这种方法还是调用API实现的，只不过是封装了接口而已）</li>
</ol>
<p>下面结合ONNX parser解析器的使用来说明</p>
<h4 id="第一步-注册plugin"><a href="#第一步-注册plugin" class="headerlink" title="第一步 注册plugin"></a>第一步 注册plugin</h4><p>首先TensorRT提供了一个工具trtexec来将onnx格式的模型转换为trt的plan engine文件。并提供了实现的源码，在TensorRT/samples/trtexec/trtexec.cpp路径下。</p>
<img src="/TensorRT/TensorRT-plugin/image-20220929191047186.png" class="" title="image-20220929191047186">
<p>从上图可以看出调用了一个函数 <code>initLibNvInferPlugins</code>这个函数就是2.1节中说到的动态注册的函数。官方有实现在 <code>plugin/api/InferPlugin.cpp</code>这里再一次贴出来源码方便理解。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInfer.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;NvInferPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;checkMacrosPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;plugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;array&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;memory&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;mutex&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unordered_set&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1::plugin;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;batchTilePlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;batchedNMSPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;coordConvACPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cropAndResizePlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;decodeBbox3D.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;detectionLayerPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;efficientNMSPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;tftrt/efficientNMSImplicitTFTRTPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;tftrt/efficientNMSExplicitTFTRTPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;flattenConcat.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;generateDetectionPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;gridAnchorPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;instanceNormalizationPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;lReluPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;multilevelCropAndResizePlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;multilevelProposeROIPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;multiscaleDeformableAttnPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;nmsPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;normalizePlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;nvFasterRCNNPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;pillarScatter.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;priorBoxPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;proposalLayerPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;proposalPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;pyramidROIAlignPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;regionPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;reorgPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;resizeNearestPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;scatterPlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;specialSlicePlugin.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;split.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;voxelGenerator.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> nvinfer1::plugin::RPROIParams;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> nvinfer1</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> plugin</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">extern</span> ILogger* gLogger;</span><br><span class="line"></span><br><span class="line"><span class="comment">// This singleton ensures that each plugin is only registered once for a given</span></span><br><span class="line"><span class="comment">// namespace and type, and attempts of duplicate registration are ignored.</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PluginCreatorRegistry</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="type">static</span> PluginCreatorRegistry&amp; <span class="title">getInstance</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="type">static</span> PluginCreatorRegistry instance;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> CreatorType&gt;</span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">addPluginCreator</span><span class="params">(<span class="type">void</span>* logger, <span class="type">const</span> <span class="type">char</span>* libNamespace)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="comment">// Make accesses to the plugin creator registry thread safe</span></span><br><span class="line">        <span class="function">std::lock_guard&lt;std::mutex&gt; <span class="title">lock</span><span class="params">(mRegistryLock)</span></span>;</span><br><span class="line"></span><br><span class="line">        std::string errorMsg;</span><br><span class="line">        std::string verboseMsg;</span><br><span class="line"></span><br><span class="line">        std::unique_ptr&lt;CreatorType&gt; pluginCreator&#123;<span class="keyword">new</span> CreatorType&#123;&#125;&#125;;</span><br><span class="line">        pluginCreator-&gt;<span class="built_in">setPluginNamespace</span>(libNamespace);</span><br><span class="line"></span><br><span class="line">        nvinfer1::plugin::gLogger = <span class="built_in">static_cast</span>&lt;nvinfer1::ILogger*&gt;(logger);</span><br><span class="line">        std::string pluginType = std::string&#123;pluginCreator-&gt;<span class="built_in">getPluginNamespace</span>()&#125;</span><br><span class="line">            + <span class="string">&quot;::&quot;</span> + std::string&#123;pluginCreator-&gt;<span class="built_in">getPluginName</span>()&#125; + <span class="string">&quot; version &quot;</span></span><br><span class="line">            + std::string&#123;pluginCreator-&gt;<span class="built_in">getPluginVersion</span>()&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (mRegistryList.<span class="built_in">find</span>(pluginType) == mRegistryList.<span class="built_in">end</span>())</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">bool</span> status = <span class="built_in">getPluginRegistry</span>()-&gt;<span class="built_in">registerCreator</span>(*pluginCreator, libNamespace);<span class="comment">//实现plugin的注册</span></span><br><span class="line">            <span class="keyword">if</span> (status)</span><br><span class="line">            &#123;</span><br><span class="line">                mRegistry.<span class="built_in">push</span>(std::<span class="built_in">move</span>(pluginCreator));</span><br><span class="line">                mRegistryList.<span class="built_in">insert</span>(pluginType);</span><br><span class="line">                verboseMsg = <span class="string">&quot;Registered plugin creator - &quot;</span> + pluginType;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                errorMsg = <span class="string">&quot;Could not register plugin creator -  &quot;</span> + pluginType;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            verboseMsg = <span class="string">&quot;Plugin creator already registered - &quot;</span> + pluginType;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (logger)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (!errorMsg.<span class="built_in">empty</span>())</span><br><span class="line">            &#123;</span><br><span class="line">                nvinfer1::plugin::gLogger-&gt;<span class="built_in">log</span>(ILogger::Severity::kERROR, errorMsg.<span class="built_in">c_str</span>());</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (!verboseMsg.<span class="built_in">empty</span>())</span><br><span class="line">            &#123;</span><br><span class="line">                nvinfer1::plugin::gLogger-&gt;<span class="built_in">log</span>(ILogger::Severity::kVERBOSE, verboseMsg.<span class="built_in">c_str</span>());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ~<span class="built_in">PluginCreatorRegistry</span>()</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function">std::lock_guard&lt;std::mutex&gt; <span class="title">lock</span><span class="params">(mRegistryLock)</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Release pluginCreators in LIFO order of registration.</span></span><br><span class="line">        <span class="keyword">while</span> (!mRegistry.<span class="built_in">empty</span>())</span><br><span class="line">        &#123;</span><br><span class="line">            mRegistry.<span class="built_in">pop</span>();</span><br><span class="line">        &#125;</span><br><span class="line">        mRegistryList.<span class="built_in">clear</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">PluginCreatorRegistry</span>() &#123;&#125;</span><br><span class="line"></span><br><span class="line">    std::mutex mRegistryLock;</span><br><span class="line">    std::stack&lt;std::unique_ptr&lt;IPluginCreator&gt;&gt; mRegistry;</span><br><span class="line">    std::unordered_set&lt;std::string&gt; mRegistryList;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">PluginCreatorRegistry</span>(PluginCreatorRegistry <span class="type">const</span>&amp;) = <span class="keyword">delete</span>;</span><br><span class="line">    <span class="type">void</span> <span class="keyword">operator</span>=(PluginCreatorRegistry <span class="type">const</span>&amp;) = <span class="keyword">delete</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> CreatorType&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">initializePlugin</span><span class="params">(<span class="type">void</span>* logger, <span class="type">const</span> <span class="type">char</span>* libNamespace)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    PluginCreatorRegistry::<span class="built_in">getInstance</span>().<span class="built_in">addPluginCreator</span>&lt;CreatorType&gt;(logger, libNamespace);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125; <span class="comment">// namespace plugin</span></span><br><span class="line">&#125; <span class="comment">// namespace nvinfer1</span></span><br><span class="line"><span class="comment">// New Plugin APIs</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">extern</span> <span class="string">&quot;C&quot;</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="function"><span class="type">bool</span> <span class="title">initLibNvInferPlugins</span><span class="params">(<span class="type">void</span>* logger, <span class="type">const</span> <span class="type">char</span>* libNamespace)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::BatchTilePluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::BatchedNMSPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::BatchedNMSDynamicPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::CoordConvACPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::CropAndResizePluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::CropAndResizeDynamicPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::DecodeBbox3DPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::DetectionLayerPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::EfficientNMSPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::EfficientNMSONNXPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::EfficientNMSExplicitTFTRTPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::EfficientNMSImplicitTFTRTPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::FlattenConcatPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::GenerateDetectionPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::GridAnchorPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::GridAnchorRectPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::InstanceNormalizationPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::LReluPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::MultilevelCropAndResizePluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::MultilevelProposeROIPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::MultiscaleDeformableAttnPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::NMSPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::NMSDynamicPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::NormalizePluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::PillarScatterPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::PriorBoxPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::ProposalLayerPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::ProposalPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::ProposalDynamicPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::PyramidROIAlignPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::RegionPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::ReorgPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::ResizeNearestPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::RPROIPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::ScatterNDPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::SpecialSlicePluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::SplitPluginCreator&gt;(logger, libNamespace);</span><br><span class="line">        <span class="built_in">initializePlugin</span>&lt;nvinfer1::plugin::VoxelGeneratorPluginCreator&gt;(logger, libNamespace);<span class="comment">//VoxelGeneratorPluginCreator就是插件的creator的名称</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="comment">// extern &quot;C&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>从上面的函数分析以下做什么了</p>
<p>initLibNvInferPlugins调用了一堆类似下面的函数，其中调用了各自plugin的Creator，initLibNvInferPlugins调用了addPluginCreator，其中又调用了registerCreator实现plugin 的注册。</p>
<p>例如：</p>
<p><code>initializePlugin&lt;nvinfer1::plugin::VoxelGeneratorPluginCreator&gt;(logger, libNamespace);</code></p>
<p>就是plugin/voxelGeneratorPlugin/voxelGenerator.cpp路径下的plugin的实际实现代码，也就是第三节中自己实现的plugin源码</p>
<img src="/TensorRT/TensorRT-plugin/image-20220929191752527.png" class="" title="image-20220929191752527">
<h4 id="第二步-将plugin加入网络"><a href="#第二步-将plugin加入网络" class="headerlink" title="第二步 将plugin加入网络"></a>第二步 将plugin加入网络</h4><p>注册后网络还是不能识别，需要将plugin添加到network中才可以。</p>
<p>按照2.1节的描述需要调用一个addPluginV2的函数。如果是使用API函数来构建network，我们需要自己调用addPluginV2函数，就类似3.4节的例子。但是使用ONNX parser是如何识别的？</p>
<p>参考官方博客<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/estimating-depth-beyond-2d-using-custom-layers-on-tensorrt-and-onnx-models/">网址</a></p>
<p>onnx的parser的源码也有 <code>parsers/onnx/builtin_op_importers.cpp</code>，如下图，可以看出来如果不是官方支持的原生plugin，那么就会运行到这里，也就是parser也调用了addPluginV2函数。这下就比较明了了。</p>
<img src="/TensorRT/TensorRT-plugin/image-20220929193123478.png" class="" title="image-20220929193123478">
<h4 id="几个注意的点"><a href="#几个注意的点" class="headerlink" title="几个注意的点"></a>几个注意的点</h4><h4 id="plugin的名称"><a href="#plugin的名称" class="headerlink" title="plugin的名称"></a>plugin的名称</h4><ol>
<li><p>参考官方博客<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/estimating-depth-beyond-2d-using-custom-layers-on-tensorrt-and-onnx-models/">网址</a>提到了我们的onnx模型中plugin的名称需要和我们编写的plugin名称一致。</p>
<p>下面图中是提到的ONNX模型，可以看出来名称是 <code>type</code>字段，也就是 <code>GroupNormalizationPluginCreator</code></p>
<img src="/TensorRT/TensorRT-plugin/post-processed-gn-layer-3.png" class="" title="The post-processed Group Normalization subgraph consists of only a single group normalization layer with scale and bias as inputs and custom plugin attributes.">
<p>再看一个例子，<strong>这里的plugin的名称是 type字段的 RPROI 不是name字段的RPROIFused</strong></p>
<img src="/TensorRT/TensorRT-plugin/image-20220929194124761.png" class="" title="image-20220929194124761">
<img src="/TensorRT/TensorRT-plugin/image-20220929194013225.png" class="" title="image-20220929194013225">
</li>
</ol>
<h4 id="plugin参数"><a href="#plugin参数" class="headerlink" title="plugin参数"></a>plugin参数</h4><p>就像上面的RPROI插件，里面有一堆的参数，如下图</p>
<p>这些参数在我们定义插件的时候如何确认？开始的时候我没有确认，名称也没有对应上，使用trtexec转换onnx模型时报了一堆的错误，例如</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Attribute poolingH not found in plugin node</span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT-plugin/image-20220929194516628.png" class="" title="image-20220929194516628">
<p>也就是plugin没有poolingH等属性。我修改之前的代码下下图注释的部分，修改为上面箭头就可以了。</p>
<p>可以看出来，编写的plugin的Creator构造函数添加的属性需要与模型的属性描述一致才可以。当然后面的Creator::createPlugin中也要对应起来。</p>
<img src="/TensorRT/TensorRT-plugin/image-20221130152004319.png" class="" title="image-20221130152004319">
<img src="/TensorRT/TensorRT-plugin/image-20220929195248286.png" class="" title="image-20220929195248286">
<h1 id="使用中的问题"><a href="#使用中的问题" class="headerlink" title="使用中的问题"></a>使用中的问题</h1><h2 id="1-trtexec-dynamic-batch-size"><a href="#1-trtexec-dynamic-batch-size" class="headerlink" title="1 trtexec dynamic batch size"></a>1 trtexec dynamic batch size</h2><p>问题描述：</p>
<p>有一个onnx模型有两个输入，分别是 blob1 [1,3,480,1088]和im_info [2,1,1,3]。可以看出batch size 分别是1和2。我希望都设定为1，并转换为trt的engine。</p>
<img src="/TensorRT/TensorRT-plugin/image-20221014150954138.png" class="" title="image-20221014150954138">
<p>在使用trtexec转换为trt模型的时候指定了—shapes=blob1:1x3x480x1088,im_info:1x1x1x3</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">	--shapes=blob1:1x3x480x1088,im_info:1x1x1x3 \</span></span><br><span class="line"><span class="language-bash"></span><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">pseudo3d_v8_20220721_release.onnx</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">new_crowd_612_20220822_fp_fpn3_0823.onnx</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">02-用上面的 .onnx 构建一个 TensorRT 引擎并作推理</span></span><br><span class="line">trtexec \</span><br><span class="line">    --onnx=new_crowd_612_0921_4cls_0926_15_2.onnx \</span><br><span class="line">	--minShapes=blob1:1x3x480x1088,im_info:1x1x1x3 \</span><br><span class="line">	--optShapes=blob1:1x3x480x1088,im_info:1x1x1x3 \</span><br><span class="line">	--maxShapes=blob1:1x3x480x1088,im_info:1x1x1x3 \</span><br><span class="line">    --memPoolSize=workspace:1024MiB \</span><br><span class="line">    --saveEngine=new_crowd_612_0921_4cls_0926_15.onnx.INT8.trtmodel \</span><br><span class="line">    --verbose \</span><br><span class="line">    &gt; result-FP32.log </span><br></pre></td></tr></table></figure>
<p>这是使用上述命令转换报错</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[10/14/2022-15:13:21] [W] [TRT] onnx2trt_utils.cpp:369: Your ONNX model has been generated with INT64 weights, <span class="keyword">while</span> TensorRT does not natively support INT64. Attempting to cast down to INT32.</span><br><span class="line">[10/14/2022-15:13:21] [E] Static model does not take explicit shapes since the shape of inference tensors will be determined by the model itself</span><br><span class="line">[10/14/2022-15:13:21] [E] Network And Config setup failed</span><br><span class="line">[10/14/2022-15:13:21] [E] Building engine failed</span><br><span class="line">[10/14/2022-15:13:21] [E] Failed to create engine from model or file.</span><br><span class="line">[10/14/2022-15:13:21] [E] Engine <span class="built_in">set</span> up failed</span><br><span class="line">[10/14/2022-15:13:25] [W] * Throughput may be bound by Enqueue Time rather than GPU Compute and the GPU may be under-utilized.</span><br><span class="line">[10/14/2022-15:13:25] [W]   If not already <span class="keyword">in</span> use, --useCudaGraph (utilize CUDA graphs <span class="built_in">where</span> possible) may increase the throughput.</span><br><span class="line">[10/14/2022-15:13:25] [W] * GPU compute time is unstable, with coefficient of variance = 4.60091%.</span><br><span class="line">[10/14/2022-15:13:25] [W]   If not already <span class="keyword">in</span> use, locking GPU clock frequency or adding --useSpinWait may improve the stability.</span><br><span class="line">huolin@huolin:/media/zyd/work/model$ </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>上面的报错说明了原因就是 <code>Static model does not take explicit shapes since the shape of inference tensors will be determined by the model itself</code>,说人话就是模型的输入是静态的，不能改变。从上面的图中看，模型的输入的确是blob1 [1,3,480,1088]和im_info [2,1,1,3]。batchsize指定为1和2.是固定的。</p>
<p>那么就需要修改一下onnx的模型了。</p>
<p>参考 <a target="_blank" rel="noopener" href="https://github.com/onnx/onnx/issues/2182">https://github.com/onnx/onnx/issues/2182</a></p>
<p>这个帖子提供了一个python脚本实现脚本修改batch size</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">change_input_dim</span>(<span class="params">model,</span>):</span><br><span class="line">    batch_size = <span class="string">&quot;1&quot;</span><span class="comment">#这里修改自己希望的batchsize</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># The following code changes the first dimension of every input to be batch_size</span></span><br><span class="line">    <span class="comment"># Modify as appropriate ... note that this requires all inputs to</span></span><br><span class="line">    <span class="comment"># have the same batch_size </span></span><br><span class="line">    inputs = model.graph.<span class="built_in">input</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">input</span> <span class="keyword">in</span> inputs:</span><br><span class="line">        <span class="comment"># Checks omitted.This assumes that all inputs are tensors and have a shape with first dim.</span></span><br><span class="line">        <span class="comment"># Add checks as needed.</span></span><br><span class="line">        dim1 = <span class="built_in">input</span>.<span class="built_in">type</span>.tensor_type.shape.dim[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># update dim to be a symbolic value</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(batch_size, <span class="built_in">str</span>):</span><br><span class="line">            <span class="comment"># set dynamic batch size</span></span><br><span class="line">            dim1.dim_param = batch_size</span><br><span class="line">        <span class="keyword">elif</span> (<span class="built_in">isinstance</span>(batch_size, <span class="built_in">str</span>) <span class="keyword">and</span> batch_size.isdigit()) <span class="keyword">or</span> <span class="built_in">isinstance</span>(batch_size, <span class="built_in">int</span>):</span><br><span class="line">            <span class="comment"># set given batch size</span></span><br><span class="line">            dim1.dim_value = <span class="built_in">int</span>(batch_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># set batch size of 1</span></span><br><span class="line">            dim1.dim_value = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply</span>(<span class="params">transform, infile, outfile</span>):</span><br><span class="line">    model = onnx.load(infile)</span><br><span class="line">    transform(model,)</span><br><span class="line">    onnx.save(model, outfile)</span><br><span class="line"><span class="comment">#apply(change_input_dim, &lt;path-to-input-model&gt;, &lt;path-to-output-model&gt;)</span></span><br><span class="line">apply(change_input_dim, <span class="string">&quot;new_crowd_612_0921_4cls_0926_15.onnx&quot;</span>, <span class="string">&quot;new_crowd_612_0921_4cls_0926_15_2.onnx&quot;</span>)<span class="comment">#这里是输入输出的onnx模型文件</span></span><br></pre></td></tr></table></figure>
<p>使用上面的脚本运行后我得到了新的onnx文件 <code>new_crowd_612_0921_4cls_0926_15_2.onnx</code>,查看batchsize如下。可以看出来已经修改了</p>
<p>分别是[1,3,480,1088]和[1,1,1,3]修改完成。</p>
<img src="/TensorRT/TensorRT-plugin/image-20221014152058430.png" class="" title="image-20221014152058430">
<h1 id="附录："><a href="#附录：" class="headerlink" title="附录："></a>附录：</h1><p>Plugin官方API <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_plugin_creator.html#a037a085fc7d0cee9c9a2789a4b83f66f">https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_plugin_creator.html#a037a085fc7d0cee9c9a2789a4b83f66f</a></p>
<p>Plugin官方开发指南 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#plugin-api-desc">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#plugin-api-desc</a></p>
<p>官方Plugin使用例子 <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/samples/sampleFasterRCNN">https://github.com/NVIDIA/TensorRT/tree/main/samples/sampleFasterRCNN</a></p>
<p>trtexec 使用 <a target="_blank" rel="noopener" href="https://www.ccoderun.ca/programming/doxygen/tensorrt/md_TensorRT_samples_opensource_trtexec_README.html">https://www.ccoderun.ca/programming/doxygen/tensorrt/md_TensorRT_samples_opensource_trtexec_README.html</a></p>
<p>博客参考：</p>
<ul>
<li>实现TensorRT自定义插件(plugin)自由 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/297002406">https://zhuanlan.zhihu.com/p/297002406</a></li>
<li><a target="_blank" rel="noopener" href="http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/AI%20%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%85%B6%E5%AE%83%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/TensorRT/%E4%B8%89%EF%BC%8C%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorRT%20C%2B%2B%20API%E6%90%AD%E5%BB%BA%E7%BD%91%E7%BB%9C/">三，如何使用tensorRT C%2B%2B API搭建网络/</a></li>
</ul>
<h2 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a><strong>RPN</strong></h2><p>全称是region proposal network作用是为第二阶段提供高质量的目标候选框，获得候选框的目的是为了给第二阶段提供优质的roi框</p>
<h2 id="ROI"><a href="#ROI" class="headerlink" title="ROI"></a>ROI</h2><p>(<strong>region of interest</strong>)关注区域</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>奔跑的IC
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://example.com/TensorRT/TensorRT-plugin/" title="TensorRT-plugin">http://example.com/TensorRT/TensorRT-plugin/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/git/" rel="tag"><i class="fa fa-tag"></i> git</a>
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C</a>
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C++</a>
              <a href="/tags/GPU/" rel="tag"><i class="fa fa-tag"></i> GPU</a>
              <a href="/tags/Tensorrt/" rel="tag"><i class="fa fa-tag"></i> Tensorrt</a>
              <a href="/tags/DeepLearning/" rel="tag"><i class="fa fa-tag"></i> DeepLearning</a>
              <a href="/tags/Typora/" rel="tag"><i class="fa fa-tag"></i> Typora</a>
              <a href="/tags/Plugin/" rel="tag"><i class="fa fa-tag"></i> Plugin</a>
              <a href="/tags/cmake/" rel="tag"><i class="fa fa-tag"></i> cmake</a>
              <a href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> 卷积神经网络</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT%20INT8%E9%87%8F%E5%8C%96%E4%BB%A3%E7%A0%81/" rel="prev" title="TensorRT INT8量化代码">
                  <i class="fa fa-chevron-left"></i> TensorRT INT8量化代码
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/1-%E7%AE%80%E4%BB%8B/" rel="next" title="1-简介">
                  1-简介 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">奔跑的IC</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"zmurder","repo":"zmurder.github.io","client_id":"cf2343f27b6c29efe0bc","client_secret":"3268a1fa92706c7358d5421f88f76a0f7ada3188","admin_user":"zmurder","distraction_free_mode":true,"proxy":"https://strong-caramel-969805.netlify.app/github_access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"1d78264833d472796ed90d86499a583c"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
