<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
  <meta name="msvalidate.01" content="7EC20DBC74B004C2782077570E15C280">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
    <meta name="description" content="TensorRT和DLA(Deep Learning Accelerator)">
<meta property="og:type" content="article">
<meta property="og:title" content="12-TensorRT和DLA(Deep_Learning_Accelerator)">
<meta property="og:url" content="http://example.com/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/12-TensorRT%E5%92%8CDLA(Deep_Learning_Accelerator)/index.html">
<meta property="og:site_name" content="奔跑的IC">
<meta property="og:description" content="TensorRT和DLA(Deep Learning Accelerator)">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/12-TensorRT%E5%92%8CDLA(Deep_Learning_Accelerator)/rdp.jpg">
<meta property="article:published_time" content="2024-12-01T10:13:45.364Z">
<meta property="article:modified_time" content="2024-12-01T10:13:45.364Z">
<meta property="article:author" content="奔跑的IC">
<meta property="article:tag" content="git">
<meta property="article:tag" content="C">
<meta property="article:tag" content="C++">
<meta property="article:tag" content="GPU">
<meta property="article:tag" content="Tensorrt">
<meta property="article:tag" content="DeepLearning">
<meta property="article:tag" content="卷积神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/12-TensorRT%E5%92%8CDLA(Deep_Learning_Accelerator)/rdp.jpg">


<link rel="canonical" href="http://example.com/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/12-TensorRT%E5%92%8CDLA(Deep_Learning_Accelerator)/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://example.com/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/12-TensorRT%E5%92%8CDLA(Deep_Learning_Accelerator)/","path":"TensorRT/TensorRT中文版开发手册/12-TensorRT和DLA(Deep_Learning_Accelerator)/","title":"12-TensorRT和DLA(Deep_Learning_Accelerator)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>12-TensorRT和DLA(Deep_Learning_Accelerator) | 奔跑的IC</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">奔跑的IC</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-reorder fa-fw"></i>文章列表</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#TensorRT%E5%92%8CDLA-Deep-Learning-Accelerator"><span class="nav-text">TensorRT和DLA(Deep Learning Accelerator)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#12-1-Running-On-DLA-During-TensorRT-Inference"><span class="nav-text">12.1. Running On DLA During TensorRT Inference</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%EF%BC%9A%E5%AF%B9%E4%BA%8E%E4%BB%BB%E4%BD%95%E5%BC%A0%E9%87%8F%EF%BC%8C%E7%B4%A2%E5%BC%95%E7%BB%B4%E5%BA%A6%E7%9A%84%E6%80%BB%E4%BD%93%E7%A7%AF%E5%8A%A0%E4%B8%8A%E8%AF%B7%E6%B1%82%E7%9A%84%E6%89%B9%E9%87%8F%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%BE%97%E8%B6%85%E8%BF%87%E6%AD%A4%E5%87%BD%E6%95%B0%E8%BF%94%E5%9B%9E%E7%9A%84%E5%80%BC%E3%80%82"><span class="nav-text">注意：对于任何张量，索引维度的总体积加上请求的批量大小不得超过此函数返回的值。</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-1-1-Example-sampleMNIST-With-DLA"><span class="nav-text">12.1.1. Example: sampleMNIST With DLA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-1-2-Example-Enable-DLA-Mode-For-A-Layer-During-Network-Creation"><span class="nav-text">12.1.2. Example: Enable DLA Mode For A Layer During Network Creation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-2-DLA-Supported-Layers"><span class="nav-text">12.2. DLA Supported Layers</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%EF%BC%9A-DLA-%E7%9A%84%E6%89%B9%E9%87%8F%E5%A4%A7%E5%B0%8F%E6%98%AF%E9%99%A4CHW%E7%BB%B4%E5%BA%A6%E4%B9%8B%E5%A4%96%E7%9A%84%E6%89%80%E6%9C%89%E7%B4%A2%E5%BC%95%E7%BB%B4%E5%BA%A6%E7%9A%84%E4%B9%98%E7%A7%AF%E3%80%82%E4%BE%8B%E5%A6%82%EF%BC%8C%E5%A6%82%E6%9E%9C%E8%BE%93%E5%85%A5%E7%BB%B4%E5%BA%A6%E4%B8%BANPQRS-%EF%BC%8C%E5%88%99%E6%9C%89%E6%95%88%E6%89%B9%E9%87%8F%E5%A4%A7%E5%B0%8F%E4%B8%BAN-P-%E3%80%82"><span class="nav-text">注意： DLA 的批量大小是除CHW维度之外的所有索引维度的乘积。例如，如果输入维度为NPQRS ，则有效批量大小为N*P 。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%EF%BC%9A%E5%9C%A8-Xavier-%E4%B8%8A%EF%BC%8CTensorRT-%E5%B0%86-DLA-Scale-%E5%B1%82%E5%92%8C-DLA-ElementWise-%E5%B1%82%E4%B8%8E%E6%93%8D%E4%BD%9CSum%E8%BF%9E%E6%8E%A5%E4%BB%A5%E6%94%AF%E6%8C%81Sub%E6%93%8D%E4%BD%9C%EF%BC%8C%E5%8D%95%E4%B8%AA-Xavier-DLA-ElementWise-%E5%B1%82%E4%B8%8D%E6%94%AF%E6%8C%81%E3%80%82"><span class="nav-text">注意：在 Xavier 上，TensorRT 将 DLA Scale 层和 DLA ElementWise 层与操作Sum连接以支持Sub操作，单个 Xavier DLA ElementWise 层不支持。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%EF%BC%9A%E5%BD%93%E4%BD%BF%E7%94%A8-TensorRT-%E5%9C%A8-DLA-%E4%B8%8A%E8%BF%90%E8%A1%8C-INT8-%E7%BD%91%E7%BB%9C%E6%97%B6%EF%BC%8C%E5%BB%BA%E8%AE%AE%E5%B0%86%E6%93%8D%E4%BD%9C%E6%B7%BB%E5%8A%A0%E5%88%B0%E5%90%8C%E4%B8%80%E5%AD%90%E5%9B%BE%E4%B8%AD%EF%BC%8C%E4%BB%A5%E9%80%9A%E8%BF%87%E5%85%81%E8%AE%B8%E5%AE%83%E4%BB%AC%E8%9E%8D%E5%90%88%E5%B9%B6%E4%B8%BA%E4%B8%AD%E9%97%B4%E7%BB%93%E6%9E%9C%E4%BF%9D%E7%95%99%E6%9B%B4%E9%AB%98%E7%9A%84%E7%B2%BE%E5%BA%A6%E6%9D%A5%E5%87%8F%E5%B0%91%E5%9C%A8-DLA-%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%9A%84%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%90%E5%9B%BE%E4%B8%8A%E7%9A%84%E9%87%8F%E5%8C%96%E8%AF%AF%E5%B7%AE%E3%80%82%E9%80%9A%E8%BF%87%E5%B0%86%E5%BC%A0%E9%87%8F%E8%AE%BE%E7%BD%AE%E4%B8%BA%E7%BD%91%E7%BB%9C%E8%BE%93%E5%87%BA%E5%BC%A0%E9%87%8F%E6%9D%A5%E6%8B%86%E5%88%86%E5%AD%90%E5%9B%BE%E4%BB%A5%E6%A3%80%E6%9F%A5%E4%B8%AD%E9%97%B4%E7%BB%93%E6%9E%9C%E5%8F%AF%E8%83%BD%E4%BC%9A%E7%94%B1%E4%BA%8E%E7%A6%81%E7%94%A8%E8%BF%99%E4%BA%9B%E4%BC%98%E5%8C%96%E8%80%8C%E5%AF%BC%E8%87%B4%E4%B8%8D%E5%90%8C%E7%BA%A7%E5%88%AB%E7%9A%84%E9%87%8F%E5%8C%96%E8%AF%AF%E5%B7%AE%E3%80%82"><span class="nav-text">注意：当使用 TensorRT 在 DLA 上运行 INT8 网络时，建议将操作添加到同一子图中，以通过允许它们融合并为中间结果保留更高的精度来减少在 DLA 上运行的网络的子图上的量化误差。通过将张量设置为网络输出张量来拆分子图以检查中间结果可能会由于禁用这些优化而导致不同级别的量化误差。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-3-GPU-Fallback-Mode"><span class="nav-text">12.3. GPU Fallback Mode</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-4-I-O-Formats-on-DLA"><span class="nav-text">12.4. I&#x2F;O Formats on DLA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-5-DLA-Standalone-Mode"><span class="nav-text">12.5. DLA Standalone Mode</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-6-Customizing-DLA-Memory-Pools"><span class="nav-text">12.6. Customizing DLA Memory Pools</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="奔跑的IC"
      src="/images/zyd.gif">
  <p class="site-author-name" itemprop="name">奔跑的IC</p>
  <div class="site-description" itemprop="description">死磕牛角的IT农民工</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">190</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/12-TensorRT%E5%92%8CDLA(Deep_Learning_Accelerator)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zyd.gif">
      <meta itemprop="name" content="奔跑的IC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="奔跑的IC">
      <meta itemprop="description" content="死磕牛角的IT农民工">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="12-TensorRT和DLA(Deep_Learning_Accelerator) | 奔跑的IC">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          12-TensorRT和DLA(Deep_Learning_Accelerator)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-01 18:13:45" itemprop="dateCreated datePublished" datetime="2024-12-01T18:13:45+08:00">2024-12-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TensorRT/" itemprop="url" rel="index"><span itemprop="name">TensorRT</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/" itemprop="url" rel="index"><span itemprop="name">TensorRT中文版开发手册</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="TensorRT和DLA-Deep-Learning-Accelerator"><a href="#TensorRT和DLA-Deep-Learning-Accelerator" class="headerlink" title="TensorRT和DLA(Deep Learning Accelerator)"></a>TensorRT和DLA(Deep Learning Accelerator)</h1><img src="/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/12-TensorRT%E5%92%8CDLA(Deep_Learning_Accelerator)/rdp.jpg" class="">
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/zh-cn/developer-program">点击此处加入NVIDIA开发者计划</a></p>
<p>NVIDIA DLA（Deep Learning Accelerator—深度学习加速器）是一款针对深度学习操作的固定功能加速器引擎。 DLA 旨在对卷积神经网络进行全硬件加速。 DLA支持卷积、反卷积、全连接、激活、池化、批量归一化等各种层，DLA不支持Explicit Quantization 。</p>
<p>有关 TensorRT 层中 DLA 支持的更多信息，请参阅<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#dla_layers">DLA 支持的层</a>。 <code>trtexec</code>工具具有在 DLA 上运行网络的附加参数，请参阅<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#trtexec">trtexec</a> 。</p>
<p><code>trtexec</code>在DLA 上运行 ResNet-50 FP16 网络：</p>
<p><code>./trtexec --onnx=data/resnet50/ResNet50.onnx --useDLACore=0 --fp16 --allowGPUFallback</code></p>
<p><code>trtexec</code>在DLA 上运行 ResNet-50 INT8 网络:</p>
<p><code>./trtexec --onnx=data/resnet50/ResNet50.onnx --useDLACore=0 --int8 --allowGPUFallback</code></p>
<h2 id="12-1-Running-On-DLA-During-TensorRT-Inference"><a href="#12-1-Running-On-DLA-During-TensorRT-Inference" class="headerlink" title="12.1. Running On DLA During TensorRT Inference"></a>12.1. Running On DLA During TensorRT Inference</h2><p>TensorRT 构建器可以配置为在 DLA 上启用推理。 DLA 支持目前仅限于在 <code>FP16</code> 或 <code>INT8</code> 模式下运行的网络。 <code>DeviceType</code>枚举用于指定网络或层在其上执行的设备。 <code>IBuilderConfig</code>类中的以下 API 函数可用于配置网络以使用 DLA：</p>
<p><strong><code>setDeviceType(ILayer* layer, DeviceType deviceType)</code></strong></p>
<p>此函数可用于设置层必须在其上执行的设备类型</p>
<p><strong><code>getDeviceType(const ILayer* layer)</code></strong></p>
<p>此函数可用于返回该层执行的设备类型。如果层在 GPU 上执行，则返回<code>DeviceType::kGPU</code> 。</p>
<p><strong><code>canRunOnDLA(const ILayer* layer)</code></strong></p>
<p>此功能可用于检查层是否可以在 DLA 上运行。</p>
<p><strong><code>setDefaultDeviceType(DeviceType deviceType)</code></strong></p>
<p>此函数设置构建器使用的默认设备类型。它确保可以在 DLA 上运行的所有层都在 DLA 上运行，除非<code>setDeviceType</code>用于覆盖层的<code>deviceType</code> 。</p>
<p><strong><code>getDefaultDeviceType()</code></strong></p>
<p>此函数返回由 <code>setDefaultDeviceType</code> 设置的默认设备类型.</p>
<p><strong><code>isDeviceTypeSet(const ILayer* layer)</code></strong></p>
<p>此函数检查是否已为该层显式设置了<code>deviceType</code> 。</p>
<p><strong><code>resetDeviceType(ILayer* layer)</code></strong></p>
<p>此函数重置此层的<code>deviceType</code> 。如果未指定，该值将重置为由<code>setDefaultDeviceType</code>或<code>DeviceType::kGPU</code>指定的设备类型。</p>
<p><strong><code>allowGPUFallback(bool setFallBackMode)</code></strong></p>
<p>如果应该在 DLA 上运行的层无法在 DLA 上运行，此函数会通知构建器使用 GPU。有关详细信息，请参阅<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#gpu_fallback">GPU 回退模式</a>。</p>
<p><strong><code>reset()</code></strong></p>
<p>此函数可用于重置<code>IBuilderConfig</code>状态，它将所有层的<code>deviceType</code>设置为<code>DeviceType::kGPU</code> 。重置后，构建器可以重新用于构建具有不同 DLA 配置的另一个网络。</p>
<p><code>IBuilder</code>类中的以下 API 函数可用于帮助配置网络以使用 DLA：</p>
<p><strong><code>getMaxDLABatchSize()</code></strong></p>
<p>此函数返回 DLA 可以支持的最大批量大小。</p>
<h4 id="注意：对于任何张量，索引维度的总体积加上请求的批量大小不得超过此函数返回的值。"><a href="#注意：对于任何张量，索引维度的总体积加上请求的批量大小不得超过此函数返回的值。" class="headerlink" title="注意：对于任何张量，索引维度的总体积加上请求的批量大小不得超过此函数返回的值。"></a>注意：对于任何张量，索引维度的总体积加上请求的批量大小不得超过此函数返回的值。</h4><p><strong><code>getNbDLACores()</code></strong></p>
<p>此函数返回用户可用的 DLA 内核数。</p>
<p>如果构建器不可访问，例如在推理应用程序中在线加载计划文件的情况下，则可以通过对 <code>IRuntime</code> 使用 DLA 扩展以不同方式指定要使用的DLA 。 <code>IRuntime</code>类中的以下 API 函数可用于配置网络以使用 DLA：</p>
<p><strong><code>getNbDLACores()</code></strong></p>
<p>此函数返回用户可访问的 DLA 内核数。</p>
<p><strong><code>setDLACore(int dlaCore)</code></strong></p>
<p>要在其上执行的 DLA 内核。其中<code>dlaCore</code>是介于0和<code>getNbDLACores() - 1</code>之间的值。默认值为0 </p>
<p><strong><code>getDLACore()</code></strong></p>
<p>运行时执行分配到的 DLA 核心。默认值为 0。</p>
<h3 id="12-1-1-Example-sampleMNIST-With-DLA"><a href="#12-1-1-Example-sampleMNIST-With-DLA" class="headerlink" title="12.1.1. Example: sampleMNIST With DLA"></a>12.1.1. Example: sampleMNIST With DLA</h3><p>本节提供有关如何在启用 DLA 的情况下运行 TensorRT 示例的详细信息。</p>
<p>位于 GitHub 存储库中的<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/samples/sampleMNIST"><code>sampleMNIST</code></a>演示了如何导入经过训练的模型、构建 TensorRT 引擎、序列化和反序列化引擎，最后使用引擎执行推理。</p>
<p>该示例首先创建构建器：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> builder = <span class="built_in">SampleUniquePtr</span>&lt;nvinfer1::IBuilder&gt;(nvinfer1::<span class="built_in">createInferBuilder</span>(gLogger));</span><br><span class="line"><span class="keyword">if</span> (!builder) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">builder-&gt;<span class="built_in">setMaxBatchSize</span>(batchSize);</span><br></pre></td></tr></table></figure></p>
<p>然后，启用GPUFallback模式：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kGPU_FALLBACK);</span><br><span class="line">config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kFP16); <span class="keyword">or</span> config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kINT8);</span><br></pre></td></tr></table></figure>
<p>在 DLA 上启用执行，其中dlaCore指定要在其上执行的 DLA 内核：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">config-&gt;<span class="built_in">setDefaultDeviceType</span>(DeviceType::kDLA);</span><br><span class="line">config-&gt;<span class="built_in">setDLACore</span>(dlaCore);</span><br></pre></td></tr></table></figure>
<p>通过这些额外的更改，sampleMNIST 已准备好在 DLA 上执行。要使用 DLA Core 1 运行 sampleMNIST，请使用以下命令：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sample_mnist --useDLACore=<span class="number">0</span> [--int8|--fp16]</span><br></pre></td></tr></table></figure>
<h3 id="12-1-2-Example-Enable-DLA-Mode-For-A-Layer-During-Network-Creation"><a href="#12-1-2-Example-Enable-DLA-Mode-For-A-Layer-During-Network-Creation" class="headerlink" title="12.1.2. Example: Enable DLA Mode For A Layer During Network Creation"></a>12.1.2. Example: Enable DLA Mode For A Layer During Network Creation</h3><p>在这个例子中，让我们创建一个包含输入、卷积和输出的简单网络。</p>
<p>1.创建构建器、构建器配置和网络：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">IBuilder* builder = <span class="built_in">createInferBuilder</span>(gLogger);</span><br><span class="line">IBuilderConfig* config = builder.<span class="built_in">createBuilderConfig</span>();</span><br><span class="line">INetworkDefinition* network = builder-&gt;<span class="built_in">createNetworkV2</span>(<span class="number">0U</span>);</span><br></pre></td></tr></table></figure></p>
<p>2.使用输入维度将输入层添加到网络。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> data = network-&gt;<span class="built_in">addInput</span>(INPUT_BLOB_NAME, dt, Dims3&#123;<span class="number">1</span>, INPUT_H, INPUT_W&#125;);</span><br></pre></td></tr></table></figure></p>
<p>3.添加具有隐藏层输入节点、步幅和权重的卷积层以用于卷积核和偏差。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> conv1 = network-&gt;<span class="built_in">addConvolution</span>(*data-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>), <span class="number">20</span>, DimsHW&#123;<span class="number">5</span>, <span class="number">5</span>&#125;, weightMap[<span class="string">&quot;conv1filter&quot;</span>], weightMap[<span class="string">&quot;conv1bias&quot;</span>]);</span><br><span class="line">conv1-&gt;<span class="built_in">setStride</span>(DimsHW&#123;<span class="number">1</span>, <span class="number">1</span>&#125;);</span><br></pre></td></tr></table></figure>
<p>4.将卷积层设置为在 DLA 上运行：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(<span class="built_in">canRunOnDLA</span>(conv1))</span><br><span class="line">&#123;</span><br><span class="line">config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kFP16); <span class="keyword">or</span> config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kINT8);</span><br><span class="line">builder-&gt;<span class="built_in">setDeviceType</span>(conv1, DeviceType::kDLA); </span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>5.标记输出<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">network-&gt;<span class="built_in">markOutput</span>(*conv1-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>));</span><br></pre></td></tr></table></figure></p>
<p>6.将 DLA 内核设置为在以下位置执行：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config-&gt;<span class="built_in">setDLACore</span>(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="12-2-DLA-Supported-Layers"><a href="#12-2-DLA-Supported-Layers" class="headerlink" title="12.2. DLA Supported Layers"></a>12.2. DLA Supported Layers</h2><p>本节列出了 DLA 支持的层以及与每个层相关的约束。</p>
<p><strong>在 DLA 上运行时的一般限制（适用于所有层）</strong></p>
<ul>
<li>支持的最大批量大小为 4096。</li>
<li>DLA 不支持动态尺寸。因此，对于通配符维度，配置文件的<code>min</code> 、 <code>max</code>和<code>opt</code>值必须相等。</li>
<li>如果违反了任何限制，并且启用了<code>GpuFallback</code>, TensorRT可以将DLA网络分成多个部分。否则，TensorRT会发出错误并返回。更多信息，请参考<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#gpu_fallback">GPU回退模式</a>。</li>
<li>由于硬件和软件内存限制，最多可以同时使用四个 DLA 可加载项。</li>
</ul>
<h4 id="注意：-DLA-的批量大小是除CHW维度之外的所有索引维度的乘积。例如，如果输入维度为NPQRS-，则有效批量大小为N-P-。"><a href="#注意：-DLA-的批量大小是除CHW维度之外的所有索引维度的乘积。例如，如果输入维度为NPQRS-，则有效批量大小为N-P-。" class="headerlink" title="注意： DLA 的批量大小是除CHW维度之外的所有索引维度的乘积。例如，如果输入维度为NPQRS ，则有效批量大小为N*P 。"></a>注意： DLA 的批量大小是除CHW维度之外的所有索引维度的乘积。例如，如果输入维度为NPQRS ，则有效批量大小为N*P 。</h4><p><strong>层特定限制</strong></p>
<p>卷积层和全连接层</p>
<ul>
<li>仅支持两个空间维度操作。</li>
<li>支持 FP16 和 INT8。</li>
<li>内核大小的每个维度都必须在[1, 32]范围内。</li>
<li>填充(Padding)必须在[0, 31]范围内。</li>
<li>填充的维度必须小于相应的内核维度。</li>
<li>步幅的尺寸必须在[1, 8]范围内。</li>
<li>输出映射的数量必须在[1, 8192]范围内。</li>
<li>对于使用格式<code>TensorFormat::kLINEAR</code> 、 <code>TensorFormat::kCHW16</code>和<code>TensorFormat::kCHW32</code>的操作，组数必须在[1, 8192]范围内。</li>
<li>对于使用格式<code>TensorFormat::kCHW4</code>的操作，组数必须在[1, 4]范围内。</li>
<li>空洞卷积(Dilated convolution )必须在[1, 32]范围内。</li>
<li>如果 CBUF 大小要求<code>wtBanksForOneKernel + minDataBanks</code>超过<code>numConvBufBankAllotted</code>限制<code>16</code> ，则不支持操作，其中 <code>CBUF</code> 是在对输入权重和激活进行操作之前存储输入权重和激活的内部卷积缓存， <code>wtBanksForOneKernel</code>是一个内核存储最小权重/卷积所需的核元素， <code>minDataBanks</code>是存储卷积所需的最小激活数据的最小库。伪代码细节如下：<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wtBanksForOneKernel = <span class="built_in">uint32</span>(<span class="built_in">ceil</span>(<span class="built_in">roundUp</span>(inputDims_c * kernelSize_h * kernelSize_w * (INT8 ? <span class="number">1</span> : <span class="number">2</span>), <span class="number">128</span>) / <span class="number">32768.0</span>))</span><br><span class="line"></span><br><span class="line">minDataBanks = <span class="built_in">uint32</span>(<span class="built_in">ceil</span>(<span class="built_in">float</span>(entriesPerDataSlice * dilatedKernelHt) / <span class="number">256.0</span>)) where entriesPerDataSlice = <span class="built_in">uint32</span>(<span class="built_in">ceil</span>(<span class="built_in">ceil</span>(inputDims_c * (INT8 ? <span class="number">1</span> : <span class="number">2</span>) / <span class="number">32.0</span>) * inputDims_w / <span class="number">4.0</span>)) <span class="keyword">and</span> dilatedKernelHt = (kernelSize_h - <span class="number">1</span>) * dilation_h + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">FAIL <span class="keyword">if</span> wtBanksForOneKernel + minDataBanks &gt; <span class="number">16</span>, PASS otherwise.</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>反卷积层</strong></p>
<ul>
<li>仅支持两个空间维度操作。</li>
<li>支持 <code>FP16</code> 和 <code>INT8</code>。</li>
<li>除了<code>1x[64, 96, 128]</code>和<code>[64, 96, 128]x1</code>之外，内核的尺寸必须在<code>[1, 32]</code>范围内。</li>
<li>TensorRT 在 DLA 上禁用了反卷积平方内核并在<code>[23 - 32]</code>范围内跨步，因为它们显着减慢了编译速度。</li>
<li>填充(Padding)必须为0 。</li>
<li>分组反卷积必须为1 。</li>
<li>扩张反卷积必须为1 。</li>
<li>输入通道数必须在[1, 8192]范围内。</li>
<li>输出通道数必须在[1, 8192]范围内。</li>
</ul>
<p><strong>池化层</strong></p>
<ul>
<li>仅支持两个空间维度操作。</li>
<li>支持 <code>FP16</code> 和 <code>INT8</code>。</li>
<li>支持的操作： <code>kMAX</code> ， <code>kAVERAGE</code> 。</li>
<li>窗口的尺寸必须在<code>[1, 8]</code>范围内。</li>
<li>填充的尺寸必须在<code>[0, 7]</code>范围内。</li>
<li>步幅的尺寸必须在[1, 16]范围内。</li>
<li>使用 <code>INT8</code> 模式，输入和输出张量标度必须相同。</li>
</ul>
<p><strong>激活层</strong></p>
<ul>
<li>仅支持两个空间维度操作。</li>
<li>支持 <code>FP16</code> 和 <code>INT8</code>。</li>
<li>支持的函数： <code>ReLU</code> 、 <code>Sigmoid</code> 、 <code>TanH</code> 、 <code>Clipped ReLU</code>和<code>Leaky ReLU</code> 。<ul>
<li>ReLU不支持负斜率。</li>
<li>Clipped ReLU仅支持[1, 127]范围内的值。</li>
<li>TanH , Sigmoid INT8 支持通过自动升级到 FP16 来支持。</li>
</ul>
</li>
</ul>
<p><strong>参数 ReLU 层</strong></p>
<ul>
<li>斜率输入必须是构建时间常数。</li>
</ul>
<p><strong>ElementWise 层</strong></p>
<ul>
<li>仅支持两个空间维度操作。</li>
<li>支持 <code>FP16</code> 和 <code>INT8</code>。</li>
<li>支持的操作： <code>Sum</code> 、 <code>Sub</code> 、 <code>Product</code> 、 <code>Max</code>和<code>Min</code> 。</li>
<li><h4 id="注意：在-Xavier-上，TensorRT-将-DLA-Scale-层和-DLA-ElementWise-层与操作Sum连接以支持Sub操作，单个-Xavier-DLA-ElementWise-层不支持。"><a href="#注意：在-Xavier-上，TensorRT-将-DLA-Scale-层和-DLA-ElementWise-层与操作Sum连接以支持Sub操作，单个-Xavier-DLA-ElementWise-层不支持。" class="headerlink" title="注意：在 Xavier 上，TensorRT 将 DLA Scale 层和 DLA ElementWise 层与操作Sum连接以支持Sub操作，单个 Xavier DLA ElementWise 层不支持。"></a>注意：在 Xavier 上，TensorRT 将 DLA Scale 层和 DLA ElementWise 层与操作Sum连接以支持Sub操作，单个 Xavier DLA ElementWise 层不支持。</h4></li>
</ul>
<p><strong>Scale层</strong></p>
<ul>
<li>仅支持两个空间维度操作。</li>
<li>支持 <code>FP16</code> 和 <code>INT8</code>。</li>
<li>支持的模式： <code>Uniform</code> 、 <code>Per-Channel</code>和<code>ElementWise</code> 。</li>
<li>仅支持缩放和移位操作。</li>
</ul>
<p><strong>LRN（局部响应归一化）层</strong></p>
<ul>
<li>允许的窗口大小为3 、 5 、 7或9 。</li>
<li>支持的规范化区域是ACROSS_CHANNELS 。</li>
<li>LRN INT8。</li>
</ul>
<p><strong>连接层</strong></p>
<ul>
<li>DLA 仅支持沿通道轴连接。</li>
<li>Concat 必须至少有两个输入。</li>
<li>所有输入必须具有相同的空间维度。</li>
<li>对于 INT8 模式，所有输入的动态范围必须相同。</li>
<li>对于 INT8 模式，输出的动态范围必须等于每个输入。</li>
</ul>
<p><strong>Resize层</strong></p>
<ul>
<li>刻度的数量必须正好是4 。</li>
<li>scale 中的前两个元素必须正好为1 （对于未更改的批次和通道尺寸）。</li>
<li>scale 中的最后两个元素，分别表示沿高度和宽度维度的比例值，在最近邻模式下需要为[1, 32]范围内的整数值，在双线性模式下需要为[1, 4]范围内的整数值。</li>
</ul>
<p><strong>Unary 层</strong></p>
<ul>
<li>仅支持 ABS 操作。</li>
</ul>
<p><strong>Softmax 层</strong></p>
<ul>
<li>仅支持 NVIDIA Orin™，不支持 Xavier™。</li>
<li>仅支持批量大小为 1 的单个输入。</li>
<li>输入的非批量、非轴维度都应该是大小 1。例如，对于轴 = 1 的 softmax（即在 C 维度上），H 和 W 维度的大小都应该是 1。</li>
</ul>
<h4 id="注意：当使用-TensorRT-在-DLA-上运行-INT8-网络时，建议将操作添加到同一子图中，以通过允许它们融合并为中间结果保留更高的精度来减少在-DLA-上运行的网络的子图上的量化误差。通过将张量设置为网络输出张量来拆分子图以检查中间结果可能会由于禁用这些优化而导致不同级别的量化误差。"><a href="#注意：当使用-TensorRT-在-DLA-上运行-INT8-网络时，建议将操作添加到同一子图中，以通过允许它们融合并为中间结果保留更高的精度来减少在-DLA-上运行的网络的子图上的量化误差。通过将张量设置为网络输出张量来拆分子图以检查中间结果可能会由于禁用这些优化而导致不同级别的量化误差。" class="headerlink" title="注意：当使用 TensorRT 在 DLA 上运行 INT8 网络时，建议将操作添加到同一子图中，以通过允许它们融合并为中间结果保留更高的精度来减少在 DLA 上运行的网络的子图上的量化误差。通过将张量设置为网络输出张量来拆分子图以检查中间结果可能会由于禁用这些优化而导致不同级别的量化误差。"></a>注意：当使用 TensorRT 在 DLA 上运行 INT8 网络时，建议将操作添加到同一子图中，以通过允许它们融合并为中间结果保留更高的精度来减少在 DLA 上运行的网络的子图上的量化误差。通过将张量设置为网络输出张量来拆分子图以检查中间结果可能会由于禁用这些优化而导致不同级别的量化误差。</h4><h2 id="12-3-GPU-Fallback-Mode"><a href="#12-3-GPU-Fallback-Mode" class="headerlink" title="12.3. GPU Fallback Mode"></a>12.3. GPU Fallback Mode</h2><p>如果被标记为在DLA上运行的层不能在DLA上运行，则<code>GPUFallbackMode</code>设置生成器使用GPU。</p>
<p>由于以下原因，层无法在 DLA 上运行：</p>
<ol>
<li>DLA 不支持层操作。</li>
<li>指定的参数超出了 DLA 支持的范围。</li>
<li>给定的批量大小超过了允许的最大 DLA 批量大小。有关详细信息，请参阅<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#dla_layers">DLA 支持的层</a>。</li>
<li>网络中的层组合导致内部状态超过 DLA 能够支持的状态。</li>
<li>平台上没有可用的 DLA 引擎。</li>
</ol>
<p>如果<code>GPUFallbackMode</code>设置为<code>false</code> ，则设置为在 DLA 上执行但无法在 DLA 上运行的层会导致错误。但是，将<code>GPUFallbackMode</code>设置为<code>true</code>后，它会在报告警告后继续在 GPU 上执行。</p>
<p>同样，如果<code>defaultDeviceType</code>设置为<code>DeviceType::kDLA</code>并且<code>GPUFallbackMode</code>设置为<code>false</code> ，则如果任何层无法在 DLA 上运行，则会导致错误。将<code>GPUFallbackMode</code>设置为<code>true</code>时，它会报告警告并继续在 GPU 上执行。</p>
<p>如果网络中的层组合无法在 DLA 上运行，则组合中的所有层都在 GPU 上执行。</p>
<h2 id="12-4-I-O-Formats-on-DLA"><a href="#12-4-I-O-Formats-on-DLA" class="headerlink" title="12.4. I/O Formats on DLA"></a>12.4. I/O Formats on DLA</h2><p>DLA 支持设备独有的格式，并且由于矢量宽度字节要求而对其布局有限制。</p>
<p>对于 DLA 输入，支持 <code>kDLA_LINEAR ( FP16 , INT8 )</code>、 <code>kDLA_HWC4 ( FP16 , INT8 )</code>、 <code>kCHW16 ( FP16 )</code> 和<code>kCHW32 ( INT8 )</code>。对于 DLA 输出，仅支持 <code>kDLA_LINEAR ( FP16 , INT8 )</code>、 <code>kCHW16 ( FP16 )</code> 和<code>kCHW32 ( INT8 )</code>。对于<code>kCHW16</code>和<code>kCHW32</code>格式，如果C不是整数倍，则必须将其填充到下一个 32 字节边界。</p>
<p>对于<code>kDLA_LINEAR</code>格式，沿W维度的步幅必须最多填充 64 个字节。内存格式等效于维度为<code>[N][C][H][roundUp(W, 64/elementSize)]</code>的C数组，其中<code>FP16</code>的<code>elementSize</code>为 2， <code>Int8</code>为 1 ，张量坐标为<code>(n, c, h, w)</code>映射到数组下标<code>[n][c][h][w]</code> 。</p>
<p>对于<code>kDLA_HWC4</code>格式，沿<code>W</code>维度的步幅必须是 Xavier 上 32 字节和 Orin 上 64 字节的倍数。</p>
<ul>
<li>当<code>C == 1</code>时，TensorRT 将格式映射到本机灰度图像格式。</li>
<li><p>当<code>C == 3</code>或<code>C == 4</code>时，它映射到本机彩色图像格式。如果<code>C == 3</code> ，沿<code>W</code>轴步进的步幅需要填充为 4 个元素。</p>
<p>  在这种情况下，填充通道位于第 4 个索引处。理想情况下，填充值无关紧要，因为权重中的第 4 个通道被 DLA 编译器填充为零；但是，应用程序分配四个通道的零填充缓冲区并填充三个有效通道是安全的。</p>
</li>
<li>当<code>C</code>为<code>&#123;1, 3, 4&#125;</code>时，填充后的 <code>C&#39;</code>分别为<code>&#123;1, 4, 4&#125;</code> ，内存布局等价于维度为<code>[N][H][roundUp(W, 32/C&#39;/elementSize)][C&#39;]</code>的C数组, 其中<code>elementSize</code>对于<code>FP16</code>为 2，对于<code>Int8</code>为 1 。张量坐标<code>(n, c, h, w)</code>映射到数组下标<code>[n][h][w][c]</code> ， <code>roundUp</code>计算大于或等于<code>W</code>的<code>64/elementSize</code>的最小倍数。</li>
</ul>
<p>使用<code>kDLA_HWC4</code>作为 DLA 输入格式时，有以下要求：</p>
<ul>
<li>C必须是1 、 3或4</li>
<li>第一层必须是卷积。</li>
<li>卷积参数必须满足 DLA 要求，请参阅D<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#dla_layers">LA Supported Layers</a> 。</li>
</ul>
<p>当<code>EngineCapability</code>为<code>EngineCapability::kSTANDARD</code>且 TensorRT 无法为给定的输入/输出格式生成无重构网络时，可以自动将不支持的 DLA 格式转换为支持的 DLA 格式。例如，如果连接到网络输入或输出的层不能在 DLA 上运行，或者如果网络不满足其他 DLA 要求，则插入重新格式化操作以满足约束。在所有情况下，TensorRT 期望数据格式化的步幅都可以通过查询<code>IExecutionContext::getStrides</code>来获得。</p>
<h2 id="12-5-DLA-Standalone-Mode"><a href="#12-5-DLA-Standalone-Mode" class="headerlink" title="12.5. DLA Standalone Mode"></a>12.5. DLA Standalone Mode</h2><p>如果您使用单独的 DLA 运行时组件，则可以使用<code>EngineCapability::kDLA_STANDALONE</code>生成 DLA 可加载项。请参阅相关 DLA 运行时组件的文档以了解如何使用可加载项。</p>
<p>当使用<code>kDLA_STANDALONE</code>时，TensorRT 为给定的输入/输出格式生成一个无重新格式化的网络。对于 DLA 输入，支持 <code>kLINEAR ( FP16 , INT8 )</code>、 <code>kCHW4 ( FP16 , INT8 )</code>、 <code>kCHW16 ( FP16 )</code> 和<code>kCHW32 ( INT8 )</code>。而对于 DLA 输出，仅支持 <code>kLINEAR ( FP16 , INT8 )</code>、 <code>kCHW16 ( FP16 )</code> 和<code>kCHW32 ( INT8 )</code>。对于<code>kCHW16</code>和<code>kCHW32</code>格式，建议C通道数等于向量大小的正整数倍。如果C不是整数倍，则必须将其填充到下一个 32 字节边界。</p>
<h2 id="12-6-Customizing-DLA-Memory-Pools"><a href="#12-6-Customizing-DLA-Memory-Pools" class="headerlink" title="12.6. Customizing DLA Memory Pools"></a>12.6. Customizing DLA Memory Pools</h2><p>您可以自定义分配给网络中每个可加载的 DLA 的内存池的大小。共有三种类型的 DLA 内存池（有关每个池的描述，请参见枚举类 <code>MemoryPoolType</code> ）：</p>
<ul>
<li>Managed SRAM</li>
<li>Local DRAM</li>
<li>Global DRAM</li>
</ul>
<p>对于每种池类型，使用 API <code>IBuilderConfig::setMemoryPoolLimit</code>和<code>IBuilderConfig::getMemoryPoolLimit</code>来设置和查询相关池的大小，以便为每个可加载的 DLA 分配更大的内存池。每个可加载的实际需要的内存量可能小于池大小，在这种情况下将分配较小的量。池大小仅用作上限。</p>
<p>请注意，所有 DLA 内存池都需要大小为 2 的幂，最小为 4 KiB。违反此要求会导致 DLA 可加载编译失败。</p>
<p>Managed SRAM 与其他 DRAM 池的区别主要在于角色的不同。以下是Managed SRAM 的一些值得注意的方面：</p>
<ul>
<li>它类似于缓存，因为资源稀缺，DLA 可以通过回退到本地 DRAM 来运行而无需它。</li>
<li>任何分配往往都会被充分利用。因此，报告的 SRAM 通常与分配的 SRAM 池的数量相同（在某些情况下可能与用户指定的大小不同）。</li>
<li>由于类似于缓存的性质，DLA 在 SRAM 不足时会回退到 DRAM，而不是失败。因此，如果可以负担得起，即使在成功的引擎构建之后也尝试增加 SRAM 的数量，以查看推理速度是否有任何提升。这尤其适用于卸载许多子图的网络。</li>
<li>Orin 和 Xavier 在每个内核可用的最大 SRAM 数量方面存在差异：Xavier 在 4 个内核（包括 2 个 DLA 内核）中提供总共 4 MiB 的 SRAM，而 Orin 为每个 DLA 内核专用 1 MiB SRAM。这意味着当在一个设备上运行多个网络时，Xavier 需要明确控制总体 SRAM 消耗，而 Orin 在这方面不必担心。</li>
</ul>
<p>在多子图情况下，重要的是要记住池大小适用于每个 DLA 子图，而不是整个网络。</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>奔跑的IC
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://example.com/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/12-TensorRT%E5%92%8CDLA(Deep_Learning_Accelerator)/" title="12-TensorRT和DLA(Deep_Learning_Accelerator)">http://example.com/TensorRT/TensorRT中文版开发手册/12-TensorRT和DLA(Deep_Learning_Accelerator)/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/git/" rel="tag"><i class="fa fa-tag"></i> git</a>
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C</a>
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C++</a>
              <a href="/tags/GPU/" rel="tag"><i class="fa fa-tag"></i> GPU</a>
              <a href="/tags/Tensorrt/" rel="tag"><i class="fa fa-tag"></i> Tensorrt</a>
              <a href="/tags/DeepLearning/" rel="tag"><i class="fa fa-tag"></i> DeepLearning</a>
              <a href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> 卷积神经网络</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/11-TensorRT%E4%B9%8B%E4%BD%BF%E7%94%A8%E6%9D%A1%E4%BB%B6/" rel="prev" title="11-TensorRT之使用条件">
                  <i class="fa fa-chevron-left"></i> 11-TensorRT之使用条件
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/13-TensorRT%E7%9A%84%E6%9C%80%E4%BD%B3%E6%80%A7%E8%83%BD%E5%AE%9E%E8%B7%B5/" rel="next" title="13-TensorRT的最佳性能实践">
                  13-TensorRT的最佳性能实践 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">奔跑的IC</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"zmurder","repo":"zmurder.github.io","client_id":"cf2343f27b6c29efe0bc","client_secret":"3268a1fa92706c7358d5421f88f76a0f7ada3188","admin_user":"zmurder","distraction_free_mode":true,"proxy":"https://strong-caramel-969805.netlify.app/github_access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"10f642af92f51c4f8fab5943f2204b4c"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
