<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
  <meta name="msvalidate.01" content="7EC20DBC74B004C2782077570E15C280">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
    <meta name="description" content="官方例子NVIDIA TensorRT嵌入式应用程序的其他示例。here.">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorRT例子">
<meta property="og:url" content="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/index.html">
<meta property="og:site_name" content="奔跑的IC">
<meta property="og:description" content="官方例子NVIDIA TensorRT嵌入式应用程序的其他示例。here.">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906102957936.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906103138934.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906103614348.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906104749994.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220907145636640.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906104642434.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906134331135.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906141106404.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220907153229288.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220907154218486.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220914131347502.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220919202315669.png">
<meta property="article:published_time" content="2024-12-01T10:13:45.403Z">
<meta property="article:modified_time" content="2024-12-01T10:13:45.403Z">
<meta property="article:author" content="奔跑的IC">
<meta property="article:tag" content="git">
<meta property="article:tag" content="C">
<meta property="article:tag" content="C++">
<meta property="article:tag" content="GPU">
<meta property="article:tag" content="Tensorrt">
<meta property="article:tag" content="DeepLearning">
<meta property="article:tag" content="Plugin">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906102957936.png">


<link rel="canonical" href="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/","path":"TensorRT/TensorRT例子/","title":"TensorRT例子"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>TensorRT例子 | 奔跑的IC</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">奔跑的IC</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-reorder fa-fw"></i>文章列表</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%98%E6%96%B9%E4%BE%8B%E5%AD%90"><span class="nav-text">官方例子</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E7%AE%80%E4%BB%8B"><span class="nav-text">1 简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-c-%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-text">1.1 c++的例子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-Python%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="nav-text">1.2 Python的例子</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E4%BE%8B%E5%AD%90"><span class="nav-text">2 交叉编译例子</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E5%85%88%E5%86%B3%E6%9D%A1%E4%BB%B6"><span class="nav-text">2.1. 先决条件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E4%B8%BAQNX-AArch64%E7%BC%96%E8%AF%91%E4%BE%8B%E5%AD%90"><span class="nav-text">2.2 为QNX AArch64编译例子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-%E4%B8%BALinux-AArch64%E7%BC%96%E8%AF%91%E4%BE%8B%E5%AD%90"><span class="nav-text">2.3 为Linux AArch64编译例子</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E4%BD%BF%E7%94%A8%E9%9D%99%E6%80%81%E5%BA%93%E6%9E%84%E5%BB%BA%E7%A4%BA%E4%BE%8B-TODO"><span class="nav-text">3 使用静态库构建示例(TODO)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BE%8B%E5%AD%901-sampleMNIST"><span class="nav-text">例子1 sampleMNIST</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-text">简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C"><span class="nav-text">示例如何工作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensorrt-API%E5%B1%82%E5%92%8COPS"><span class="nav-text">Tensorrt API层和OPS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%86%E5%A4%87%E7%A4%BA%E4%BE%8B%E6%95%B0%E6%8D%AE"><span class="nav-text">准备示例数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C%E7%A4%BA%E4%BE%8B"><span class="nav-text">运行示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sampleMNIST%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="nav-text">sampleMNIST代码解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E5%87%BD%E6%95%B0"><span class="nav-text">主函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-text">参数初始化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Build"><span class="nav-text">Build</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Infer"><span class="nav-text">Infer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#zyd%E4%BF%AE%E6%94%B9%E6%B5%8B%E8%AF%95"><span class="nav-text">zyd修改测试</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BE%8B%E5%AD%902-sampleMNISTAPI"><span class="nav-text">例子2 sampleMNISTAPI</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#sampleMNISTAPI%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="nav-text">sampleMNISTAPI代码解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E5%87%BD%E6%95%B0-1"><span class="nav-text">主函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Build-1"><span class="nav-text">Build</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E6%9E%90-mnistapi-wts-%E6%96%87%E4%BB%B6"><span class="nav-text">解析 mnistapi.wts 文件</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BE%8B%E5%AD%903-sampleOnnxMNIST"><span class="nav-text">例子3 sampleOnnxMNIST</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="nav-text">代码解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#build"><span class="nav-text">build</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%9A%E5%AE%A2%E6%80%BB%E7%BB%93"><span class="nav-text">博客总结</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-TRT%E6%B5%81%E7%A8%8B"><span class="nav-text">1 TRT流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B-%E5%92%8C-engine"><span class="nav-text">构建模型 和 engine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Engine-%E5%BA%8F%E5%88%97%E5%8C%96%E5%92%8C%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96"><span class="nav-text">Engine 序列化和反序列化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-engine-%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B"><span class="nav-text">使用 engine 进行预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E9%A2%84%E6%B5%8B"><span class="nav-text">前向预测</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84TensorRT%E4%BE%8B%E5%AD%90"><span class="nav-text">2 最简单的TensorRT例子</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E6%9E%90"><span class="nav-text">分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-text">代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TIPS"><span class="nav-text">TIPS:</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#saveEngine"><span class="nav-text">saveEngine</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#loadEngine"><span class="nav-text">loadEngine</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#caffemodel"><span class="nav-text">caffemodel</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-text">附录</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BC%96%E5%86%99"><span class="nav-text">编写</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="奔跑的IC"
      src="/images/zyd.gif">
  <p class="site-author-name" itemprop="name">奔跑的IC</p>
  <div class="site-description" itemprop="description">死磕牛角的IT农民工</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">170</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zyd.gif">
      <meta itemprop="name" content="奔跑的IC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="奔跑的IC">
      <meta itemprop="description" content="死磕牛角的IT农民工">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="TensorRT例子 | 奔跑的IC">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          TensorRT例子
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-01 18:13:45" itemprop="dateCreated datePublished" datetime="2024-12-01T18:13:45+08:00">2024-12-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TensorRT/" itemprop="url" rel="index"><span itemprop="name">TensorRT</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="官方例子"><a href="#官方例子" class="headerlink" title="官方例子"></a>官方例子</h1><p>NVIDIA TensorRT嵌入式应用程序的其他示例。<a target="_blank" rel="noopener" href="https://github.com/dusty-nv/jetson-inference">here</a>. </p>
<p>NVIDIA TensorRT 8.4.3示例的概述。</p>
<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p>下面的示例展示了如何在许多用例中使用NVIDIA TensorRT，同时突出显示了该接口的不同功能。</p>
<h2 id="1-1-c-的例子"><a href="#1-1-c-的例子" class="headerlink" title="1.1 c++的例子"></a>1.1 c++的例子</h2><p>可以在<code>/usr/src/tensorrt/samples</code>中或者<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/samples">GitHub</a>找到c++的例子。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#mnist_sample">“Hello World” For TensorRT</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#mnistapi_sample">Building A Simple MNIST Network Layer By Layer</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#mnist_uff_sample">Importing The TensorFlow Model And Running Inference</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#onnx_mnist_sample">“Hello World” For TensorRT From ONNX</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#googlenet_sample">Building And Running GoogleNet In TensorRT</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#charRNN_sample">Building An RNN Network Layer By Layer</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#int8_sample">Performing Inference In INT8 Using Custom Calibration</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#int8_api_sample">Performing Inference In INT8 Precision</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#fasterrcnn_sample">Object Detection With Faster R-CNN</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#uffssd_sample">Object Detection With A TensorFlow SSD Network</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#sample_ssd">Object Detection With SSD</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#samplereformatfreeio">Specifying I/O Formats</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#sampleUffPluginV2Ext">Adding A Custom Layer That Supports INT8 I/O To Your Network In TensorRT</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#sample-dynamic-reshape">Digit Recognition With Dynamic Shapes In TensorRT</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#sampleuffmaskrcnn">Object Detection And Instance Segmentation With A TensorFlow Mask R-CNN Network</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#sampleufffasterrcnn">Object Detection With A TensorFlow Faster R-CNN Network</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#samplealgorithmselection">Algorithm Selection API Usage Example Based On sampleMNIST In TensorRT</a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#fntarg_1">1</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#sample_onnx_mnist_coordconvac">Implementing CoordConv in TensorRT with a custom plugin using sampleOnnxMnistCoordConvAC In TensorRT</a></li>
</ul>
<p>每个c++示例都包含一个README.md文件，它提供了关于示例如何工作的详细信息，示例代码，以及如何运行和验证其输出的分步说明。</p>
<p>在Linux上运行c++的例子</p>
<p>如果您使用debian文件安装了TensorRT，请在构建C ++示例之前先将/usr/src/tensorrt复制到新目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> &lt;samples_dir&gt;</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">make -j4</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> ../bin</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">./&lt;sample_bin&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="1-2-Python的例子"><a href="#1-2-Python的例子" class="headerlink" title="1.2 Python的例子"></a>1.2 Python的例子</h2><p>python的例子在                              <code>/usr/src/tensorrt/samples/python</code></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#introductory_parser_samples">Introduction To Importing Caffe, TensorFlow And ONNX Models Into TensorRT Using Python</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#end_to_end_tensorflow_mnist">“Hello World” For TensorRT Using TensorFlow And Python</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#network_api_pytorch_mnist">“Hello World” For TensorRT Using PyTorch And Python</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#uff_custom_plugin">Adding A Custom Layer To Your TensorFlow Network In TensorRT In Python</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#yolov3_onnx">Object Detection With The ONNX TensorRT Backend In Python</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#uff_ssd">Object Detection With SSD In Python</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#int8_caffe_mnist">INT8 Calibration In Python</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#engine_refit_mnist">Refitting An Engine In Python</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#onnx_packnet">TensorRT Inference Of ONNX Models With Custom Layers In Python</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#engine_refit_onnx_bidaf">Refitting An Engine Built From An ONNX Model In Python</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#efficientdet-sample">Scalable And Efficient Object Detection With EfficientDet Networks In Python</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#efficientnet-sample">Scalable And Efficient Image Classification With EfficientNet Networks In Python</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#tensorflow_object_detection_api">Object Detection with TensorFlow Object Detection API Model Zoo Networks in Python</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#detectron2">Object Detection with Detectron 2 Mask R-CNN R50-FPN 3x Network in Python</a></li>
</ul>
<p>每一个例子都有README.md文件</p>
<p>运行一个python例子</p>
<ul>
<li>安装示例需求 其中python<x>为python2或python3。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python&lt;x&gt; -m pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
<ul>
<li>如果TensorRT示例数据不在默认位置，则使用提供的数据目录运行示例代码。例如</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python&lt;x&gt; sample.py [-d DATA_DIR]</span><br></pre></td></tr></table></figure>
<h1 id="2-交叉编译例子"><a href="#2-交叉编译例子" class="headerlink" title="2 交叉编译例子"></a>2 交叉编译例子</h1><p>以下各节显示了如何在X86_64 Linux下为AARCH64 QNX和Linux平台进行交叉编译.</p>
<h2 id="2-1-先决条件"><a href="#2-1-先决条件" class="headerlink" title="2.1. 先决条件"></a>2.1. 先决条件</h2><ol>
<li><p>为相应的目标安装CUDA跨平台工具包，并设置环境变量CUDA_INSTALL_DIR。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">export</span> CUDA_INSTALL_DIR=<span class="string">&quot;your cuda install dir&quot;</span></span></span><br></pre></td></tr></table></figure>
<p>默认情况下，CUDA_INSTALL_DIR设置为/usr/local/cuda。</p>
</li>
<li><p>为相应的目标安装CUDNN跨平台库，并设置环境变量CUDNN_INSTALL_DIR。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">export</span> CUDNN_INSTALL_DIR=<span class="string">&quot;your cudnn install dir&quot;</span></span></span><br></pre></td></tr></table></figure>
<p>默认情况下，CUDNN_INSTALL_DIR设置为CUDA_INSTALL_DIR。</p>
</li>
<li><p>为相应的目标安装Tensorrt交叉兼容debian软件包。</p>
<ul>
<li>QNX AArch64<ul>
<li>tensorrt-cross-qnx-dev                                                                     </li>
</ul>
</li>
<li>Linux AArch64<ul>
<li>tensorrt-cross-aarch64-dev                                                                     </li>
</ul>
</li>
<li>Linux SBSA<ul>
<li>tensorrt-cross-sbsa-dev                 </li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="2-2-为QNX-AArch64编译例子"><a href="#2-2-为QNX-AArch64编译例子" class="headerlink" title="2.2 为QNX AArch64编译例子"></a>2.2 为QNX AArch64编译例子</h2><ol>
<li><p>下载QNX工具链并导出以下环境变量。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">export</span> QNX_HOST=/path/to/your/qnx/toolchains/host/linux/x86_64</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">export</span> QNX_TARGET=/path/to/your/qnx/toolchain/target/qnx7</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>使用下面命令构建</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> /path/to/TensorRT/samples</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">make TARGET=qnx</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="2-3-为Linux-AArch64编译例子"><a href="#2-3-为Linux-AArch64编译例子" class="headerlink" title="2.3 为Linux AArch64编译例子"></a>2.3 为Linux AArch64编译例子</h2><ol>
<li><p>安装相应的GCC编译器aarch64-linux-gnu-g++。在Ubuntu中，可以通过：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sudo apt-get install g++-aarch64-linux-gnu</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>使用下面命令构建</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cd</span> /path/to/TensorRT/samples</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">make TARGET=aarch64</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="3-使用静态库构建示例-TODO"><a href="#3-使用静态库构建示例-TODO" class="headerlink" title="3 使用静态库构建示例(TODO)"></a>3 使用静态库构建示例(TODO)</h1><h1 id="例子1-sampleMNIST"><a href="#例子1-sampleMNIST" class="headerlink" title="例子1 sampleMNIST"></a>例子1 sampleMNIST</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>这是一个c++的例子，执行TensorRT的基本设置和初始化使用Caffe解析器。位于samples/sampleMNIST文件夹。</p>
<h3 id="示例如何工作"><a href="#示例如何工作" class="headerlink" title="示例如何工作"></a>示例如何工作</h3><p>该示例使用了在MNIST数据集上训练的Caffe模型。</p>
<ul>
<li>使用Caffe解析器执行TensorRT的基本设置和初始化</li>
<li>使用Caffe解析器导入一个训练好的Caffe模型</li>
<li>对输入进行预处理，并将结果存储在托管的缓冲区中</li>
<li>构建一个engine</li>
<li>序列化和反序列化engine</li>
<li>使用引擎对输入图像执行推理</li>
</ul>
<p>为了验证引擎是否正确运行，该示例随机选择了28x28的数字图像，并使用其创建的引擎对其进行推断。网络的输出是数字上的概率分布，显示图像中的哪个数字可能是该数字。</p>
<h3 id="Tensorrt-API层和OPS"><a href="#Tensorrt-API层和OPS" class="headerlink" title="Tensorrt API层和OPS"></a>Tensorrt API层和OPS</h3><p>在本示例中，将使用以下层，层的详细信息<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#layers">TensorRT Developer Guide: Layers</a> </p>
<p>Activation layer</p>
<p>Convolution layer</p>
<p>FullyConnected layer </p>
<p>Pooling layer</p>
<p>Scale layer</p>
<p>SoftMax layer </p>
<h3 id="准备示例数据"><a href="#准备示例数据" class="headerlink" title="准备示例数据"></a>准备示例数据</h3><p>数据已经在/usr/src/tensorrt/data中，设置环境变量$TRT_DATADIR</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export TRT_DATADIR=/usr/src/tensorrt/data</span><br><span class="line">pushd $TRT_DATADIR/mnist</span><br><span class="line">pip3 install Pillow</span><br><span class="line">popd</span><br></pre></td></tr></table></figure>
<h3 id="运行示例"><a href="#运行示例" class="headerlink" title="运行示例"></a>运行示例</h3><ol>
<li><p>编译</p>
<p>进入例子的文件夹，我这里是拷贝了一份到<code>/home/huolin/WorkSpace/zyd/test/tensorrt/samples/sampleMNIST</code></p>
<img src="/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906102957936.png" class="" title="image-20220906102957936">
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">make -j8</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>会在../../bin下生成对应的可执行程序</p>
<img src="/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906103138934.png" class="" title="image-20220906103138934">
</li>
<li><p>运行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./sample_mnist [-h] [--datadir=/path/to/data/dir/] [--useDLA=N] [--fp16 or --int8]</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">例如</span></span><br><span class="line">./sample_mnist --datadir $TRT_DATADIR/mnist --fp16</span><br></pre></td></tr></table></figure>
<p>该示例读取三个Caffe文件以构建网络：</p>
<ul>
<li><code>mnist.prototxt</code> 包含网络设计的prototxt文件。</li>
<li><code>mnist.caffemodel</code>包含训练好的网络权值的模型文件</li>
<li><code>mnist_mean.binaryproto</code>包含均值的二进制文件。</li>
</ul>
<p>该示例也可以在FP16和INT8模式下运行。</p>
<p>注意：默认情况下，示例期望这些文件在data/samples/mnist/或data/mnist/目录中。可以通过以—datadir=/new/path/为命令行参数添加一个或多个路径来更改默认目录的列表。</p>
</li>
<li><p>验证示例是否成功运行。如果示例成功运行，您应该看到类似如下的输出;输入图像的ASCII显示数字3</p>
<img src="/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906103614348.png" class="" title="image-20220906103614348">
</li>
</ol>
<p>数据及网络结构在<code>/home/huolin/WorkSpace/zyd/test/tensorrt/data/mnist</code>文件夹下</p>
<img src="/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906104749994.png" class="" title="image-20220906104749994">
<p>使用netron查看（在浏览器中打开网址 <a target="_blank" rel="noopener" href="https://netron.app/">https://netron.app/</a> 在ubuntu中使用命令<code>snap install netron</code>安装）</p>
<p>mnist.prototxt结构如下</p>
<img src="/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220907145636640.png" class="" title="image-20220907145636640">
<p>mnist.onnx的网络结构如下：</p>
<img src="/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906104642434.png" class="" title="image-20220906104642434">
<h2 id="sampleMNIST代码解析"><a href="#sampleMNIST代码解析" class="headerlink" title="sampleMNIST代码解析"></a>sampleMNIST代码解析</h2><p>TensorRT的使用包括两个阶段，Build和Deployment。</p>
<h3 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 参数解析</span></span><br><span class="line">    samplesCommon::Args args;</span><br><span class="line">    <span class="type">bool</span> argsOK = samplesCommon::<span class="built_in">parseArgs</span>(args, argc, argv);</span><br><span class="line">    <span class="keyword">if</span> (!argsOK)</span><br><span class="line">    &#123;</span><br><span class="line">        sample::gLogError &lt;&lt; <span class="string">&quot;Invalid arguments&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">printHelpInfo</span>();</span><br><span class="line">        <span class="keyword">return</span> EXIT_FAILURE;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 打印帮助信息</span></span><br><span class="line">    <span class="keyword">if</span> (args.help)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printHelpInfo</span>();</span><br><span class="line">        <span class="keyword">return</span> EXIT_SUCCESS;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> sampleTest = sample::gLogger.<span class="built_in">defineTest</span>(gSampleName, argc, argv);</span><br><span class="line"></span><br><span class="line">    sample::gLogger.<span class="built_in">reportTestStart</span>(sampleTest);</span><br><span class="line">	<span class="comment">// 使用命令行参数初始化params结构的成员</span></span><br><span class="line">    samplesCommon::CaffeSampleParams params = <span class="built_in">initializeSampleParams</span>(args);</span><br><span class="line">	<span class="comment">// 构造SampleMNIST对象</span></span><br><span class="line">    <span class="function">SampleMNIST <span class="title">sample</span><span class="params">(params)</span></span>;</span><br><span class="line">    sample::gLogInfo &lt;&lt; <span class="string">&quot;Building and running a GPU inference engine for MNIST&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">// Build 此函数通过解析caffe模型创建MNIST网络，并构建用于运行MNIST（mEngine）的引擎</span></span><br><span class="line">    <span class="keyword">if</span> (!sample.<span class="built_in">build</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> sample::gLogger.<span class="built_in">reportFail</span>(sampleTest);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 前向推理如果没成功，用gLogger报告状态</span></span><br><span class="line">    <span class="keyword">if</span> (!sample.<span class="built_in">infer</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> sample::gLogger.<span class="built_in">reportFail</span>(sampleTest);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 用于清除示例类中创建的任何状态，内存释放</span></span><br><span class="line">    <span class="keyword">if</span> (!sample.<span class="built_in">teardown</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> sample::gLogger.<span class="built_in">reportFail</span>(sampleTest);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 报告例子运行成功</span></span><br><span class="line">    <span class="keyword">return</span> sample::gLogger.<span class="built_in">reportPass</span>(sampleTest);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以清晰的看到代码主要分为参数初始化，Build，Infer这三大部分</p>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><p>参数初始化主要由<code>initializeSampleParams</code>函数来完成，这个函数的详细注释如下，具体就是根据输入数据和网络文件所在的文件夹去读取LeNet的Caffe原始模型文件和均值文件，另外设置一些如输出Tensor名字，batch大小，运行时精度模式等关键参数，最后返回一个<code>params</code>对象。注意这里使用的LeNet模型是Caffe的原始模型，因为TensorRT是直接支持Caffe的原始模型解析的，但例如Pytorch模型之类的还要进行转换，</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \brief Initializes members of the params struct using the command line args</span></span><br><span class="line"><span class="comment">//! 使用命令行参数初始化params结构的成员</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="function">samplesCommon::CaffeSampleParams <span class="title">initializeSampleParams</span><span class="params">(<span class="type">const</span> samplesCommon::Args&amp; args)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    samplesCommon::CaffeSampleParams params;</span><br><span class="line">    <span class="keyword">if</span> (args.dataDirs.<span class="built_in">empty</span>()) <span class="comment">// Use default directories if user hasn&#x27;t provided directory paths如果用户未提供目录路径，则使用默认目录</span></span><br><span class="line">    &#123;</span><br><span class="line">        params.dataDirs.<span class="built_in">push_back</span>(<span class="string">&quot;data/mnist/&quot;</span>);</span><br><span class="line">        params.dataDirs.<span class="built_in">push_back</span>(<span class="string">&quot;data/samples/mnist/&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="comment">// Use the data directory provided by the user使用用户提供的目录路径</span></span><br><span class="line">    &#123;</span><br><span class="line">        params.dataDirs = args.dataDirs;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    params.prototxtFileName = <span class="built_in">locateFile</span>(<span class="string">&quot;mnist.prototxt&quot;</span>, params.dataDirs);<span class="comment">//读取params.dataDirs文件夹下模型文件mnist.prototxt</span></span><br><span class="line">    params.weightsFileName = <span class="built_in">locateFile</span>(<span class="string">&quot;mnist.caffemodel&quot;</span>, params.dataDirs);<span class="comment">//读取params.dataDirs文件夹下权重文件mnist.caffemodel</span></span><br><span class="line">    params.meanFileName = <span class="built_in">locateFile</span>(<span class="string">&quot;mnist_mean.binaryproto&quot;</span>, params.dataDirs);<span class="comment">//读取MNIST数字识别网络的均值文件</span></span><br><span class="line">    params.inputTensorNames.<span class="built_in">push_back</span>(<span class="string">&quot;data&quot;</span>);<span class="comment">// 输入Tensor</span></span><br><span class="line">    params.batchSize = <span class="number">1</span>;<span class="comment">//设置batch_size大小</span></span><br><span class="line">    params.outputTensorNames.<span class="built_in">push_back</span>(<span class="string">&quot;prob&quot;</span>);<span class="comment">// 输出Tensor</span></span><br><span class="line">    params.dlaCore = args.useDLACore;<span class="comment">// 是否使用DLA核心</span></span><br><span class="line">    params.int8 = args.runInInt8;<span class="comment">//以INT8的方式运行</span></span><br><span class="line">    params.fp16 = args.runInFp16;<span class="comment">//以FP16的方式运行</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params;<span class="comment">// 返回Params对象</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Build"><a href="#Build" class="headerlink" title="Build"></a>Build</h3><p>如下图所示，Build阶段主要完成模型转换(从Caffe/TensorFlow/Onnx-&gt;TensorRT)，在转换阶段会完成优化过程中的计算图融合，精度校准。这一步的输出是一个针对特定GPU平台和网络模型的优化过的TensorRT模型。这个TensorRT模型可以序列化的存储到磁盘或者内存中。存储到磁盘中的文件叫plan file。在sampleMNIST例子中只需要给tensorRT提供Caffe的<em>.prototxt，</em>.caffemodel,*.mean.binaryproto文件即可完成Build过程，另外这个还需要指定batch的大小并标记输出层。下面展示了sampleMNIST例子中的Build代码解析。</p>
<img src="/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906134331135.png" class="" title="image-20220906134331135">
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \brief Creates the network, configures the builder and creates the network engine</span></span><br><span class="line"><span class="comment">//!创建网络、配置生成器并创建网络引擎</span></span><br><span class="line"><span class="comment">//! \details This function creates the MNIST network by parsing the caffe model and builds</span></span><br><span class="line"><span class="comment">//!          the engine that will be used to run MNIST (mEngine)</span></span><br><span class="line"><span class="comment">//!此函数通过解析caffe模型创建MNIST网络，并构建用于运行MNIST（mEngine）的引擎</span></span><br><span class="line"><span class="comment">//! \return true if the engine was created successfully and false otherwise</span></span><br><span class="line"><span class="comment">//!如果引擎被创建成功，直接返回True</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">SampleMNIST::build</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//创建一个 IBuilder，传进gLogger参数是为了方便打印信息。</span></span><br><span class="line">    <span class="keyword">auto</span> builder = <span class="built_in">SampleUniquePtr</span>&lt;nvinfer1::IBuilder&gt;(nvinfer1::<span class="built_in">createInferBuilder</span>(sample::gLogger.<span class="built_in">getTRTLogger</span>()));</span><br><span class="line">    <span class="keyword">if</span> (!builder)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//创建空的network，后面 constructNetwork 中会定义</span></span><br><span class="line">    <span class="keyword">auto</span> network = <span class="built_in">SampleUniquePtr</span>&lt;nvinfer1::INetworkDefinition&gt;(builder-&gt;<span class="built_in">createNetworkV2</span>(<span class="number">0</span>));</span><br><span class="line">    <span class="keyword">if</span> (!network)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//创建一个配置文件解析对象</span></span><br><span class="line">    <span class="keyword">auto</span> config = <span class="built_in">SampleUniquePtr</span>&lt;nvinfer1::IBuilderConfig&gt;(builder-&gt;<span class="built_in">createBuilderConfig</span>());</span><br><span class="line">    <span class="keyword">if</span> (!config)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//创建一个caffe模型解析对象,在constructNetwork函数中解析模型，转换为 network</span></span><br><span class="line">    <span class="keyword">auto</span> parser = <span class="built_in">SampleUniquePtr</span>&lt;nvcaffeparser1::ICaffeParser&gt;(nvcaffeparser1::<span class="built_in">createCaffeParser</span>());</span><br><span class="line">    <span class="keyword">if</span> (!parser)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 解析 caffe 模型，并转换为 network 形式</span></span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">constructNetwork</span>(parser, network))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 设置batch大小，工作空间等等</span></span><br><span class="line">    builder-&gt;<span class="built_in">setMaxBatchSize</span>(mParams.batchSize);</span><br><span class="line">    config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kGPU_FALLBACK);</span><br><span class="line">    <span class="keyword">if</span> (mParams.fp16)</span><br><span class="line">    &#123;</span><br><span class="line">        config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kFP16);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (mParams.int8)</span><br><span class="line">    &#123;</span><br><span class="line">        config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kINT8);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    samplesCommon::<span class="built_in">enableDLA</span>(builder.<span class="built_in">get</span>(), config.<span class="built_in">get</span>(), mParams.dlaCore);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// CUDA stream used for profiling by the builder.</span></span><br><span class="line">    <span class="keyword">auto</span> profileStream = samplesCommon::<span class="built_in">makeCudaStream</span>();</span><br><span class="line">    <span class="keyword">if</span> (!profileStream)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    config-&gt;<span class="built_in">setProfileStream</span>(*profileStream);</span><br><span class="line"></span><br><span class="line">    SampleUniquePtr&lt;IHostMemory&gt; plan&#123;builder-&gt;<span class="built_in">buildSerializedNetwork</span>(*network, *config)&#125;;</span><br><span class="line">    <span class="keyword">if</span> (!plan)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    SampleUniquePtr&lt;IRuntime&gt; runtime&#123;<span class="built_in">createInferRuntime</span>(sample::gLogger.<span class="built_in">getTRTLogger</span>())&#125;;</span><br><span class="line">    <span class="keyword">if</span> (!runtime)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">//构建 tensorrt 引擎</span></span><br><span class="line">    mEngine = std::<span class="built_in">shared_ptr</span>&lt;nvinfer1::ICudaEngine&gt;(</span><br><span class="line">        runtime-&gt;<span class="built_in">deserializeCudaEngine</span>(plan-&gt;<span class="built_in">data</span>(), plan-&gt;<span class="built_in">size</span>()), samplesCommon::<span class="built_in">InferDeleter</span>());</span><br><span class="line">    <span class="keyword">if</span> (!mEngine)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">ASSERT</span>(network-&gt;<span class="built_in">getNbInputs</span>() == <span class="number">1</span>);</span><br><span class="line">    mInputDims = network-&gt;<span class="built_in">getInput</span>(<span class="number">0</span>)-&gt;<span class="built_in">getDimensions</span>();<span class="comment">//后面推理需要输入参数，这里取出来后面用</span></span><br><span class="line">    <span class="built_in">ASSERT</span>(mInputDims.nbDims == <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个代码中的关键函数是constructNetwork，这个函数的作用是使用caffe解析器创建MNIST数字识别网络(LeNet)并标记输出层，我们可以看一下它的代码解析。可以看到代码中主要就是标记了输出Tensor，并且对网络的输入数据进行预处理包括减均值和缩放之类的操作。</p>
<ul>
<li>其实模型转换本身，<code>parser-&gt;parse</code> 一个函数就解决了。</li>
<li>下面代码的大量篇幅是在：在模型开头添加 <code>输入图片减去平均数</code> 操作上。可能是网络训练是后减去均值并缩放了</li>
<li>替换了第两层网络，原网络第一层应该就是数据输入，这里替换为数据输入后减均值</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \brief Uses a caffe parser to create the MNIST Network and marks the</span></span><br><span class="line"><span class="comment">//!        output layers</span></span><br><span class="line"><span class="comment">//!使用caffe解析器创建MNIST网络并标记输出层</span></span><br><span class="line"><span class="comment">//! \param network Pointer to the network that will be populated with the MNIST network</span></span><br><span class="line"><span class="comment">//!指向将用MNIST网络填充的网络指针</span></span><br><span class="line"><span class="comment">//! \param builder Pointer to the engine builder</span></span><br><span class="line"><span class="comment">//!指向引擎生成器的生成器指针</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">SampleMNIST::constructNetwork</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    SampleUniquePtr&lt;nvcaffeparser1::ICaffeParser&gt;&amp; parser, SampleUniquePtr&lt;nvinfer1::INetworkDefinition&gt;&amp; network)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//这里就解析出了network并包含了权重</span></span><br><span class="line">    <span class="type">const</span> nvcaffeparser1::IBlobNameToTensor* blobNameToTensor = parser-&gt;<span class="built_in">parse</span>(</span><br><span class="line">        mParams.prototxtFileName.<span class="built_in">c_str</span>(), mParams.weightsFileName.<span class="built_in">c_str</span>(), *network, nvinfer1::DataType::kFLOAT);</span><br><span class="line">	<span class="comment">//输出Tensor标记</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; s : mParams.outputTensorNames)</span><br><span class="line">    &#123;</span><br><span class="line">        network-&gt;<span class="built_in">markOutput</span>(*blobNameToTensor-&gt;<span class="built_in">find</span>(s.<span class="built_in">c_str</span>()));</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">// 在网络开头添加减均值操作</span></span><br><span class="line">    <span class="comment">// add mean subtraction to the beginning of the network</span></span><br><span class="line">    nvinfer1::Dims inputDims = network-&gt;<span class="built_in">getInput</span>(<span class="number">0</span>)-&gt;<span class="built_in">getDimensions</span>();</span><br><span class="line">    <span class="comment">//// 读取均值文件的数据</span></span><br><span class="line">    mMeanBlob</span><br><span class="line">        = <span class="built_in">SampleUniquePtr</span>&lt;nvcaffeparser1::IBinaryProtoBlob&gt;(parser-&gt;<span class="built_in">parseBinaryProto</span>(mParams.meanFileName.<span class="built_in">c_str</span>()));</span><br><span class="line">    nvinfer1::Weights meanWeights&#123;nvinfer1::DataType::kFLOAT, mMeanBlob-&gt;<span class="built_in">getData</span>(), inputDims.d[<span class="number">1</span>] * inputDims.d[<span class="number">2</span>]&#125;;</span><br><span class="line">    <span class="comment">// For this sample, a large range based on the mean data is chosen and applied to the head of the network.</span></span><br><span class="line">    <span class="comment">// After the mean subtraction occurs, the range is expected to be between -127 and 127, so the rest of the network</span></span><br><span class="line">    <span class="comment">// is given a generic range.</span></span><br><span class="line">    <span class="comment">// The preferred method is use scales computed based on a representative data set</span></span><br><span class="line">    <span class="comment">// and apply each one individually based on the tensor. The range here is large enough for the</span></span><br><span class="line">    <span class="comment">// network, but is chosen for example purposes only.</span></span><br><span class="line">    <span class="comment">//数据的原始分布是[0,256]</span></span><br><span class="line">    <span class="comment">// 减去均值之后是[-127,127]</span></span><br><span class="line">    <span class="type">float</span> maxMean</span><br><span class="line">        = samplesCommon::<span class="built_in">getMaxValue</span>(<span class="built_in">static_cast</span>&lt;<span class="type">const</span> <span class="type">float</span>*&gt;(meanWeights.values), samplesCommon::<span class="built_in">volume</span>(inputDims));</span><br><span class="line">	<span class="comment">//在网络中添加一个常量的层 常量是均值 1 28 28</span></span><br><span class="line">    <span class="keyword">auto</span> mean = network-&gt;<span class="built_in">addConstant</span>(nvinfer1::<span class="built_in">Dims3</span>(<span class="number">1</span>, inputDims.d[<span class="number">1</span>], inputDims.d[<span class="number">2</span>]), meanWeights);</span><br><span class="line">    <span class="keyword">if</span> (!mean-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>)-&gt;<span class="built_in">setDynamicRange</span>(-maxMean, maxMean))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!network-&gt;<span class="built_in">getInput</span>(<span class="number">0</span>)-&gt;<span class="built_in">setDynamicRange</span>(-maxMean, maxMean))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//向网络添加一个元素操作层 执行减均值操作 </span></span><br><span class="line">    <span class="keyword">auto</span> meanSub = network-&gt;<span class="built_in">addElementWise</span>(*network-&gt;<span class="built_in">getInput</span>(<span class="number">0</span>), *mean-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>), ElementWiseOperation::kSUB);<span class="comment">//元素减</span></span><br><span class="line">    <span class="keyword">if</span> (!meanSub-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>)-&gt;<span class="built_in">setDynamicRange</span>(-maxMean, maxMean))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    network-&gt;<span class="built_in">getLayer</span>(<span class="number">0</span>)-&gt;<span class="built_in">setInput</span>(<span class="number">0</span>, *meanSub-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>));<span class="comment">//将这一层的输入替换为一个特定的张量。 替换第0层的网络为meanSub的输出</span></span><br><span class="line">    <span class="comment">// 设置范围</span></span><br><span class="line">    samplesCommon::<span class="built_in">setAllDynamicRanges</span>(network.<span class="built_in">get</span>(), <span class="number">127.0f</span>, <span class="number">127.0f</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Infer"><a href="#Infer" class="headerlink" title="Infer"></a>Infer</h3><img src="/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220906141106404.png" class="" title="image-20220906141106404">
<p>如上图所示，Infer阶段就是完成前向推理过程了，这里将Build过程中获得的plan文件首先反序列化，并创建一个 runtime engine，然后就可以输入数据，然后输出分类向量结果或检测结果。Deploy阶段的实现在infer函数中，它负责分配缓冲区，设置输入，执行推理引擎并验证输出。代码解析如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \brief Runs the TensorRT inference engine for this sample</span></span><br><span class="line"><span class="comment">//!对这个例子执行TensorRT的前向推理</span></span><br><span class="line"><span class="comment">//! \details This function is the main execution function of the sample. It allocates</span></span><br><span class="line"><span class="comment">//!          the buffer, sets inputs, executes the engine, and verifies the output.</span></span><br><span class="line"><span class="comment">//!此函数是示例的主要执行功能。 它分配缓冲区，设置输入，执行推理引擎并验证输出。</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">SampleMNIST::infer</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Create RAII buffer manager object</span></span><br><span class="line">    <span class="comment">// 缓存对象管理</span></span><br><span class="line">    <span class="function">samplesCommon::BufferManager <span class="title">buffers</span><span class="params">(mEngine, mParams.batchSize)</span></span>;</span><br><span class="line">	<span class="comment">// 创建上下文</span></span><br><span class="line">    <span class="keyword">auto</span> context = <span class="built_in">SampleUniquePtr</span>&lt;nvinfer1::IExecutionContext&gt;(mEngine-&gt;<span class="built_in">createExecutionContext</span>());</span><br><span class="line">    <span class="keyword">if</span> (!context)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Pick a random digit to try to infer</span></span><br><span class="line">    <span class="comment">// 随机选择一个数字</span></span><br><span class="line">    <span class="built_in">srand</span>(<span class="built_in">time</span>(<span class="literal">NULL</span>));</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> digit = <span class="built_in">rand</span>() % <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Read the input data into the managed buffers</span></span><br><span class="line">    <span class="comment">// There should be just 1 input tensor</span></span><br><span class="line">    <span class="comment">// 读取输入数据到缓存对象中</span></span><br><span class="line">    <span class="comment">// 即将 digit 写入 buffers 中，名字为 mParams.inputTensorNames[0]</span></span><br><span class="line">    <span class="built_in">ASSERT</span>(mParams.inputTensorNames.<span class="built_in">size</span>() == <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">processInput</span>(buffers, mParams.inputTensorNames[<span class="number">0</span>], digit))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Create CUDA stream for the execution of this inference.</span></span><br><span class="line">    <span class="comment">// 创建 cuda 流，准备执行推理</span></span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaStreamCreate</span>(&amp;stream));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Asynchronously copy data from host input buffers to device input buffers</span></span><br><span class="line">    <span class="comment">// 异步将数据从主机输入缓冲区(buffer)复制到设备输入缓冲区(stream)</span></span><br><span class="line">    buffers.<span class="built_in">copyInputToDeviceAsync</span>(stream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Asynchronously enqueue the inference work</span></span><br><span class="line">    <span class="comment">// 异步将推理任务加入队列中</span></span><br><span class="line">    <span class="keyword">if</span> (!context-&gt;<span class="built_in">enqueue</span>(mParams.batchSize, buffers.<span class="built_in">getDeviceBindings</span>().<span class="built_in">data</span>(), stream, <span class="literal">nullptr</span>))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Asynchronously copy data from device output buffers to host output buffers</span></span><br><span class="line">    <span class="comment">// 异步将模型结果从设备(stream)保存到主机缓冲区(buffers)</span></span><br><span class="line">    buffers.<span class="built_in">copyOutputToHostAsync</span>(stream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Wait for the work in the stream to complete</span></span><br><span class="line">    <span class="comment">// 等待工作结束，关闭stream</span></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaStreamSynchronize</span>(stream));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Release stream</span></span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaStreamDestroy</span>(stream));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Check and print the output of the inference</span></span><br><span class="line">    <span class="comment">// There should be just one output tensor</span></span><br><span class="line">    <span class="comment">// 得到结果，判断结果是否准确</span></span><br><span class="line">    <span class="comment">// 即从 buffer 中获取名为 mParams.outputTensorNames[0] 的结果，判断与digit是否相同</span></span><br><span class="line">    <span class="built_in">ASSERT</span>(mParams.outputTensorNames.<span class="built_in">size</span>() == <span class="number">1</span>);</span><br><span class="line">    <span class="type">bool</span> outputCorrect = <span class="built_in">verifyOutput</span>(buffers, mParams.outputTensorNames[<span class="number">0</span>], digit);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> outputCorrect;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \brief Reads the input and mean data, preprocesses, and stores the result in a managed buffer</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">SampleMNIST::processInput</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> samplesCommon::BufferManager&amp; buffers, <span class="type">const</span> std::string&amp; inputTensorName, <span class="type">int</span> inputFileIdx)</span> <span class="type">const</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> inputH = mInputDims.d[<span class="number">1</span>];<span class="comment">//mInputDims.d[0]=1 mInputDims.d[1]=28 mInputDims.d[2]=28</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> inputW = mInputDims.d[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Read a random digit file</span></span><br><span class="line">    <span class="built_in">srand</span>(<span class="built_in">unsigned</span>(<span class="built_in">time</span>(<span class="literal">nullptr</span>)));</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">uint8_t</span>&gt; <span class="title">fileData</span><span class="params">(inputH * inputW)</span></span>;</span><br><span class="line">    <span class="comment">//读取图片</span></span><br><span class="line">    <span class="built_in">readPGMFile</span>(<span class="built_in">locateFile</span>(std::<span class="built_in">to_string</span>(inputFileIdx) + <span class="string">&quot;.pgm&quot;</span>, mParams.dataDirs), fileData.<span class="built_in">data</span>(), inputH, inputW);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Print ASCII representation of digit</span></span><br><span class="line">    sample::gLogInfo &lt;&lt; <span class="string">&quot;Input:\n&quot;</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; inputH * inputW; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        sample::gLogInfo &lt;&lt; (<span class="string">&quot; .:-=+*#%@&quot;</span>[fileData[i] / <span class="number">26</span>]) &lt;&lt; (((i + <span class="number">1</span>) % inputW) ? <span class="string">&quot;&quot;</span> : <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    sample::gLogInfo &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span>* hostInputBuffer = <span class="built_in">static_cast</span>&lt;<span class="type">float</span>*&gt;(buffers.<span class="built_in">getHostBuffer</span>(inputTensorName));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; inputH * inputW; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        hostInputBuffer[i] = <span class="built_in">float</span>(fileData[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="zyd修改测试"><a href="#zyd修改测试" class="headerlink" title="zyd修改测试"></a>zyd修改测试</h3><p>实现了engine文件的保存和加载，主要就是修改了build部分</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">SampleMNIST::build</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> builder = <span class="built_in">SampleUniquePtr</span>&lt;nvinfer1::IBuilder&gt;(nvinfer1::<span class="built_in">createInferBuilder</span>(sample::gLogger.<span class="built_in">getTRTLogger</span>()));</span><br><span class="line">    <span class="keyword">if</span> (!builder)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> network = <span class="built_in">SampleUniquePtr</span>&lt;nvinfer1::INetworkDefinition&gt;(builder-&gt;<span class="built_in">createNetworkV2</span>(<span class="number">0</span>));</span><br><span class="line">    <span class="keyword">if</span> (!network)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> config = <span class="built_in">SampleUniquePtr</span>&lt;nvinfer1::IBuilderConfig&gt;(builder-&gt;<span class="built_in">createBuilderConfig</span>());</span><br><span class="line">    <span class="keyword">if</span> (!config)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> parser = <span class="built_in">SampleUniquePtr</span>&lt;nvcaffeparser1::ICaffeParser&gt;(nvcaffeparser1::<span class="built_in">createCaffeParser</span>());</span><br><span class="line">    <span class="keyword">if</span> (!parser)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">constructNetwork</span>(parser, network))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    builder-&gt;<span class="built_in">setMaxBatchSize</span>(mParams.batchSize);</span><br><span class="line">    config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kGPU_FALLBACK);</span><br><span class="line">    <span class="keyword">if</span> (mParams.fp16)</span><br><span class="line">    &#123;</span><br><span class="line">        config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kFP16);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (mParams.int8)</span><br><span class="line">    &#123;</span><br><span class="line">        config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kINT8);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    samplesCommon::<span class="built_in">enableDLA</span>(builder.<span class="built_in">get</span>(), config.<span class="built_in">get</span>(), mParams.dlaCore);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// CUDA stream used for profiling by the builder.</span></span><br><span class="line">    <span class="keyword">auto</span> profileStream = samplesCommon::<span class="built_in">makeCudaStream</span>();</span><br><span class="line">    <span class="keyword">if</span> (!profileStream)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    config-&gt;<span class="built_in">setProfileStream</span>(*profileStream);</span><br><span class="line"></span><br><span class="line">    SampleUniquePtr&lt;IHostMemory&gt; plan&#123;builder-&gt;<span class="built_in">buildSerializedNetwork</span>(*network, *config)&#125;;</span><br><span class="line">    <span class="keyword">if</span> (!plan)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">//zyd add 保存engine文件</span></span><br><span class="line">	<span class="function">std::ofstream <span class="title">engineFile</span><span class="params">(<span class="string">&quot;MNIST.plan&quot;</span>, std::ios::binary)</span></span>;</span><br><span class="line">    <span class="keyword">if</span> (!engineFile)</span><br><span class="line">    &#123;</span><br><span class="line">        sample::gLogError &lt;&lt; <span class="string">&quot;Cannot open engine file: &quot;</span>  &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">	engineFile.<span class="built_in">write</span>(<span class="built_in">static_cast</span>&lt;<span class="type">char</span>*&gt;(plan-&gt;<span class="built_in">data</span>()), plan-&gt;<span class="built_in">size</span>());</span><br><span class="line">	<span class="comment">// zyd add end</span></span><br><span class="line">    SampleUniquePtr&lt;IRuntime&gt; runtime&#123;<span class="built_in">createInferRuntime</span>(sample::gLogger.<span class="built_in">getTRTLogger</span>())&#125;;</span><br><span class="line">    <span class="keyword">if</span> (!runtime)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">//zyd add 读取engin文件</span></span><br><span class="line">	<span class="function">std::ifstream <span class="title">readEngineFile</span><span class="params">(<span class="string">&quot;MNIST.plan&quot;</span>, std::ios::binary)</span></span>;</span><br><span class="line">	<span class="keyword">if</span> (!readEngineFile)</span><br><span class="line">	&#123;</span><br><span class="line">		std::cout &lt;&lt; <span class="string">&quot;Error opening engine file: &quot;</span>  &lt;&lt; std::endl;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	readEngineFile.<span class="built_in">seekg</span>(<span class="number">0</span>, readEngineFile.end);</span><br><span class="line">	<span class="type">long</span> <span class="type">int</span> fsize = readEngineFile.<span class="built_in">tellg</span>();</span><br><span class="line">	readEngineFile.<span class="built_in">seekg</span>(<span class="number">0</span>, readEngineFile.beg);</span><br><span class="line"></span><br><span class="line">	<span class="function">std::vector&lt;<span class="type">char</span>&gt; <span class="title">engineData</span><span class="params">(fsize)</span></span>;</span><br><span class="line">	readEngineFile.<span class="built_in">read</span>(engineData.<span class="built_in">data</span>(), fsize);</span><br><span class="line">	mEngine = std::<span class="built_in">shared_ptr</span>&lt;nvinfer1::ICudaEngine&gt;(</span><br><span class="line">         runtime-&gt;<span class="built_in">deserializeCudaEngine</span>(engineData.<span class="built_in">data</span>(), engineData.<span class="built_in">size</span>()), samplesCommon::<span class="built_in">InferDeleter</span>());</span><br><span class="line">    <span class="comment">//zyd add end</span></span><br><span class="line">	<span class="comment">// 修改为从plan文件读取</span></span><br><span class="line">	<span class="comment">// mEngine = std::shared_ptr&lt;nvinfer1::ICudaEngine&gt;(</span></span><br><span class="line">    <span class="comment">//     runtime-&gt;deserializeCudaEngine(plan-&gt;data(), plan-&gt;size()), samplesCommon::InferDeleter());</span></span><br><span class="line">    <span class="keyword">if</span> (!mEngine)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">ASSERT</span>(network-&gt;<span class="built_in">getNbInputs</span>() == <span class="number">1</span>);</span><br><span class="line">    mInputDims = network-&gt;<span class="built_in">getInput</span>(<span class="number">0</span>)-&gt;<span class="built_in">getDimensions</span>();</span><br><span class="line">    <span class="built_in">ASSERT</span>(mInputDims.nbDims == <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="例子2-sampleMNISTAPI"><a href="#例子2-sampleMNISTAPI" class="headerlink" title="例子2 sampleMNISTAPI"></a>例子2 sampleMNISTAPI</h1><p>例子1主要是用 TensorRT 提供的<code>NvCaffeParser</code>来将<code>Caffe</code>中的<code>model</code>转换成 TensorRT 中特有的模型结构。其中<code>NvCaffeParser</code>是<code>TensorRT</code>封装好的一个用以解析<code>Caffe</code>模型的工具 （高层的 API），同样的还有<code>NvUffPaser</code>用于解析 TensorFlow 的<code>pb</code>模型，<code>NvONNXParse</code>用于解析 Onnx 模型。除了这几个工具之外，TensorRT 还提供了 C++ API（底层的 API）直接在 TensorRT 中创建模型。这时候 TensorRT 相当于是一个独立的深度学习框架，不过这个框架只负责前向推理 (Inference)。</p>
<p>使用 C++ API 函数部署网络主要分成 4 个步骤，即： <strong>1. 创建网络。 2. 给网络添加输入。 3. 添加各种各样的层。 4. 设定网络输出。</strong></p>
<h2 id="sampleMNISTAPI代码解析"><a href="#sampleMNISTAPI代码解析" class="headerlink" title="sampleMNISTAPI代码解析"></a>sampleMNISTAPI代码解析</h2><p>和上面的例子几乎一样的结构</p>
<h3 id="主函数-1"><a href="#主函数-1" class="headerlink" title="主函数"></a>主函数</h3><p>几乎没有区别</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    samplesCommon::Args args;</span><br><span class="line">    <span class="type">bool</span> argsOK = samplesCommon::<span class="built_in">parseArgs</span>(args, argc, argv);</span><br><span class="line">    <span class="keyword">if</span> (!argsOK)</span><br><span class="line">    &#123;</span><br><span class="line">        sample::gLogError &lt;&lt; <span class="string">&quot;Invalid arguments&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">printHelpInfo</span>();</span><br><span class="line">        <span class="keyword">return</span> EXIT_FAILURE;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (args.help)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printHelpInfo</span>();</span><br><span class="line">        <span class="keyword">return</span> EXIT_SUCCESS;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> sampleTest = sample::gLogger.<span class="built_in">defineTest</span>(gSampleName, argc, argv);</span><br><span class="line"></span><br><span class="line">    sample::gLogger.<span class="built_in">reportTestStart</span>(sampleTest);</span><br><span class="line"></span><br><span class="line">    <span class="function">SampleMNISTAPI <span class="title">sample</span><span class="params">(initializeSampleParams(args))</span></span>;<span class="comment">//只有这里不一样 上一个是指定读取的模型/权重/均值文件 这里是读取的权值mnistapi.wts</span></span><br><span class="line"></span><br><span class="line">    sample::gLogInfo &lt;&lt; <span class="string">&quot;Building and running a GPU inference engine for MNIST API&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!sample.<span class="built_in">build</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> sample::gLogger.<span class="built_in">reportFail</span>(sampleTest);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!sample.<span class="built_in">infer</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> sample::gLogger.<span class="built_in">reportFail</span>(sampleTest);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!sample.<span class="built_in">teardown</span>())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> sample::gLogger.<span class="built_in">reportFail</span>(sampleTest);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sample::gLogger.<span class="built_in">reportPass</span>(sampleTest);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Build-1"><a href="#Build-1" class="headerlink" title="Build"></a>Build</h3><p>重要的区别就在这里面，上一个例子是使用其中从的constructNetwork构建网络，其中调用了<em>parser</em>-&gt;parse直接就构建出了网络，只是将网络输入层修改了一下，添加了两层作减去均值的处理。</p>
<p>这里就是使用API函数一步一步的建立网络（没有作减均值处理，减均值的操作在推理读取图片时处理了）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \brief Creates the network, configures the builder and creates the network engine</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \details This function creates the MNIST network by using the API to create a model and builds</span></span><br><span class="line"><span class="comment">//!          the engine that will be used to run MNIST (mEngine)</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \return true if the engine was created successfully and false otherwise</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">SampleMNISTAPI::build</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;	<span class="comment">////加载权重，*.wts文件</span></span><br><span class="line">    mWeightMap = <span class="built_in">loadWeights</span>(<span class="built_in">locateFile</span>(mParams.weightsFile, mParams.dataDirs));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> builder = <span class="built_in">SampleUniquePtr</span>&lt;nvinfer1::IBuilder&gt;(nvinfer1::<span class="built_in">createInferBuilder</span>(sample::gLogger.<span class="built_in">getTRTLogger</span>()));</span><br><span class="line">    <span class="keyword">if</span> (!builder)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> explicitBatchFlag = <span class="number">1U</span> &lt;&lt; <span class="built_in">static_cast</span>&lt;<span class="type">uint32_t</span>&gt;(nvinfer1::NetworkDefinitionCreationFlag::kEXPLICIT_BATCH);</span><br><span class="line">    <span class="keyword">auto</span> network = <span class="built_in">SampleUniquePtr</span>&lt;nvinfer1::INetworkDefinition&gt;(builder-&gt;<span class="built_in">createNetworkV2</span>(explicitBatchFlag));</span><br><span class="line">    <span class="keyword">if</span> (!network)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> config = <span class="built_in">SampleUniquePtr</span>&lt;nvinfer1::IBuilderConfig&gt;(builder-&gt;<span class="built_in">createBuilderConfig</span>());</span><br><span class="line">    <span class="keyword">if</span> (!config)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> constructed = <span class="built_in">constructNetwork</span>(builder, network, config);<span class="comment">//区别在这里面，使用的是API函数来创建网络结构并加载权重</span></span><br><span class="line">    <span class="keyword">if</span> (!constructed)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">ASSERT</span>(network-&gt;<span class="built_in">getNbInputs</span>() == <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">auto</span> inputDims = network-&gt;<span class="built_in">getInput</span>(<span class="number">0</span>)-&gt;<span class="built_in">getDimensions</span>();</span><br><span class="line">    <span class="built_in">ASSERT</span>(inputDims.nbDims == <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">ASSERT</span>(network-&gt;<span class="built_in">getNbOutputs</span>() == <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">auto</span> outputDims = network-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>)-&gt;<span class="built_in">getDimensions</span>();</span><br><span class="line">    <span class="built_in">ASSERT</span>(outputDims.nbDims == <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>按照mnist.caffemodel文件创建层，如下图</p>
<img src="/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220907153229288.png" class="" title="image-20220907153229288">
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \brief Uses the API to create the MNIST Network</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \param network Pointer to the network that will be populated with the MNIST network</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \param builder Pointer to the engine builder</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">SampleMNISTAPI::constructNetwork</span><span class="params">(SampleUniquePtr&lt;nvinfer1::IBuilder&gt;&amp; builder,</span></span></span><br><span class="line"><span class="params"><span class="function">    SampleUniquePtr&lt;nvinfer1::INetworkDefinition&gt;&amp; network, SampleUniquePtr&lt;nvinfer1::IBuilderConfig&gt;&amp; config)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Create input tensor of shape &#123; 1, 1, 28, 28 &#125;</span></span><br><span class="line">    <span class="comment">//将输入张量添加到网络中。Dims4&#123;1,1, mParams.inputH, mParams.inputW&#125;指的是，batch_size为 1，channel为 1，输入height和width分别为 INPUT_H, INPUT_W 的blob。</span></span><br><span class="line">    ITensor* data = network-&gt;<span class="built_in">addInput</span>(</span><br><span class="line">        mParams.inputTensorNames[<span class="number">0</span>].<span class="built_in">c_str</span>(), DataType::kFLOAT, Dims4&#123;<span class="number">1</span>, <span class="number">1</span>, mParams.inputH, mParams.inputW&#125;);</span><br><span class="line">    <span class="built_in">ASSERT</span>(data);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create scale layer with default power/shift and specified scale parameter.</span></span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> scaleParam = <span class="number">0.0125f</span>;<span class="comment">//可以从mnist.caffemodel的power层看出这里的值</span></span><br><span class="line">    <span class="type">const</span> Weights power&#123;DataType::kFLOAT, <span class="literal">nullptr</span>, <span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">const</span> Weights shift&#123;DataType::kFLOAT, <span class="literal">nullptr</span>, <span class="number">0</span>&#125;;</span><br><span class="line">    <span class="type">const</span> Weights scale&#123;DataType::kFLOAT, &amp;scaleParam, <span class="number">1</span>&#125;;</span><br><span class="line">    <span class="comment">//后面就是添加各种层</span></span><br><span class="line">    IScaleLayer* scale_1 = network-&gt;<span class="built_in">addScale</span>(*data, ScaleMode::kUNIFORM, shift, scale, power);</span><br><span class="line">    <span class="built_in">ASSERT</span>(scale_1);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add convolution layer with 20 outputs and a 5x5 filter.</span></span><br><span class="line">    <span class="comment">//`20`表示卷积核的个数，`DimsHW&#123;5, 5&#125;`表示卷积核的大小，`weightMap[&quot;conv1filter&quot;]和weightMap[&quot;conv1bias&quot;]`表示权值系数矩阵</span></span><br><span class="line">    IConvolutionLayer* conv1 = network-&gt;<span class="built_in">addConvolutionNd</span>(</span><br><span class="line">        *scale_1-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>), <span class="number">20</span>, Dims&#123;<span class="number">2</span>, &#123;<span class="number">5</span>, <span class="number">5</span>&#125;&#125;, mWeightMap[<span class="string">&quot;conv1filter&quot;</span>], mWeightMap[<span class="string">&quot;conv1bias&quot;</span>]);</span><br><span class="line">    <span class="built_in">ASSERT</span>(conv1);</span><br><span class="line">    conv1-&gt;<span class="built_in">setStride</span>(DimsHW&#123;<span class="number">1</span>, <span class="number">1</span>&#125;);<span class="comment">//设置步长为1 这些数值都可以在mnist.caffemodel中获取</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add max pooling layer with stride of 2x2 and kernel size of 2x2.</span></span><br><span class="line">    IPoolingLayer* pool1 = network-&gt;<span class="built_in">addPoolingNd</span>(*conv1-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>), PoolingType::kMAX, Dims&#123;<span class="number">2</span>, &#123;<span class="number">2</span>, <span class="number">2</span>&#125;&#125;);</span><br><span class="line">    <span class="built_in">ASSERT</span>(pool1);</span><br><span class="line">    pool1-&gt;<span class="built_in">setStride</span>(DimsHW&#123;<span class="number">2</span>, <span class="number">2</span>&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add second convolution layer with 50 outputs and a 5x5 filter.</span></span><br><span class="line">    IConvolutionLayer* conv2 = network-&gt;<span class="built_in">addConvolutionNd</span>(</span><br><span class="line">        *pool1-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>), <span class="number">50</span>, Dims&#123;<span class="number">2</span>, &#123;<span class="number">5</span>, <span class="number">5</span>&#125;&#125;, mWeightMap[<span class="string">&quot;conv2filter&quot;</span>], mWeightMap[<span class="string">&quot;conv2bias&quot;</span>]);</span><br><span class="line">    <span class="built_in">ASSERT</span>(conv2);</span><br><span class="line">    conv2-&gt;<span class="built_in">setStride</span>(DimsHW&#123;<span class="number">1</span>, <span class="number">1</span>&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add second max pooling layer with stride of 2x2 and kernel size of 2x3&gt;</span></span><br><span class="line">    IPoolingLayer* pool2 = network-&gt;<span class="built_in">addPoolingNd</span>(*conv2-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>), PoolingType::kMAX, Dims&#123;<span class="number">2</span>, &#123;<span class="number">2</span>, <span class="number">2</span>&#125;&#125;);</span><br><span class="line">    <span class="built_in">ASSERT</span>(pool2);</span><br><span class="line">    pool2-&gt;<span class="built_in">setStride</span>(DimsHW&#123;<span class="number">2</span>, <span class="number">2</span>&#125;);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Utility for use MatMul as FC</span></span><br><span class="line">    <span class="keyword">auto</span> addMatMulasFCLayer</span><br><span class="line">        = [&amp;network](ITensor* input, <span class="type">int32_t</span> <span class="type">const</span> outputs, Weights&amp; filterWeights, Weights&amp; biasWeights) -&gt; ILayer* &#123;</span><br><span class="line">        Dims inputDims = input-&gt;<span class="built_in">getDimensions</span>();</span><br><span class="line">        <span class="type">int32_t</span> <span class="type">const</span> m = inputDims.d[<span class="number">0</span>];</span><br><span class="line">        <span class="type">int32_t</span> <span class="type">const</span> k</span><br><span class="line">            = std::<span class="built_in">accumulate</span>(inputDims.d + <span class="number">1</span>, inputDims.d + inputDims.nbDims, <span class="number">1</span>, std::<span class="built_in">multiplies</span>&lt;<span class="type">int32_t</span>&gt;());</span><br><span class="line">        <span class="type">int32_t</span> <span class="type">const</span> n = <span class="built_in">static_cast</span>&lt;<span class="type">int32_t</span>&gt;(filterWeights.count / <span class="built_in">static_cast</span>&lt;<span class="type">int64_t</span>&gt;(k));</span><br><span class="line">        <span class="built_in">ASSERT</span>(<span class="built_in">static_cast</span>&lt;<span class="type">int64_t</span>&gt;(n) * <span class="built_in">static_cast</span>&lt;<span class="type">int64_t</span>&gt;(k) == filterWeights.count);</span><br><span class="line">        <span class="built_in">ASSERT</span>(<span class="built_in">static_cast</span>&lt;<span class="type">int64_t</span>&gt;(n) == biasWeights.count);</span><br><span class="line">        <span class="built_in">ASSERT</span>(n == outputs);</span><br><span class="line"></span><br><span class="line">        IShuffleLayer* inputReshape = network-&gt;<span class="built_in">addShuffle</span>(*input);</span><br><span class="line">        <span class="built_in">ASSERT</span>(inputReshape);</span><br><span class="line">        inputReshape-&gt;<span class="built_in">setReshapeDimensions</span>(Dims&#123;<span class="number">2</span>, &#123;m, k&#125;&#125;);</span><br><span class="line"></span><br><span class="line">        IConstantLayer* filterConst = network-&gt;<span class="built_in">addConstant</span>(Dims&#123;<span class="number">2</span>, &#123;n, k&#125;&#125;, filterWeights);</span><br><span class="line">        <span class="built_in">ASSERT</span>(filterConst);</span><br><span class="line">        IMatrixMultiplyLayer* mm = network-&gt;<span class="built_in">addMatrixMultiply</span>(*inputReshape-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>), MatrixOperation::kNONE,</span><br><span class="line">            *filterConst-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>), MatrixOperation::kTRANSPOSE);</span><br><span class="line">        <span class="built_in">ASSERT</span>(mm);</span><br><span class="line"></span><br><span class="line">        IConstantLayer* biasConst = network-&gt;<span class="built_in">addConstant</span>(Dims&#123;<span class="number">2</span>, &#123;<span class="number">1</span>, n&#125;&#125;, biasWeights);</span><br><span class="line">        <span class="built_in">ASSERT</span>(biasConst);</span><br><span class="line">        IElementWiseLayer* biasAdd</span><br><span class="line">            = network-&gt;<span class="built_in">addElementWise</span>(*mm-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>), *biasConst-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>), ElementWiseOperation::kSUM);</span><br><span class="line">        <span class="built_in">ASSERT</span>(biasAdd);</span><br><span class="line"></span><br><span class="line">        IShuffleLayer* outputReshape = network-&gt;<span class="built_in">addShuffle</span>(*biasAdd-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>));</span><br><span class="line">        <span class="built_in">ASSERT</span>(outputReshape);</span><br><span class="line">        outputReshape-&gt;<span class="built_in">setReshapeDimensions</span>(Dims&#123;<span class="number">4</span>, &#123;m, n, <span class="number">1</span>, <span class="number">1</span>&#125;&#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> outputReshape;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add fully connected layer with 500 outputs.</span></span><br><span class="line">    ILayer* ip1 = <span class="built_in">addMatMulasFCLayer</span>(pool2-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>), <span class="number">500</span>, mWeightMap[<span class="string">&quot;ip1filter&quot;</span>], mWeightMap[<span class="string">&quot;ip1bias&quot;</span>]);</span><br><span class="line">    <span class="built_in">ASSERT</span>(ip1);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add activation layer using the ReLU algorithm.</span></span><br><span class="line">    IActivationLayer* relu1 = network-&gt;<span class="built_in">addActivation</span>(*ip1-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>), ActivationType::kRELU);</span><br><span class="line">    <span class="built_in">ASSERT</span>(relu1);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add second fully connected layer with 20 outputs.</span></span><br><span class="line">    ILayer* ip2</span><br><span class="line">        = <span class="built_in">addMatMulasFCLayer</span>(relu1-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>), mParams.outputSize, mWeightMap[<span class="string">&quot;ip2filter&quot;</span>], mWeightMap[<span class="string">&quot;ip2bias&quot;</span>]);</span><br><span class="line">    <span class="built_in">ASSERT</span>(ip2);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add softmax layer to determine the probability.</span></span><br><span class="line">    ISoftMaxLayer* prob = network-&gt;<span class="built_in">addSoftMax</span>(*ip2-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>));</span><br><span class="line">    <span class="built_in">ASSERT</span>(prob);</span><br><span class="line">    prob-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>)-&gt;<span class="built_in">setName</span>(mParams.outputTensorNames[<span class="number">0</span>].<span class="built_in">c_str</span>());</span><br><span class="line">    network-&gt;<span class="built_in">markOutput</span>(*prob-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Build engine</span></span><br><span class="line">    <span class="keyword">if</span> (mParams.fp16)</span><br><span class="line">    &#123;</span><br><span class="line">        config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kFP16);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (mParams.int8)</span><br><span class="line">    &#123;</span><br><span class="line">        config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kINT8);</span><br><span class="line">        samplesCommon::<span class="built_in">setAllDynamicRanges</span>(network.<span class="built_in">get</span>(), <span class="number">64.0f</span>, <span class="number">64.0f</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    samplesCommon::<span class="built_in">enableDLA</span>(builder.<span class="built_in">get</span>(), config.<span class="built_in">get</span>(), mParams.dlaCore);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// CUDA stream used for profiling by the builder.</span></span><br><span class="line">    <span class="keyword">auto</span> profileStream = samplesCommon::<span class="built_in">makeCudaStream</span>();</span><br><span class="line">    <span class="keyword">if</span> (!profileStream)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    config-&gt;<span class="built_in">setProfileStream</span>(*profileStream);</span><br><span class="line"></span><br><span class="line">    SampleUniquePtr&lt;IHostMemory&gt; plan&#123;builder-&gt;<span class="built_in">buildSerializedNetwork</span>(*network, *config)&#125;;</span><br><span class="line">    <span class="keyword">if</span> (!plan)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    SampleUniquePtr&lt;IRuntime&gt; runtime&#123;<span class="built_in">createInferRuntime</span>(sample::gLogger.<span class="built_in">getTRTLogger</span>())&#125;;</span><br><span class="line">    <span class="keyword">if</span> (!runtime)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    mEngine = std::<span class="built_in">shared_ptr</span>&lt;nvinfer1::ICudaEngine&gt;(</span><br><span class="line">        runtime-&gt;<span class="built_in">deserializeCudaEngine</span>(plan-&gt;<span class="built_in">data</span>(), plan-&gt;<span class="built_in">size</span>()), samplesCommon::<span class="built_in">InferDeleter</span>());</span><br><span class="line">    <span class="keyword">if</span> (!mEngine)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>```c++<br>IScaleLayer<em> scale_1 = network-&gt;addScale(</em>data, ScaleMode::kUNIFORM, shift, scale, power);<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  * 调用了一个`addScale()`函数，后面接受的参数是这一层需要设置的参数，Scale 层的作用是为每个输入数据执行幂运算，公式为</span><br><span class="line"></span><br><span class="line">  &#123;% asset_img image-20220907153430227.png image-20220907153430227 %&#125;</span><br><span class="line"></span><br><span class="line">  ​	层的类型为`Power`。可选参数为：power: 默认为1。scale: 默认为1。shift: 默认为0。</span><br><span class="line"></span><br><span class="line">* ```c++</span><br><span class="line">  // Add convolution layer with 20 outputs and a 5x5 filter.</span><br><span class="line">  //添加第一个卷积层</span><br><span class="line">      IConvolutionLayer* conv1 = network-&gt;addConvolutionNd(</span><br><span class="line">          *scale_1-&gt;getOutput(0), 20, Dims&#123;2, &#123;5, 5&#125;&#125;, mWeightMap[&quot;conv1filter&quot;], mWeightMap[&quot;conv1bias&quot;]);</span><br><span class="line">      ASSERT(conv1);</span><br><span class="line">      conv1-&gt;setStride(DimsHW&#123;1, 1&#125;);</span><br><span class="line">  //Scale 层是没有训练参数的，ReLU 层，Pooling 层都没有训练参数。而有训练参数的如卷积层，全连接层，在构造的时候则需要先加载权重文件。</span><br><span class="line">  //注意这里的`mWeightMap`在`bool SampleMNISTAPI::build()`函数里面已经加载了，权重只用加载一次。在第一行添加卷积层的函数里面，`*scale_1-&gt;getOutput(0)` 用来获取上一层 Scale 层的输出，`20`表示卷积核的个数，`DimsHW&#123;5, 5&#125;`表示卷积核的大小，`weightMap[&quot;conv1filter&quot;]和weightMap[&quot;conv1bias&quot;]`表示权值系数矩阵。</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="解析-mnistapi-wts-文件"><a href="#解析-mnistapi-wts-文件" class="headerlink" title="解析 mnistapi.wts 文件"></a>解析 mnistapi.wts 文件</h3><img src="/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220907154218486.png" class="" title="image-20220907154218486">
<p>容易发现每一行都是一层的一些参数，比如<code>conv1bias</code>就是第一个卷积层的偏置系数，后面的<code>0</code>指的是 kFLOAT 类型，也就是<code>float 32</code>；后面的<code>20</code>是系数的个数，因为输出是<code>20</code>，所以偏置是<code>20</code>个；下面一行是卷积核的系数，因为是<code>20</code>个<code>5 x 5</code>的卷积核，所以有<code>20 x 5 x 5=500</code>个参数</p>
<h1 id="例子3-sampleOnnxMNIST"><a href="#例子3-sampleOnnxMNIST" class="headerlink" title="例子3 sampleOnnxMNIST"></a>例子3 sampleOnnxMNIST</h1><p>这个例子演示了一个训练好的模型（ONNX格式）转换为TensorRT网络并进行推理。</p>
<p>ONNX是表示深度学习模型的标准，它允许模型在框架之间转移。</p>
<p>实例大概为以下三步</p>
<ul>
<li>将ONNX模型转换为TensorRT网络（与第一个例子几乎一致）</li>
<li>构建一个engine（与上个例子几乎一致）</li>
<li>使用生成的TensorRT网络进行推理（与上个例子几乎一致）</li>
</ul>
<h2 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h2><h3 id="build"><a href="#build" class="headerlink" title="build"></a>build</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \brief Creates the network, configures the builder and creates the network engine</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \details This function creates the Onnx MNIST network by parsing the Onnx model and builds</span></span><br><span class="line"><span class="comment">//!          the engine that will be used to run MNIST (mEngine)</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \return true if the engine was created successfully and false otherwise</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">SampleOnnxMNIST::build</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> builder = <span class="built_in">SampleUniquePtr</span>&lt;nvinfer1::IBuilder&gt;(nvinfer1::<span class="built_in">createInferBuilder</span>(sample::gLogger.<span class="built_in">getTRTLogger</span>()));</span><br><span class="line">    <span class="keyword">if</span> (!builder)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="keyword">auto</span> explicitBatch = <span class="number">1U</span> &lt;&lt; <span class="built_in">static_cast</span>&lt;<span class="type">uint32_t</span>&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH);</span><br><span class="line">    <span class="keyword">auto</span> network = <span class="built_in">SampleUniquePtr</span>&lt;nvinfer1::INetworkDefinition&gt;(builder-&gt;<span class="built_in">createNetworkV2</span>(explicitBatch));</span><br><span class="line">    <span class="keyword">if</span> (!network)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> config = <span class="built_in">SampleUniquePtr</span>&lt;nvinfer1::IBuilderConfig&gt;(builder-&gt;<span class="built_in">createBuilderConfig</span>());</span><br><span class="line">    <span class="keyword">if</span> (!config)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">//类似第一个例子 使用onnx的解析器，只是参数不一样</span></span><br><span class="line">    <span class="keyword">auto</span> parser</span><br><span class="line">        = <span class="built_in">SampleUniquePtr</span>&lt;nvonnxparser::IParser&gt;(nvonnxparser::<span class="built_in">createParser</span>(*network, sample::gLogger.<span class="built_in">getTRTLogger</span>()));</span><br><span class="line">    <span class="keyword">if</span> (!parser)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">	</span><br><span class="line">    <span class="keyword">auto</span> constructed = <span class="built_in">constructNetwork</span>(builder, network, config, parser);<span class="comment">//区别还是这个构建网络的函数，下面列出函数内容</span></span><br><span class="line">    <span class="keyword">if</span> (!constructed)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// CUDA stream used for profiling by the builder.</span></span><br><span class="line">    <span class="keyword">auto</span> profileStream = samplesCommon::<span class="built_in">makeCudaStream</span>();</span><br><span class="line">    <span class="keyword">if</span> (!profileStream)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    config-&gt;<span class="built_in">setProfileStream</span>(*profileStream);</span><br><span class="line"></span><br><span class="line">    SampleUniquePtr&lt;IHostMemory&gt; plan&#123;builder-&gt;<span class="built_in">buildSerializedNetwork</span>(*network, *config)&#125;;</span><br><span class="line">    <span class="keyword">if</span> (!plan)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    SampleUniquePtr&lt;IRuntime&gt; runtime&#123;<span class="built_in">createInferRuntime</span>(sample::gLogger.<span class="built_in">getTRTLogger</span>())&#125;;</span><br><span class="line">    <span class="keyword">if</span> (!runtime)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    mEngine = std::<span class="built_in">shared_ptr</span>&lt;nvinfer1::ICudaEngine&gt;(</span><br><span class="line">        runtime-&gt;<span class="built_in">deserializeCudaEngine</span>(plan-&gt;<span class="built_in">data</span>(), plan-&gt;<span class="built_in">size</span>()), samplesCommon::<span class="built_in">InferDeleter</span>());</span><br><span class="line">    <span class="keyword">if</span> (!mEngine)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">ASSERT</span>(network-&gt;<span class="built_in">getNbInputs</span>() == <span class="number">1</span>);</span><br><span class="line">    mInputDims = network-&gt;<span class="built_in">getInput</span>(<span class="number">0</span>)-&gt;<span class="built_in">getDimensions</span>();</span><br><span class="line">    <span class="built_in">ASSERT</span>(mInputDims.nbDims == <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">ASSERT</span>(network-&gt;<span class="built_in">getNbOutputs</span>() == <span class="number">1</span>);</span><br><span class="line">    mOutputDims = network-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>)-&gt;<span class="built_in">getDimensions</span>();</span><br><span class="line">    <span class="built_in">ASSERT</span>(mOutputDims.nbDims == <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \brief Uses a ONNX parser to create the Onnx MNIST Network and marks the</span></span><br><span class="line"><span class="comment">//!        output layers</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \param network Pointer to the network that will be populated with the Onnx MNIST network</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="comment">//! \param builder Pointer to the engine builder</span></span><br><span class="line"><span class="comment">//!</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">SampleOnnxMNIST::constructNetwork</span><span class="params">(SampleUniquePtr&lt;nvinfer1::IBuilder&gt;&amp; builder,</span></span></span><br><span class="line"><span class="params"><span class="function">    SampleUniquePtr&lt;nvinfer1::INetworkDefinition&gt;&amp; network, SampleUniquePtr&lt;nvinfer1::IBuilderConfig&gt;&amp; config,</span></span></span><br><span class="line"><span class="params"><span class="function">    SampleUniquePtr&lt;nvonnxparser::IParser&gt;&amp; parser)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//类比第一个例子，就是使用onnx的解析器创建TensorRT的网络</span></span><br><span class="line">    <span class="keyword">auto</span> parsed = parser-&gt;<span class="built_in">parseFromFile</span>(<span class="built_in">locateFile</span>(mParams.onnxFileName, mParams.dataDirs).<span class="built_in">c_str</span>(),</span><br><span class="line">        <span class="built_in">static_cast</span>&lt;<span class="type">int</span>&gt;(sample::gLogger.<span class="built_in">getReportableSeverity</span>()));</span><br><span class="line">    <span class="keyword">if</span> (!parsed)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (mParams.fp16)</span><br><span class="line">    &#123;</span><br><span class="line">        config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kFP16);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (mParams.int8)</span><br><span class="line">    &#123;</span><br><span class="line">        config-&gt;<span class="built_in">setFlag</span>(BuilderFlag::kINT8);</span><br><span class="line">        samplesCommon::<span class="built_in">setAllDynamicRanges</span>(network.<span class="built_in">get</span>(), <span class="number">127.0f</span>, <span class="number">127.0f</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    samplesCommon::<span class="built_in">enableDLA</span>(builder.<span class="built_in">get</span>(), config.<span class="built_in">get</span>(), mParams.dlaCore);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>剩下的部分没什么说的，类比上面两个改变不大。</p>
<h1 id="博客总结"><a href="#博客总结" class="headerlink" title="博客总结"></a>博客总结</h1><p>由于官方文档不够详细，隐藏的细节比较多，参考网上的博客编写</p>
<p>参考的例子在<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/trt-samples-for-hackathon-cn">https://github.com/NVIDIA/trt-samples-for-hackathon-cn</a></p>
<p>文档在50-Resource文件夹 TensorRT教程-TRT8.2.3-V1.1.pdf</p>
<img src="/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220914131347502.png" class="" title="image-20220914131347502">
<h1 id="1-TRT流程"><a href="#1-TRT流程" class="headerlink" title="1 TRT流程"></a>1 TRT流程</h1><p>参考 <a target="_blank" rel="noopener" href="https://murphypei.github.io/blog/2019/09/trt-useage.html">https://murphypei.github.io/blog/2019/09/trt-useage.html</a></p>
<h3 id="构建模型-和-engine"><a href="#构建模型-和-engine" class="headerlink" title="构建模型 和 engine"></a>构建模型 和 engine</h3><p>TRT 将模型结构和参数以及相应 kernel 计算方法都编译成一个二进制 engine，因此在部署之后大大加快了推理速度。为了能够使用 TRT 进行推理，需要创建一个 eninge。TRT 中 engine 的创建有两种方式：</p>
<ul>
<li>通过网络模型结构和参数文件编译得到，很慢。</li>
<li>读取一个已有的 engine（gie 文件），因为跳过了模型解析等过程，速度更快。</li>
</ul>
<p>第一种方式很慢，但是在第一次部署某个模型，或者修改模型的精度、输入数据类型、网络结构等等，只要修改了模型，就必须重新编译（其实 TRT 还有一种可以重新加载参数的方式，不是本文所涉及的）。</p>
<p>现在假设我们是第一次用 TRT，所以就只能选择第一种方式来创建一个 engine。为了创建一个 engine，我们需要有模型结构和模型参数两个文件，同时需要能够解析这两个文件的方法。在 TRT 中，编译 engine 是通过 <code>IBuilder</code> 对象进行的，因此我们首先需要新键一个 <code>IBuilder</code> 对象：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">nvinfer1::IBuilder *builder = createInferBuilder(gLogger);</span><br></pre></td></tr></table></figure>
<blockquote>
<p><code>gLogger</code> 是 TRT 中的日志接口 <code>ILogger</code> ，继承这个接口并创建自己的 logger 对象传入即可。</p>
</blockquote>
<p>为了编译一个 engine，<code>builder</code> 首先需要创建一个 <code>INetworkDefinition</code> 作为模型的容器：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">nvinfer1::INetworkDefinition *network = builder-&gt;createNetwork();</span><br></pre></td></tr></table></figure>
<p>注意，<strong>此时 <code>network</code> 是空的</strong>，我们需要填充模型结构和参数，也就是解析我们自己的模型结构和参数文件，获取数据放到其中。</p>
<p><strong>TRT 官方给了三种主流框架模型格式的解析器（parser）</strong>，分别是：</p>
<ul>
<li>ONNX：<code>IOnnxParser parser = nvonnxparser::createParser(*network, gLogger);</code></li>
<li>Caffe：<code>ICaffeParser parser = nvcaffeparser1::createCaffeParser();</code></li>
<li>UFF：<code>IUffParser parser = nvuffparser::createUffParser();</code></li>
</ul>
<p>其中 UFF 是用于 TensorFlow 的格式。调用这三种解析器就可以解析相应的文件。以 <code>ICaffeParser</code> 为例，调用其 <code>parse</code> 方法来填充 <code>network</code>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">virtual const IBlobNameToTensor* nvcaffeparser1::ICaffeParser::parse(</span><br><span class="line">    const char* deploy, </span><br><span class="line">    const char * model, </span><br><span class="line">	nvinfer1::INetworkDefinition &amp;network, </span><br><span class="line">	nvinfer1::DataType weightType)</span><br><span class="line"></span><br><span class="line">//Parameters</span><br><span class="line">//deploy	    The plain text, prototxt file used to define the network configuration.</span><br><span class="line">//model	        The binaryproto Caffe model that contains the weights associated with the network.</span><br><span class="line">//network	    Network in which the CaffeParser will fill the layers.</span><br><span class="line">//weightType    The type to which the weights will transformed.</span><br></pre></td></tr></table></figure>
<p>这样就能得到一个填充好的 <code>network</code> ，就可以编译 engine 了，似乎一切都很美妙呢…</p>
<p>然而实际 TRT 并不完善，比如 TensorFlow 的很多操作并不支持，因此你传入的文件往往是根本就解析不了（深度学习框架最常见的困境之一）。因此我们需要自己去做填充 <code>network</code> 这件事，这就需要调用 TRT 中低级别的接口来创建模型结构，类似于你在 Caffe 或者 TensorFlow 中做的那样。</p>
<p>TRT 提供了较为丰富的接口让你可以直接通过这些接口创建自己的网络，比如添加一个卷积层：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">virtual IConvolutionLayer* nvinfer1::INetworkDefinition::addConvolution(ITensor &amp;input, </span><br><span class="line">                                                                        int nbOutputMaps,</span><br><span class="line">                                                                        DimsHW kernelSize,</span><br><span class="line">                                                                        Weights kernelWeights,</span><br><span class="line">                                                                        Weights biasWeights)		</span><br><span class="line"></span><br><span class="line">// Parameters</span><br><span class="line">// input	The input tensor to the convolution.</span><br><span class="line">// nbOutputMaps	The number of output feature maps for the convolution.</span><br><span class="line">// kernelSize	The HW-dimensions of the convolution kernel.</span><br><span class="line">// kernelWeights	The kernel weights for the convolution.</span><br><span class="line">// biasWeights	The optional bias weights for the convolution.</span><br></pre></td></tr></table></figure>
<p>这里的参数基本上就是和其他深度学习框架类似的意思，没有什么好讲的。就是把数据封装成 TRT  中的数据结构即可。可能和平时构建训练网络不同的地方就是需要填充好模型的参数，因为 TRT  是推理框架，参数是已知确定的。这个过程一般是读取已经训练好的模型，构造 TRT 的数据结构类型放到其中，也就是需要你自己去解析模型参数文件。</p>
<p>之所以说 TRT 的网络构造接口是<strong>较为丰富</strong>，是因为即使使用这些低级接口这样，很多操作还是没办法完成，也就是没有相应的 <code>add*</code> 方法，更何况现实业务可能还会涉及很多自定义的功能层，因此 TRT 又有了 plugin 接口，允许你自己定义一个 <code>add*</code> 的操作。其流程就是继承 <code>nvinfer1::IPluginV2</code> 接口，利用 cuda 编写一个自定义层的功能，然后继承 <code>nvinfer1::IPluginCreator</code> 编写其创建类，需要重写其虚方法 <code>createPlugin</code>。最后调用 <code>REGISTER_TENSORRT_PLUGIN</code> 宏来注册这个 plugin 就可以用了。plugin 接口的成员函数介绍。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">// 获得该自定义层的输出个数，比如 leaky relu 层的输出个数为1</span><br><span class="line">virtual int getNbOutputs() const = 0;</span><br><span class="line"></span><br><span class="line">// 得到输出 Tensor 的维数</span><br><span class="line">virtual Dims getOutputDimensions(int index, const Dims* inputs, int nbInputDims) = 0;</span><br><span class="line"></span><br><span class="line">// 配置该层的参数。该函数在 initialize() 函数之前被构造器调用。它为该层提供了一个机会，可以根据其权重、尺寸和最大批量大小来做出算法选择。</span><br><span class="line">virtual void configure(const Dims* inputDims, int nbInputs, const Dims* outputDims, int nbOutputs, int maxBatchSize) = 0;</span><br><span class="line"></span><br><span class="line">// 对该层进行初始化，在 engine 创建时被调用。</span><br><span class="line">virtual int initialize() = 0;</span><br><span class="line"></span><br><span class="line">// 该函数在 engine 被摧毁时被调用</span><br><span class="line">virtual void terminate() = 0;</span><br><span class="line"></span><br><span class="line">// 获得该层所需的临时显存大小。</span><br><span class="line">virtual size_t getWorkspaceSize(int maxBatchSize) const = 0;</span><br><span class="line"></span><br><span class="line">// 执行该层</span><br><span class="line">virtual int enqueue(int batchSize, const void*const * inputs, void** outputs, void* workspace, cudaStream_t stream) = 0;</span><br><span class="line"></span><br><span class="line">// 获得该层进行 serialization 操作所需要的内存大小</span><br><span class="line">virtual size_t getSerializationSize() = 0;</span><br><span class="line"></span><br><span class="line">// 序列化该层，根据序列化大小 getSerializationSize()，将该类的参数和额外内存空间全都写入到系列化buffer中。</span><br><span class="line">virtual void serialize(void* buffer) = 0;</span><br></pre></td></tr></table></figure>
<p>我们需要根据自己层的功能，重写这里全部或者部分函数的实现，这里有很多细节，没办法一一展开，需要自定义的时候还是需要看官方 API。</p>
<p>构建好了网络模型，就可以执行 engine 的编译了，还需要对 engine 进行一些设置。比如计算精度，支持的 batch size 等等，因为这些设置不同，编译出来的 engine 也不同。</p>
<p>TRT 支持 FP16 计算，也是官方推荐的计算精度，其设置也比简单，直接调用：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">builder-&gt;setFp16Mode(true);</span><br></pre></td></tr></table></figure>
<p>另外在设置精度的时候，还有一个设置 strict 策略的接口：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">builder-&gt;setStrictTypeConstraints(true);</span><br></pre></td></tr></table></figure>
<p>这个接口就是是否严格按照设置的精度进行类型转换，如果不设置 strict 策略，则 TRT 在某些计算中可能会选择更高精度（不影响性能）的计算类型。 </p>
<p>除了精度，还需要设置好运行的 batch size 和 workspace size：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">builder-&gt;setMaxBatchSize(batch_size);</span><br><span class="line">builder-&gt;setMaxWorkspaceSize(workspace_size);</span><br></pre></td></tr></table></figure>
<p>这里的 batch size 是运行时最大能够支持的 batch size，运行时可以选择比这个值小的 batch size，workspace 也是相对于这个最大 batch size 设置的。</p>
<p>设置好上述参数，就可以编译 engine 了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">nvinfer1::ICudaEngine *engine = builder-&gt;buildCudaEngine(*network);</span><br></pre></td></tr></table></figure>
<p>编译需要花较长时间，耐心等待。</p>
<h3 id="Engine-序列化和反序列化"><a href="#Engine-序列化和反序列化" class="headerlink" title="Engine 序列化和反序列化"></a>Engine 序列化和反序列化</h3><p>编译 engine 需要较长时间，在模型和计算精度、batch size 等均保持不变的情况下，我们可以选择保存 engine 到本地，供下次运行使用，也就是 engine 序列化。TRT 提供了很方便的序列化方法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">nvinfer1::IHostMemory *modelStream = engine-&gt;serialize();</span><br></pre></td></tr></table></figure>
<p>通过这个调用，得到的是一个二进制流，将这个流写入到一个文件中即可保存下来。</p>
<p>如果需要再次部署，可以直接反序列化保存好的文件，略过编译环节。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">IRuntime* runtime = createInferRuntime(gLogger);</span><br><span class="line">ICudaEngine* engine = runtime-&gt;deserializeCudaEngine(modelData, modelSize, nullptr);</span><br></pre></td></tr></table></figure>
<h3 id="使用-engine-进行预测"><a href="#使用-engine-进行预测" class="headerlink" title="使用 engine 进行预测"></a>使用 engine 进行预测</h3><p>有了 engine 之后就可以使用它进行 inference 了。</p>
<p>首先创建一个 inference 的 context。这个 context 类似命名空间，用于保存一个 inference 任务的变量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">IExecutionContext *context = engine-&gt;createExecutionContext();</span><br></pre></td></tr></table></figure>
<p><strong>一个 engine 可以有多个 context</strong>，也就是说一个 engine 可以同时进行多个预测任务。</p>
<p>然后就是绑定输入和输出的 index。这一步的原因在于 TRT 在 build engine  的过程中，将输入和输出映射为索引编号序列，因此我们只能通过索引编号来获取输入和输出层的信息。虽然 TRT  提供了通过名字获取索引编号的接口，但是本地保存可以方便后续操作。</p>
<p>我们可以先获取索引编号的数量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">int index_number = engine-&gt;getNbBindings();</span><br></pre></td></tr></table></figure>
<p>我们可以判断这个编号数量是不是和我们网络的输入输出之和相同，比如你有一个输入和一个输出，那么编号的数量就是2。如果不是，则说明这个 engine 是有问题的；如果没问题，我们就可以通过名字获取输入输出对应的序号：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">int input_index = engine-&gt;getBindingIndex(input_layer_name);</span><br><span class="line">int output_index = engine-&gt;getBindingIndex(output_layer_name);</span><br></pre></td></tr></table></figure>
<p>对于常见的一个输入和输出的网络，输入的索引编号就是 0，输出的索引编号就是 1，所以这一步也不是必须的。</p>
<p>接下来就需要为输入和输出层分配显存空间了。为了分配显存空间，我们需要知道输入输出的维度信息和存放的数据类型，TRT 中维度信息和数据类型的表示如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">class Dims</span><br><span class="line">&#123;</span><br><span class="line">public:</span><br><span class="line">    static const int MAX_DIMS = 8; //!&lt; The maximum number of dimensions supported for a tensor.</span><br><span class="line">    int nbDims;                    //!&lt; The number of dimensions.</span><br><span class="line">    int d[MAX_DIMS];               //!&lt; The extent of each dimension.</span><br><span class="line">    DimensionType type[MAX_DIMS];  //!&lt; The type of each dimension.</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">enum class DataType : int</span><br><span class="line">&#123;</span><br><span class="line">    kFLOAT = 0, //!&lt; FP32 format.</span><br><span class="line">    kHALF = 1,  //!&lt; FP16 format.</span><br><span class="line">    kINT8 = 2,  //!&lt; quantized INT8 format.</span><br><span class="line">    kINT32 = 3  //!&lt; INT32 format.</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>我们通过索引编号获取输入和输出的数据维度（dims）和数据类型（dtype），然后为每个输出层开辟显存空间，存放输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">for (int i = 0; i &lt; index_number; ++i)</span><br><span class="line">&#123;</span><br><span class="line">	nvinfer1::Dims dims = engine-&gt;getBindingDimensions(i);</span><br><span class="line">	nvinfer1::DataType dtype = engine-&gt;getBindingDataType(i);</span><br><span class="line">    // 获取数据长度</span><br><span class="line">    auto buff_len = std::accumulate(dims.d, dims.d + dims.nbDims, 1, std::multiplies&lt;int64_t&gt;());</span><br><span class="line">    // ...</span><br><span class="line">    // 获取数据类型大小</span><br><span class="line">    dtype_size = getTypeSize(dtype);	// 自定义函数</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 为 output 分配显存空间</span><br><span class="line">for (auto &amp;output_i : outputs)</span><br><span class="line">&#123;</span><br><span class="line">    cudaMalloc(buffer_len_i * dtype_size_i * batch_size);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>本文给出的是伪代码，仅表示逻辑，因此会涉及一些简单的自定义函数。</p>
</blockquote>
<p>至此，我们已经做好了准备工作，现在就可以把数据塞进模型进行推理了。</p>
<h3 id="前向预测"><a href="#前向预测" class="headerlink" title="前向预测"></a>前向预测</h3><p>TRT 的前向预测执行是异步的，context 通过一个 enqueue 调用来提交任务：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cudaStream_t stream;</span><br><span class="line">cudaStreamCreate(&amp;stream);</span><br><span class="line">context-&gt;enqueue(batch_size, buffers, stream, nullptr);</span><br><span class="line">cudaStreamSynchronize(stream);</span><br></pre></td></tr></table></figure>
<p>enqueue 是 TRT 的实际执行任务的函数，我们在写 plugin 的时候也需要实现这个函数接口。其中：</p>
<ul>
<li><p><code>batch_size</code>：engine 在 build 过程中传入的 <code>max_batch_size</code>。</p>
</li>
<li><p><code>buffers</code>：是一个指针数组，其下标对应的就是输入输出层的索引编号，存放的就是输入的数据指针以及输出的数据存放地址（也就是开辟的显存地址）。</p>
</li>
<li><p><code>stream</code>：stream 是 cuda 一系列顺序操作的概念。对于我们的模型来说就是将所有的模型操作按照（网络结构）指定的顺序在指定的设备上执行。</p>
<blockquote>
<p>cuda stream 是指一堆异步的 cuda 操作，他们按照 host 代码调用的顺序执行在 device 上。stream  维护了这些操作的顺序，并在所有预处理完成后允许这些操作进入工作队列，同时也可以对这些操作进行一些查询操作。这些操作包括 host 到  device 的数据传输，launch kernel 以及其他的 host 发起由 device 执行的动作。这些操作的执行总是异步的，cuda runtime 会决定这些操作合适的执行时机。我们则可以使用相应的cuda api 来保证所取得结果是在所有操作完成后获得的。<strong>同一个 stream 里的操作有严格的执行顺序</strong>，不同的 stream 则没有此限制。</p>
</blockquote>
</li>
</ul>
<p>这里需要注意，输入数据和输出数据在 buffers 数组中都是在 GPU 上的，可以通过 <code>cudaMemcpy</code> 拷贝 CPU 上的输入数据到 GPU 中（需要提前开辟一块显存来存放）。同理，输出数据也需要从 GPU 中拷贝到 CPU 中。</p>
<p>前两句创建了一个 cuda stream，最后一句则是等待这个异步 stream 执行完毕，然后从显存中将数据拷贝出来即可。</p>
<p>至此，我们就完成了 TRT 一个基本的预测流程。</p>
<img src="/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/image-20220919202315669.png" class="" title="image-20220919202315669">
<h1 id="2-最简单的TensorRT例子"><a href="#2-最简单的TensorRT例子" class="headerlink" title="2 最简单的TensorRT例子"></a>2 最简单的TensorRT例子</h1><p>下面结合<code>note/tensorRT/code/trt-samples-for-hackathon-cn-master/cookbook/01-SimpleDemo/TensorRT8.4</code>的例子进行说明</p>
<p>参考 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#c_topics">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#c_topics</a></p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>这个例子执行两次推理，只是简单的将host的内存拷贝到gpu中，执行推理后从GPU中拷贝出数据（推理没有作任何内容，因此推理前后数据不变）</p>
<ul>
<li>第一次构建一个engine文件并保存本地，然后再进行推理</li>
<li>第二次加载上面保存在本地的model.plan 这个engine文件进行推理</li>
<li>使用了Dynamic Shape 模式</li>
<li>explicit batch mode 显式批处理模式，网络指定[N,3,H,W]，因此指定的dim是4维的</li>
</ul>
<p>分为以下几个步骤</p>
<ol>
<li><p>建立Builder（引擎构建器）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IBuilder * builder = <span class="built_in">createInferBuilder</span>(gLogger);</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建Network（计算图内容）</p>
<ul>
<li>自定义了一个网络，指定输入和输出的维度</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">INetworkDefinition *  network = builder-&gt;<span class="built_in">createNetworkV2</span>(<span class="number">1U</span> &lt;&lt; <span class="built_in">int</span>(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));<span class="comment">//显式批处理模式</span></span><br><span class="line">IOptimizationProfile *profile = builder-&gt;<span class="built_in">createOptimizationProfile</span>();</span><br><span class="line">IBuilderConfig *      config  = builder-&gt;<span class="built_in">createBuilderConfig</span>();<span class="comment">//创建一个构建配置 以指定Tensorrt应该如何优化模型。</span></span><br><span class="line"></span><br><span class="line">profile-&gt;<span class="built_in">setDimensions</span>(<span class="string">&quot;foo&quot;</span>, OptProfileSelector::kMIN, <span class="built_in">Dims3</span>(<span class="number">3</span>,<span class="number">100</span>,<span class="number">200</span>);</span><br><span class="line">profile-&gt;<span class="built_in">setDimensions</span>(<span class="string">&quot;foo&quot;</span>, OptProfileSelector::kOPT, <span class="built_in">Dims3</span>(<span class="number">3</span>,<span class="number">150</span>,<span class="number">250</span>);</span><br><span class="line">profile-&gt;<span class="built_in">setDimensions</span>(<span class="string">&quot;foo&quot;</span>, OptProfileSelector::kMAX, <span class="built_in">Dims3</span>(<span class="number">3</span>,<span class="number">200</span>,<span class="number">300</span>);</span><br><span class="line"></span><br><span class="line">config-&gt;<span class="built_in">addOptimizationProfile</span>(profile)<span class="comment">//修改网络参数，例如输入的batchsize,配置config</span></span><br><span class="line"><span class="comment">//api 手动创建层或者导入</span></span><br><span class="line"><span class="comment">//这里注意两种方式，</span></span><br><span class="line"><span class="comment">//第一种方式：手动创建网络就是类似sampleMINISTAPI中调用network-&gt;addInput等创建网络结构</span></span><br><span class="line"><span class="comment">//第二种方式：类似IOnnxParser parser = nvonnxparser::createParser(*network, gLogger);使用解析器来构建网络，类似sampleMINST中的代码</span></span><br><span class="line"><span class="comment">//auto parsed = parser-&gt;parseFromFile(locateFile(mParams.onnxFileName, mParams.dataDirs).c_str(),static_cast&lt;int&gt;(sample::gLogger.getReportableSeverity()));</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>生成SerializedNetwork（网络的TRT内部表示）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IHostMemory *engineString = builder-&gt;<span class="built_in">buildSerializedNetwork</span>(*network, *config);</span><br></pre></td></tr></table></figure>
</li>
<li><p>建立Engine</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">IRuntime *runtime &#123;<span class="built_in">createInferRuntime</span>(gLogger)&#125;;</span><br><span class="line">engine = runtime-&gt;<span class="built_in">deserializeCudaEngine</span>(engineString-&gt;<span class="built_in">data</span>(), engineString-&gt;<span class="built_in">size</span>());</span><br></pre></td></tr></table></figure>
</li>
<li><p>建立Context</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IExecutionContext *context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br></pre></td></tr></table></figure>
</li>
<li><p>Buffer准备（Host端/Device端/拷贝）</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;vBufferD[i], vBindingSize[i]));</span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(vBufferD[i], vBufferH[i], vBindingSize[i], cudaMemcpyHostToDevice));<span class="comment">//将数据拷贝到gpu中</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>执行推理</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">context-&gt;<span class="built_in">executeV2</span>(vBufferD.<span class="built_in">data</span>());</span><br><span class="line"><span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(vBufferH[i], vBufferD[i], vBindingSize[i], cudaMemcpyDeviceToHost));<span class="comment">//从gpu中取出结果</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Copyright (c) 2021-2022, NVIDIA CORPORATION. All rights reserved.</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment"> * you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment"> * You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"> * See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"> * limitations under the License.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;cookbookHelper.hpp&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> nvinfer1;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> std::string trtFile &#123;<span class="string">&quot;./model.plan&quot;</span>&#125;;</span><br><span class="line"><span class="function"><span class="type">static</span> Logger     <span class="title">gLogger</span><span class="params">(ILogger::Severity::kERROR)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">run</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ICudaEngine *engine = <span class="literal">nullptr</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">access</span>(trtFile.<span class="built_in">c_str</span>(), F_OK) == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function">std::ifstream <span class="title">engineFile</span><span class="params">(trtFile, std::ios::binary)</span></span>;</span><br><span class="line">        <span class="type">long</span> <span class="type">int</span>      fsize = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        engineFile.<span class="built_in">seekg</span>(<span class="number">0</span>, engineFile.end);</span><br><span class="line">        fsize = engineFile.<span class="built_in">tellg</span>();</span><br><span class="line">        engineFile.<span class="built_in">seekg</span>(<span class="number">0</span>, engineFile.beg);</span><br><span class="line">        <span class="function">std::vector&lt;<span class="type">char</span>&gt; <span class="title">engineString</span><span class="params">(fsize)</span></span>;</span><br><span class="line">        engineFile.<span class="built_in">read</span>(engineString.<span class="built_in">data</span>(), fsize);</span><br><span class="line">        <span class="keyword">if</span> (engineString.<span class="built_in">size</span>() == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Failed getting serialized engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Succeeded getting serialized engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">		<span class="comment">//有了engin文件 如果要执行推理就需要创建runtime</span></span><br><span class="line">		<span class="comment">//反序列化plan文件</span></span><br><span class="line">        IRuntime *runtime &#123;<span class="built_in">createInferRuntime</span>(gLogger)&#125;;</span><br><span class="line">		<span class="comment">//将模型读入缓冲区后，可以对其进行反序列化以获得引擎</span></span><br><span class="line">        engine = runtime-&gt;<span class="built_in">deserializeCudaEngine</span>(engineString.<span class="built_in">data</span>(), fsize);</span><br><span class="line">        <span class="keyword">if</span> (engine == <span class="literal">nullptr</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Failed loading engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Succeeded loading engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">		<span class="comment">//首次执行并没有model.plan engine文件，因此需要创建一个engin并保存为文件model.plan</span></span><br><span class="line">		<span class="comment">//要创建构建器，首先必须实例化ILogger接口。参考 https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#c_topics</span></span><br><span class="line">		<span class="comment">//setp1：建立Builder</span></span><br><span class="line">        IBuilder *            builder = <span class="built_in">createInferBuilder</span>(gLogger);</span><br><span class="line">		<span class="comment">//step2：创建Network 显式批处理模式 空的我们需要填充模型结构和参数，也就是解析我们自己的模型结构和参数文件，获取数据放到其中。</span></span><br><span class="line">        INetworkDefinition *  network = builder-&gt;<span class="built_in">createNetworkV2</span>(<span class="number">1U</span> &lt;&lt; <span class="built_in">int</span>(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span><br><span class="line">        <span class="comment">//参考 https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#opt_profiles</span></span><br><span class="line">		<span class="comment">//自定义了一个网络 有点类似与Parser的导入功能</span></span><br><span class="line">		<span class="comment">//在构造 CudaEngine 的时候至少要有一个 IOptimizationProfile，因为每个 ExecutionContext 在使用之前都要先指定一个 IOptimizationProfile 才可以执行推理操作。</span></span><br><span class="line">		IOptimizationProfile *profile = builder-&gt;<span class="built_in">createOptimizationProfile</span>();</span><br><span class="line">		<span class="comment">//创建一个构建配置 以指定Tensorrt应该如何优化模型。</span></span><br><span class="line">        IBuilderConfig *      config  = builder-&gt;<span class="built_in">createBuilderConfig</span>();</span><br><span class="line">		<span class="comment">//应该是设置了内存大小</span></span><br><span class="line">        config-&gt;<span class="built_in">setMemoryPoolLimit</span>(MemoryPoolType::kWORKSPACE, <span class="number">1</span> &lt;&lt; <span class="number">30</span>);</span><br><span class="line">		<span class="comment">//自定义网络的实现</span></span><br><span class="line">		<span class="comment">//添加输入层</span></span><br><span class="line">		<span class="comment">//通过指定输入张量的名称、数据类型和完整维度，将输入层添加到网络中。</span></span><br><span class="line">        ITensor *inputTensor = network-&gt;<span class="built_in">addInput</span>(<span class="string">&quot;inputT0&quot;</span>, DataType::kFLOAT, Dims32 &#123;<span class="number">4</span>, &#123;<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>&#125;&#125;);</span><br><span class="line">        <span class="comment">//网络输入的维度范围以及自动调优器用于优化的维度。</span></span><br><span class="line">		profile-&gt;<span class="built_in">setDimensions</span>(inputTensor-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kMIN, Dims32 &#123;<span class="number">4</span>, &#123;<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>&#125;&#125;);</span><br><span class="line">        profile-&gt;<span class="built_in">setDimensions</span>(inputTensor-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kOPT, Dims32 &#123;<span class="number">4</span>, &#123;<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;&#125;);</span><br><span class="line">        profile-&gt;<span class="built_in">setDimensions</span>(inputTensor-&gt;<span class="built_in">getName</span>(), OptProfileSelector::kMAX, Dims32 &#123;<span class="number">4</span>, &#123;<span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>&#125;&#125;);</span><br><span class="line">        config-&gt;<span class="built_in">addOptimizationProfile</span>(profile);</span><br><span class="line">		<span class="comment">//添加标识层</span></span><br><span class="line">        IIdentityLayer *identityLayer = network-&gt;<span class="built_in">addIdentity</span>(*inputTensor);</span><br><span class="line">		<span class="comment">//标记为整个网络的输出</span></span><br><span class="line">        network-&gt;<span class="built_in">markOutput</span>(*identityLayer-&gt;<span class="built_in">getOutput</span>(<span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">		<span class="comment">//step：3 生成SerializedNetwork 配置完成了 构建engine</span></span><br><span class="line">        IHostMemory *engineString = builder-&gt;<span class="built_in">buildSerializedNetwork</span>(*network, *config);</span><br><span class="line">        <span class="keyword">if</span> (engineString == <span class="literal">nullptr</span> || engineString-&gt;<span class="built_in">size</span>() == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Failed building serialized engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Succeeded building serialized engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">		<span class="comment">//有了engin文件 如果要执行推理就需要创建runtime</span></span><br><span class="line">        IRuntime *runtime &#123;<span class="built_in">createInferRuntime</span>(gLogger)&#125;;</span><br><span class="line">		<span class="comment">//将模型读入缓冲区后，可以对其进行反序列化以获得引擎</span></span><br><span class="line">		<span class="comment">//step4 建立Engine</span></span><br><span class="line">        engine = runtime-&gt;<span class="built_in">deserializeCudaEngine</span>(engineString-&gt;<span class="built_in">data</span>(), engineString-&gt;<span class="built_in">size</span>());</span><br><span class="line">        <span class="keyword">if</span> (engine == <span class="literal">nullptr</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Failed building engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Succeeded building engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">        <span class="function">std::ofstream <span class="title">engineFile</span><span class="params">(trtFile, std::ios::binary)</span></span>;</span><br><span class="line">        <span class="keyword">if</span> (!engineFile)</span><br><span class="line">        &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Failed opening file to write&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        engineFile.<span class="built_in">write</span>(<span class="built_in">static_cast</span>&lt;<span class="type">char</span> *&gt;(engineString-&gt;<span class="built_in">data</span>()), engineString-&gt;<span class="built_in">size</span>());</span><br><span class="line">        <span class="keyword">if</span> (engineFile.<span class="built_in">fail</span>())</span><br><span class="line">        &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Failed saving .plan file!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Succeeded saving .plan file!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">//step5 建立Context</span></span><br><span class="line">    IExecutionContext *context = engine-&gt;<span class="built_in">createExecutionContext</span>();</span><br><span class="line">	<span class="comment">//在运行时，必须在选择优化概要文件之后设置输入维度。因为之前只是设定的动态范围。没有确定哪一个</span></span><br><span class="line">    context-&gt;<span class="built_in">setBindingDimensions</span>(<span class="number">0</span>, Dims32 &#123;<span class="number">4</span>, &#123;<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;&#125;);</span><br><span class="line">    std::cout &lt;&lt; std::<span class="built_in">string</span>(<span class="string">&quot;Binding all? &quot;</span>) &lt;&lt; std::<span class="built_in">string</span>(context-&gt;<span class="built_in">allInputDimensionsSpecified</span>() ? <span class="string">&quot;Yes&quot;</span> : <span class="string">&quot;No&quot;</span>) &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">//step6：Buffer准备</span></span><br><span class="line">	<span class="type">int</span> nBinding = engine-&gt;<span class="built_in">getNbBindings</span>();<span class="comment">//2</span></span><br><span class="line">    <span class="type">int</span> nInput   = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        nInput += <span class="built_in">int</span>(engine-&gt;<span class="built_in">bindingIsInput</span>(i));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> nOutput = nBinding - nInput;</span><br><span class="line">	std::cout &lt;&lt;<span class="string">&quot;nInput=&quot;</span>&lt;&lt;nInput&lt;&lt;<span class="string">&quot; nBinding=&quot;</span>&lt;&lt;nBinding&lt;&lt;<span class="string">&quot; nOutput=&quot;</span>&lt;&lt;nOutput&lt;&lt;std::endl;<span class="comment">// 1 2 1</span></span><br><span class="line">    <span class="comment">//打印输出 Bind[0]:i[0]-&gt;FP32  (2, 3, 4, 5) inputT0</span></span><br><span class="line">	<span class="comment">//Bind[1]:o[0]-&gt;FP32  (2, 3, 4, 5) (Unnamed Layer* 0) [Identity]_output</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; std::<span class="built_in">string</span>(<span class="string">&quot;Bind[&quot;</span>) &lt;&lt; i &lt;&lt; std::<span class="built_in">string</span>(i &lt; nInput ? <span class="string">&quot;]:i[&quot;</span> : <span class="string">&quot;]:o[&quot;</span>) &lt;&lt; (i &lt; nInput ? i : i - nInput) &lt;&lt; std::<span class="built_in">string</span>(<span class="string">&quot;]-&gt;&quot;</span>);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">dataTypeToString</span>(engine-&gt;<span class="built_in">getBindingDataType</span>(i)) &lt;&lt; std::<span class="built_in">string</span>(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">shapeToString</span>(context-&gt;<span class="built_in">getBindingDimensions</span>(i)) &lt;&lt; std::<span class="built_in">string</span>(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        std::cout &lt;&lt; engine-&gt;<span class="built_in">getBindingName</span>(i) &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">vBindingSize</span><span class="params">(nBinding, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        Dims32 dim  = context-&gt;<span class="built_in">getBindingDimensions</span>(i);</span><br><span class="line">        <span class="type">int</span>    size = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; dim.nbDims; ++j)<span class="comment">//nbDims=4 因为定义的输入dim是4维的</span></span><br><span class="line">        &#123;</span><br><span class="line">            size *= dim.d[j];<span class="comment">//dim的维度是2*3*4*5=120</span></span><br><span class="line">        &#125;</span><br><span class="line">        vBindingSize[i] = size * <span class="built_in">dataTypeToSize</span>(engine-&gt;<span class="built_in">getBindingDataType</span>(i));<span class="comment">//120*4 FP32是4字节</span></span><br><span class="line">		<span class="comment">//打印输出 i=0 dim.nbDims=4 vBindingSize[i]=480</span></span><br><span class="line">		<span class="comment">//i=1 dim.nbDims=4 vBindingSize[i]=480</span></span><br><span class="line">		std::cout &lt;&lt;<span class="string">&quot;i=&quot;</span>&lt;&lt;i&lt;&lt;<span class="string">&quot; dim.nbDims=&quot;</span>&lt;&lt;dim.nbDims&lt;&lt;<span class="string">&quot; vBindingSize[i]=&quot;</span>&lt;&lt;vBindingSize[i]&lt;&lt;std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::vector&lt;<span class="type">void</span> *&gt; vBufferH &#123;nBinding, <span class="literal">nullptr</span>&#125;;</span><br><span class="line">    std::vector&lt;<span class="type">void</span> *&gt; vBufferD &#123;nBinding, <span class="literal">nullptr</span>&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        vBufferH[i] = (<span class="type">void</span> *)<span class="keyword">new</span> <span class="type">char</span>[vBindingSize[i]];<span class="comment">//Host的内存</span></span><br><span class="line">        <span class="built_in">CHECK</span>(<span class="built_in">cudaMalloc</span>(&amp;vBufferD[i], vBindingSize[i]));<span class="comment">//Device内存 GPU内存</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *pData = (<span class="type">float</span> *)vBufferH[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; vBindingSize[<span class="number">0</span>] / <span class="built_in">dataTypeToSize</span>(engine-&gt;<span class="built_in">getBindingDataType</span>(<span class="number">0</span>)); ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        pData[i] = <span class="built_in">float</span>(i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nInput; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(vBufferD[i], vBufferH[i], vBindingSize[i], cudaMemcpyHostToDevice));<span class="comment">//从CPU拷贝内存到GPU 用于推理</span></span><br><span class="line">    &#125;</span><br><span class="line">	<span class="comment">//step7：执行推理</span></span><br><span class="line">    context-&gt;<span class="built_in">executeV2</span>(vBufferD.<span class="built_in">data</span>());</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = nInput; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpy</span>(vBufferH[i], vBufferD[i], vBindingSize[i], cudaMemcpyDeviceToHost));<span class="comment">//推理完成 结果从GPU拷贝到CPU</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printArrayInfomation</span>((<span class="type">float</span> *)vBufferH[i], context-&gt;<span class="built_in">getBindingDimensions</span>(i), std::<span class="built_in">string</span>(engine-&gt;<span class="built_in">getBindingName</span>(i)), <span class="literal">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">delete</span>[] vBufferH[i];</span><br><span class="line">        <span class="built_in">CHECK</span>(<span class="built_in">cudaFree</span>(vBufferD[i]));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaSetDevice</span>(<span class="number">0</span>));</span><br><span class="line">    <span class="built_in">run</span>();</span><br><span class="line">    <span class="built_in">run</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>程序输出如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br></pre></td><td class="code"><pre><span class="line">huolin@huolin:~/WorkSpace/zyd/note/tensorRT/code/trt-samples-for-hackathon-cn-master/cookbook/01-SimpleDemo/TensorRT8.4$ make test</span><br><span class="line">make clean</span><br><span class="line">make[1]: Entering directory &#x27;/home/huolin/WorkSpace/zyd/note/tensorRT/code/trt-samples-for-hackathon-cn-master/cookbook/01-SimpleDemo/TensorRT8.4&#x27;</span><br><span class="line">rm -rf ./*.d ./*.o ./*.so ./*.exe ./*.plan</span><br><span class="line">make[1]: Leaving directory &#x27;/home/huolin/WorkSpace/zyd/note/tensorRT/code/trt-samples-for-hackathon-cn-master/cookbook/01-SimpleDemo/TensorRT8.4&#x27;</span><br><span class="line">make</span><br><span class="line">make[1]: Entering directory &#x27;/home/huolin/WorkSpace/zyd/note/tensorRT/code/trt-samples-for-hackathon-cn-master/cookbook/01-SimpleDemo/TensorRT8.4&#x27;</span><br><span class="line">/usr/local/cuda/bin/nvcc -w -std=c++14 -O3 -UDEBUG -Xcompiler -fPIC -use_fast_math -I. -I/usr/local/cuda/include -I/usr/include/x86_64-linux-gnu -M -MT main.o -o main.d main.cpp</span><br><span class="line">/usr/local/cuda/bin/nvcc -w -std=c++14 -O3 -UDEBUG -Xcompiler -fPIC -use_fast_math -I. -I/usr/local/cuda/include -I/usr/include/x86_64-linux-gnu -Xcompiler -fPIC -o main.o -c main.cpp</span><br><span class="line">/usr/local/cuda/bin/nvcc -w -std=c++14 -O3 -UDEBUG -Xcompiler -fPIC -use_fast_math -L/usr/local/cuda/lib64 -lcudart -L/usr/lib/x86_64-linux-gnu -lnvinfer -o main.exe main.o</span><br><span class="line">rm main.o</span><br><span class="line">make[1]: Leaving directory &#x27;/home/huolin/WorkSpace/zyd/note/tensorRT/code/trt-samples-for-hackathon-cn-master/cookbook/01-SimpleDemo/TensorRT8.4&#x27;</span><br><span class="line">rm -rf ./*.plan</span><br><span class="line">././main.exe</span><br><span class="line">Succeeded building serialized engine!</span><br><span class="line">Succeeded building engine!</span><br><span class="line">Succeeded saving .plan file!</span><br><span class="line">Binding all? Yes</span><br><span class="line">nInput=1 nBinding=2 nOutput=1</span><br><span class="line">Bind[0]:i[0]-&gt;FP32  (2, 3, 4, 5) inputT0</span><br><span class="line">Bind[1]:o[0]-&gt;FP32  (2, 3, 4, 5) (Unnamed Layer* 0) [Identity]_output</span><br><span class="line">i=0 dim.nbDims=4 vBindingSize[i]=480</span><br><span class="line">i=1 dim.nbDims=4 vBindingSize[i]=480</span><br><span class="line"></span><br><span class="line">inputT0: (2, 3, 4, 5, )</span><br><span class="line">absSum=7140.0000,mean=59.5000,var=1199.9167,max=119.0000,min= 0.0000,diff=119.0000,</span><br><span class="line"> 0.00000,  1.00000,  2.00000,  3.00000,  4.00000,  5.00000,  6.00000,  7.00000,  8.00000,  9.00000, </span><br><span class="line">110.00000, 111.00000, 112.00000, 113.00000, 114.00000, 115.00000, 116.00000, 117.00000, 118.00000, 119.00000, </span><br><span class="line"> 0.000  1.000  2.000  3.000  4.000 </span><br><span class="line"> 5.000  6.000  7.000  8.000  9.000 </span><br><span class="line">10.000 11.000 12.000 13.000 14.000 </span><br><span class="line">15.000 16.000 17.000 18.000 19.000 </span><br><span class="line"></span><br><span class="line">20.000 21.000 22.000 23.000 24.000 </span><br><span class="line">25.000 26.000 27.000 28.000 29.000 </span><br><span class="line">30.000 31.000 32.000 33.000 34.000 </span><br><span class="line">35.000 36.000 37.000 38.000 39.000 </span><br><span class="line"></span><br><span class="line">40.000 41.000 42.000 43.000 44.000 </span><br><span class="line">45.000 46.000 47.000 48.000 49.000 </span><br><span class="line">50.000 51.000 52.000 53.000 54.000 </span><br><span class="line">55.000 56.000 57.000 58.000 59.000 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">60.000 61.000 62.000 63.000 64.000 </span><br><span class="line">65.000 66.000 67.000 68.000 69.000 </span><br><span class="line">70.000 71.000 72.000 73.000 74.000 </span><br><span class="line">75.000 76.000 77.000 78.000 79.000 </span><br><span class="line"></span><br><span class="line">80.000 81.000 82.000 83.000 84.000 </span><br><span class="line">85.000 86.000 87.000 88.000 89.000 </span><br><span class="line">90.000 91.000 92.000 93.000 94.000 </span><br><span class="line">95.000 96.000 97.000 98.000 99.000 </span><br><span class="line"></span><br><span class="line">100.000 101.000 102.000 103.000 104.000 </span><br><span class="line">105.000 106.000 107.000 108.000 109.000 </span><br><span class="line">110.000 111.000 112.000 113.000 114.000 </span><br><span class="line">115.000 116.000 117.000 118.000 119.000 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(Unnamed Layer* 0) [Identity]_output: (2, 3, 4, 5, )</span><br><span class="line">absSum=7140.0000,mean=59.5000,var=1199.9167,max=119.0000,min= 0.0000,diff=119.0000,</span><br><span class="line"> 0.00000,  1.00000,  2.00000,  3.00000,  4.00000,  5.00000,  6.00000,  7.00000,  8.00000,  9.00000, </span><br><span class="line">110.00000, 111.00000, 112.00000, 113.00000, 114.00000, 115.00000, 116.00000, 117.00000, 118.00000, 119.00000, </span><br><span class="line"> 0.000  1.000  2.000  3.000  4.000 </span><br><span class="line"> 5.000  6.000  7.000  8.000  9.000 </span><br><span class="line">10.000 11.000 12.000 13.000 14.000 </span><br><span class="line">15.000 16.000 17.000 18.000 19.000 </span><br><span class="line"></span><br><span class="line">20.000 21.000 22.000 23.000 24.000 </span><br><span class="line">25.000 26.000 27.000 28.000 29.000 </span><br><span class="line">30.000 31.000 32.000 33.000 34.000 </span><br><span class="line">35.000 36.000 37.000 38.000 39.000 </span><br><span class="line"></span><br><span class="line">40.000 41.000 42.000 43.000 44.000 </span><br><span class="line">45.000 46.000 47.000 48.000 49.000 </span><br><span class="line">50.000 51.000 52.000 53.000 54.000 </span><br><span class="line">55.000 56.000 57.000 58.000 59.000 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">60.000 61.000 62.000 63.000 64.000 </span><br><span class="line">65.000 66.000 67.000 68.000 69.000 </span><br><span class="line">70.000 71.000 72.000 73.000 74.000 </span><br><span class="line">75.000 76.000 77.000 78.000 79.000 </span><br><span class="line"></span><br><span class="line">80.000 81.000 82.000 83.000 84.000 </span><br><span class="line">85.000 86.000 87.000 88.000 89.000 </span><br><span class="line">90.000 91.000 92.000 93.000 94.000 </span><br><span class="line">95.000 96.000 97.000 98.000 99.000 </span><br><span class="line"></span><br><span class="line">100.000 101.000 102.000 103.000 104.000 </span><br><span class="line">105.000 106.000 107.000 108.000 109.000 </span><br><span class="line">110.000 111.000 112.000 113.000 114.000 </span><br><span class="line">115.000 116.000 117.000 118.000 119.000 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Succeeded getting serialized engine!</span><br><span class="line">Succeeded loading engine!</span><br><span class="line">Binding all? Yes</span><br><span class="line">nInput=1 nBinding=2 nOutput=1</span><br><span class="line">Bind[0]:i[0]-&gt;FP32  (2, 3, 4, 5) inputT0</span><br><span class="line">Bind[1]:o[0]-&gt;FP32  (2, 3, 4, 5) (Unnamed Layer* 0) [Identity]_output</span><br><span class="line">i=0 dim.nbDims=4 vBindingSize[i]=480</span><br><span class="line">i=1 dim.nbDims=4 vBindingSize[i]=480</span><br><span class="line"></span><br><span class="line">inputT0: (2, 3, 4, 5, )</span><br><span class="line">absSum=7140.0000,mean=59.5000,var=1199.9167,max=119.0000,min= 0.0000,diff=119.0000,</span><br><span class="line"> 0.00000,  1.00000,  2.00000,  3.00000,  4.00000,  5.00000,  6.00000,  7.00000,  8.00000,  9.00000, </span><br><span class="line">110.00000, 111.00000, 112.00000, 113.00000, 114.00000, 115.00000, 116.00000, 117.00000, 118.00000, 119.00000, </span><br><span class="line"> 0.000  1.000  2.000  3.000  4.000 </span><br><span class="line"> 5.000  6.000  7.000  8.000  9.000 </span><br><span class="line">10.000 11.000 12.000 13.000 14.000 </span><br><span class="line">15.000 16.000 17.000 18.000 19.000 </span><br><span class="line"></span><br><span class="line">20.000 21.000 22.000 23.000 24.000 </span><br><span class="line">25.000 26.000 27.000 28.000 29.000 </span><br><span class="line">30.000 31.000 32.000 33.000 34.000 </span><br><span class="line">35.000 36.000 37.000 38.000 39.000 </span><br><span class="line"></span><br><span class="line">40.000 41.000 42.000 43.000 44.000 </span><br><span class="line">45.000 46.000 47.000 48.000 49.000 </span><br><span class="line">50.000 51.000 52.000 53.000 54.000 </span><br><span class="line">55.000 56.000 57.000 58.000 59.000 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">60.000 61.000 62.000 63.000 64.000 </span><br><span class="line">65.000 66.000 67.000 68.000 69.000 </span><br><span class="line">70.000 71.000 72.000 73.000 74.000 </span><br><span class="line">75.000 76.000 77.000 78.000 79.000 </span><br><span class="line"></span><br><span class="line">80.000 81.000 82.000 83.000 84.000 </span><br><span class="line">85.000 86.000 87.000 88.000 89.000 </span><br><span class="line">90.000 91.000 92.000 93.000 94.000 </span><br><span class="line">95.000 96.000 97.000 98.000 99.000 </span><br><span class="line"></span><br><span class="line">100.000 101.000 102.000 103.000 104.000 </span><br><span class="line">105.000 106.000 107.000 108.000 109.000 </span><br><span class="line">110.000 111.000 112.000 113.000 114.000 </span><br><span class="line">115.000 116.000 117.000 118.000 119.000 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(Unnamed Layer* 0) [Identity]_output: (2, 3, 4, 5, )</span><br><span class="line">absSum=7140.0000,mean=59.5000,var=1199.9167,max=119.0000,min= 0.0000,diff=119.0000,</span><br><span class="line"> 0.00000,  1.00000,  2.00000,  3.00000,  4.00000,  5.00000,  6.00000,  7.00000,  8.00000,  9.00000, </span><br><span class="line">110.00000, 111.00000, 112.00000, 113.00000, 114.00000, 115.00000, 116.00000, 117.00000, 118.00000, 119.00000, </span><br><span class="line"> 0.000  1.000  2.000  3.000  4.000 </span><br><span class="line"> 5.000  6.000  7.000  8.000  9.000 </span><br><span class="line">10.000 11.000 12.000 13.000 14.000 </span><br><span class="line">15.000 16.000 17.000 18.000 19.000 </span><br><span class="line"></span><br><span class="line">20.000 21.000 22.000 23.000 24.000 </span><br><span class="line">25.000 26.000 27.000 28.000 29.000 </span><br><span class="line">30.000 31.000 32.000 33.000 34.000 </span><br><span class="line">35.000 36.000 37.000 38.000 39.000 </span><br><span class="line"></span><br><span class="line">40.000 41.000 42.000 43.000 44.000 </span><br><span class="line">45.000 46.000 47.000 48.000 49.000 </span><br><span class="line">50.000 51.000 52.000 53.000 54.000 </span><br><span class="line">55.000 56.000 57.000 58.000 59.000 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">60.000 61.000 62.000 63.000 64.000 </span><br><span class="line">65.000 66.000 67.000 68.000 69.000 </span><br><span class="line">70.000 71.000 72.000 73.000 74.000 </span><br><span class="line">75.000 76.000 77.000 78.000 79.000 </span><br><span class="line"></span><br><span class="line">80.000 81.000 82.000 83.000 84.000 </span><br><span class="line">85.000 86.000 87.000 88.000 89.000 </span><br><span class="line">90.000 91.000 92.000 93.000 94.000 </span><br><span class="line">95.000 96.000 97.000 98.000 99.000 </span><br><span class="line"></span><br><span class="line">100.000 101.000 102.000 103.000 104.000 </span><br><span class="line">105.000 106.000 107.000 108.000 109.000 </span><br><span class="line">110.000 111.000 112.000 113.000 114.000 </span><br><span class="line">115.000 116.000 117.000 118.000 119.000 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="TIPS"><a href="#TIPS" class="headerlink" title="TIPS:"></a>TIPS:</h1><p>在<code>tensorrt/samples/common/sampleEngines.cpp</code>文件中有保存和加载engine的操作函数。</p>
<h2 id="saveEngine"><a href="#saveEngine" class="headerlink" title="saveEngine"></a>saveEngine</h2><p>用于保存engine文件，可以根据自己的需求进行修改</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">saveEngine</span><span class="params">(<span class="type">const</span> ICudaEngine&amp; engine, std::string <span class="type">const</span>&amp; fileName, std::ostream&amp; err)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">std::ofstream <span class="title">engineFile</span><span class="params">(fileName, std::ios::binary)</span></span>;</span><br><span class="line">    <span class="keyword">if</span> (!engineFile)</span><br><span class="line">    &#123;</span><br><span class="line">        err &lt;&lt; <span class="string">&quot;Cannot open engine file: &quot;</span> &lt;&lt; fileName &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::unique_ptr&lt;IHostMemory&gt; serializedEngine&#123;engine.<span class="built_in">serialize</span>()&#125;;</span><br><span class="line">    <span class="keyword">if</span> (serializedEngine == <span class="literal">nullptr</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        err &lt;&lt; <span class="string">&quot;Engine serialization failed&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    engineFile.<span class="built_in">write</span>(<span class="built_in">static_cast</span>&lt;<span class="type">char</span>*&gt;(serializedEngine-&gt;<span class="built_in">data</span>()), serializedEngine-&gt;<span class="built_in">size</span>());</span><br><span class="line">    <span class="keyword">return</span> !engineFile.<span class="built_in">fail</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="loadEngine"><a href="#loadEngine" class="headerlink" title="loadEngine"></a>loadEngine</h2><p>用于加载engine文件，根据自己需求修改</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">   <span class="keyword">struct</span> <span class="title class_">stat</span> my_stat;</span><br><span class="line"><span class="type">char</span>* filename = <span class="string">&quot;./onnx_engine.engine&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">stat</span>(filename, &amp;my_stat) == <span class="number">0</span>) </span><br><span class="line">   &#123;</span><br><span class="line">	<span class="built_in">initLibNvInferPlugins</span>(&amp;gLogger.<span class="built_in">getTRTLogger</span>(), <span class="string">&quot;&quot;</span>);</span><br><span class="line"></span><br><span class="line">	<span class="function">std::ifstream <span class="title">engineFile</span><span class="params">(filename, std::ios::binary)</span></span>;</span><br><span class="line">	<span class="keyword">if</span> (!engineFile)</span><br><span class="line">	&#123;</span><br><span class="line">		std::cout &lt;&lt; <span class="string">&quot;Error opening engine file: &quot;</span> &lt;&lt; filename &lt;&lt; std::endl;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	engineFile.<span class="built_in">seekg</span>(<span class="number">0</span>, engineFile.end);</span><br><span class="line">	<span class="type">long</span> <span class="type">int</span> fsize = engineFile.<span class="built_in">tellg</span>();</span><br><span class="line">	engineFile.<span class="built_in">seekg</span>(<span class="number">0</span>, engineFile.beg);</span><br><span class="line"></span><br><span class="line">	<span class="function">std::vector&lt;<span class="type">char</span>&gt; <span class="title">engineData</span><span class="params">(fsize)</span></span>;</span><br><span class="line">	engineFile.<span class="built_in">read</span>(engineData.<span class="built_in">data</span>(), fsize);</span><br><span class="line"></span><br><span class="line">	SampleUniquePtr&lt;IRuntime&gt; runtime&#123; <span class="built_in">createInferRuntime</span>(gLogger.<span class="built_in">getTRTLogger</span>()) &#125;;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">auto</span> mEngine = runtime-&gt;<span class="built_in">deserializeCudaEngine</span>(engineData.<span class="built_in">data</span>(), engineData.<span class="built_in">size</span>(), <span class="literal">nullptr</span>);</span><br><span class="line"></span><br><span class="line">	nvinfer1::Dims mInputDims = mEngine-&gt;<span class="built_in">getBindingDimensions</span>(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h2 id="caffemodel"><a href="#caffemodel" class="headerlink" title="caffemodel"></a>caffemodel</h2><p>里不仅存储了权重和偏置等信息，还存储了整个训练网络的结构信息，即.prototxt信息</p>
<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#c_samples_section">官方</a></p>
<p>API参考 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_network_definition.html#a00980627b900c6fe4b293e0f8206077c">https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_network_definition.html#a00980627b900c6fe4b293e0f8206077c</a></p>
<p>官方例子说明 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#samples">https://docs.nvidia.com/deeplearning/tensorrt/sample-support-guide/index.html#samples</a></p>
<p>官方例子git <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/main/samples">https://github.com/NVIDIA/TensorRT/tree/main/samples</a></p>
<p>官方开发指南 <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#create_network_c">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#create_network_c</a></p>
<p>sampleMNIST 参考博客</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.freesion.com/article/3453987630/">https://www.freesion.com/article/3453987630/</a> </li>
<li><a target="_blank" rel="noopener" href="https://blog.51cto.com/u_15490502/5220171">https://blog.51cto.com/u_15490502/5220171</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/HaoBBNuanMM/article/details/102841685">https://blog.csdn.net/HaoBBNuanMM/article/details/102841685</a></li>
<li><a target="_blank" rel="noopener" href="http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/AI%20%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%85%B6%E5%AE%83%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/TensorRT/%E4%B8%89%EF%BC%8C%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorRT%20C%2B%2B%20API%E6%90%AD%E5%BB%BA%E7%BD%91%E7%BB%9C/">http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/AI%20%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%85%B6%E5%AE%83%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/TensorRT/%E4%B8%89%EF%BC%8C%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8tensorRT%20C%2B%2B%20API%E6%90%AD%E5%BB%BA%E7%BD%91%E7%BB%9C/</a></li>
</ul>
<p>非常好的实践例子 <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/trt-samples-for-hackathon-cn">https://github.com/NVIDIA/trt-samples-for-hackathon-cn</a></p>
<h1 id="编写"><a href="#编写" class="headerlink" title="编写"></a>编写</h1><p>zyd</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>奔跑的IC
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://example.com/TensorRT/TensorRT%E4%BE%8B%E5%AD%90/" title="TensorRT例子">http://example.com/TensorRT/TensorRT例子/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/git/" rel="tag"><i class="fa fa-tag"></i> git</a>
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C</a>
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C++</a>
              <a href="/tags/GPU/" rel="tag"><i class="fa fa-tag"></i> GPU</a>
              <a href="/tags/Tensorrt/" rel="tag"><i class="fa fa-tag"></i> Tensorrt</a>
              <a href="/tags/DeepLearning/" rel="tag"><i class="fa fa-tag"></i> DeepLearning</a>
              <a href="/tags/Plugin/" rel="tag"><i class="fa fa-tag"></i> Plugin</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/README/" rel="prev" title="README">
                  <i class="fa fa-chevron-left"></i> README
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/" rel="next" title="TensorRT快速开始">
                  TensorRT快速开始 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">奔跑的IC</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"zmurder","repo":"zmurder.github.io","client_id":"cf2343f27b6c29efe0bc","client_secret":"3268a1fa92706c7358d5421f88f76a0f7ada3188","admin_user":"zmurder","distraction_free_mode":true,"proxy":"https://strong-caramel-969805.netlify.app/github_access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"a38bd13bd8ff2e3d67125edd889ac66f"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
