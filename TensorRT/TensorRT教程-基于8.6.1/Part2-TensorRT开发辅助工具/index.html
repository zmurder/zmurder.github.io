<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
  <meta name="msvalidate.01" content="7EC20DBC74B004C2782077570E15C280">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
    <meta name="description" content="1 背景本文档是记录学习Nvidia官方B站的视频，参考对应的PDF文件 TensorRTTraining-TRT8.6.1-Part5-V1.1.pdf 的记录。对应的官方代码[trt-samples-for-hackathon-cn]">
<meta property="og:type" content="article">
<meta property="og:title" content="Part2-TensorRT开发辅助工具">
<meta property="og:url" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/index.html">
<meta property="og:site_name" content="奔跑的IC">
<meta property="og:description" content="1 背景本文档是记录学习Nvidia官方B站的视频，参考对应的PDF文件 TensorRTTraining-TRT8.6.1-Part5-V1.1.pdf 的记录。对应的官方代码[trt-samples-for-hackathon-cn]">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241022204349489.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/trtexec.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241023164742091.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241023165629568.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024110529480.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024100423440.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024100731267.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024100812905.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024101051929.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024101313424.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024101511418.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024101612206.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024101641401.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024101707816.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024091135885.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024091234428.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024092022891.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024092200932.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024093553154.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024093636233.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024093656626.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024093756458.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024093940935.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024094013676.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024094039101.png">
<meta property="article:published_time" content="2024-12-01T10:13:45.452Z">
<meta property="article:modified_time" content="2024-12-01T10:13:45.452Z">
<meta property="article:author" content="奔跑的IC">
<meta property="article:tag" content="git">
<meta property="article:tag" content="C">
<meta property="article:tag" content="GPU">
<meta property="article:tag" content="Tensorrt">
<meta property="article:tag" content="DeepLearning">
<meta property="article:tag" content="Plugin">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241022204349489.png">


<link rel="canonical" href="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/","path":"TensorRT/TensorRT教程-基于8.6.1/Part2-TensorRT开发辅助工具/","title":"Part2-TensorRT开发辅助工具"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Part2-TensorRT开发辅助工具 | 奔跑的IC</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">奔跑的IC</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-reorder fa-fw"></i>文章列表</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E8%83%8C%E6%99%AF"><span class="nav-text">1 背景</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7"><span class="nav-text">2 开发辅助工具</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-trtexec"><span class="nav-text">2.1 trtexec</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-trtexec-%E5%B8%B8%E7%94%A8%E9%80%89%E9%A1%B9"><span class="nav-text">2.2 trtexec 常用选项</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-1-%E6%9E%84%E5%BB%BA%E9%98%B6%E6%AE%B5"><span class="nav-text">2.2.1 构建阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-2-%E8%BF%90%E8%A1%8C%E9%98%B6%E6%AE%B5"><span class="nav-text">2.2.2 运行阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-3-%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C"><span class="nav-text">2.2.3 性能测试结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-Netron"><span class="nav-text">2.3 Netron</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-onnx-graphsurgeon"><span class="nav-text">2.4 onnx-graphsurgeon</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-1-CreateModel"><span class="nav-text">2.4.1 CreateModel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-2-AddNode"><span class="nav-text">2.4.2 AddNode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-3-RemoveNode"><span class="nav-text">2.4.3 RemoveNode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-4-ReplaceNode"><span class="nav-text">2.4.4 ReplaceNode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-5-PrintGraphInformation"><span class="nav-text">2.4.5 PrintGraphInformation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-6-Fold"><span class="nav-text">2.4.6 Fold</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-7-ShapeOperationAndSimplify"><span class="nav-text">2.4.7 ShapeOperationAndSimplify</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-8-IsolateSubgraph"><span class="nav-text">2.4.8 IsolateSubgraph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-9-BuildModelWithAPI"><span class="nav-text">2.4.9 BuildModelWithAPI</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-polygraphy"><span class="nav-text">2.5 polygraphy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-1-run%E6%A8%A1%E5%BC%8F"><span class="nav-text">2.5.1 run模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-2-inspect-%E6%A8%A1%E5%BC%8F"><span class="nav-text">2.5.2 inspect 模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-3-surgeon-%E6%A8%A1%E5%BC%8F"><span class="nav-text">2.5.3 surgeon 模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-4-%E5%85%B6%E4%BB%96%E6%A8%A1%E5%BC%8F"><span class="nav-text">2.5.4 其他模式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-nsight-systems"><span class="nav-text">2.6 nsight systems</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-text">附录</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="奔跑的IC"
      src="/images/zyd.gif">
  <p class="site-author-name" itemprop="name">奔跑的IC</p>
  <div class="site-description" itemprop="description">死磕牛角的IT农民工</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">186</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zyd.gif">
      <meta itemprop="name" content="奔跑的IC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="奔跑的IC">
      <meta itemprop="description" content="死磕牛角的IT农民工">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Part2-TensorRT开发辅助工具 | 奔跑的IC">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Part2-TensorRT开发辅助工具
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-01 18:13:45" itemprop="dateCreated datePublished" datetime="2024-12-01T18:13:45+08:00">2024-12-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TensorRT/" itemprop="url" rel="index"><span itemprop="name">TensorRT</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8-6-1/" itemprop="url" rel="index"><span itemprop="name">TensorRT教程-基于8.6.1</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h1><p>本文档是记录学习Nvidia官方B站的<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1jj411Z7wG?spm_id_from=333.788.videopod.sections&amp;vd_source=cde2e7b9bca1a7048a13eaf0b48210b6">视频</a>，参考对应的PDF文件 TensorRTTraining-TRT8.6.1-Part5-V1.1.pdf 的记录。对应的官方代码<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/trt-samples-for-hackathon-cn">[trt-samples-for-hackathon-cn]</a></p>
<p>官方的视频教程基于TensorRT8.6.1版本。但是官方代码没有对应的tag。只有8.4、8.5和截至目前最新的8.10（master分支）。因此我这里参考的都是8.4分支的代码。</p>
<ul>
<li>part1 TensorRT简介</li>
<li>part2 开发辅助工具</li>
<li>part3 插件书写</li>
<li>part4 TensorRT高级用法</li>
<li>part5 常见优化策略</li>
</ul>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241022204349489.png" class="" title="image-20241022204349489">
<h1 id="2-开发辅助工具"><a href="#2-开发辅助工具" class="headerlink" title="2 开发辅助工具"></a>2 开发辅助工具</h1><ul>
<li><p>trtexec :TensorRT 命令行工具，主要的 End2End 性能测试工具</p>
</li>
<li><p>Netron :网络可视化</p>
</li>
<li><p>onnx-graphsurgeon :onnx 计算图编辑</p>
</li>
<li><p>polygraphy :结果验证与定位，图优化</p>
</li>
<li><p>Nsight Systems :性能分析</p>
</li>
</ul>
<p>希望解决的问题</p>
<ul>
<li><p>不想写脚本来跑 TensorRT： trtexec用命令行搞定</p>
</li>
<li><p>怎么进行简单的推理性能测试？： trtexec测量延迟、吞吐量等</p>
</li>
<li><p>网络结构可视化？：Netron</p>
</li>
<li><p>计算图上有些节点阻碍 TensorRT 自动优化 ：onnx-graphsurgeon手工调整以便 TensorRT 能够处理</p>
</li>
<li><p>怎么处理 TensorRT 不支持的网络结构？ :onnx-graphsurgeon手工调整以便 TensorRT 能够处理</p>
</li>
<li><p>怎么检验 TensorRT 上计算结果正确性 / 精度？ :polygraphy同时在原框架和 TensorRT 上运行</p>
</li>
<li><p>怎么找出计算错误 / 精度不足的层？ :polygraphy模型逐层比较</p>
</li>
<li><p>怎么进行简单的计算图优化？ :polygraphy手工调整</p>
</li>
<li><p>怎样找出最耗时的层？ :nsight system找到热点集中优化</p>
</li>
</ul>
<h2 id="2-1-trtexec"><a href="#2-1-trtexec" class="headerlink" title="2.1 trtexec"></a>2.1 <strong>trtexec</strong></h2><ul>
<li><p>TensorRT 的命令行工具，</p>
<ul>
<li><p>随 TensorRT 安装，位于 tensorrt-XX/bin/trtexec</p>
</li>
<li><p>docker 中位于：/opt/tensorrt/bin/</p>
</li>
</ul>
</li>
<li><p>功能</p>
<ul>
<li><p>由 ONNX 文件生成 TensorRT 引擎并序列化为 Plan 文件</p>
</li>
<li><p>查看 ONNX 文件或 Plan 文件的网络逐层信息</p>
</li>
<li><p>模型性能测试（测试 TensorRT 引擎基于随机输入或给定输入下的性能）</p>
</li>
</ul>
</li>
</ul>
<p>一些简单的指令</p>
<ul>
<li>07-Tool/trtexec，运行 ./command.sh</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 02-用上面的 .onnx 构建一个 TensorRT 引擎并作推理</span></span><br><span class="line">trtexec \</span><br><span class="line">    --onnx=model.onnx \</span><br><span class="line">    --minShapes=tensor-0:1x1x28x28 \</span><br><span class="line">    --optShapes=tensor-0:4x1x28x28 \</span><br><span class="line">    --maxShapes=tensor-0:16x1x28x28 \</span><br><span class="line">    --memPoolSize=workspace:1024MiB \</span><br><span class="line">    --saveEngine=model-FP32.plan \</span><br><span class="line">    --shapes=tensor-0:4x1x28x28 \</span><br><span class="line">    --verbose \</span><br><span class="line">    &gt; result-FP32.<span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意参数名和格式跟 polygrapy 不一样，多个形状之间用逗号分隔，如：</span></span><br><span class="line"><span class="comment"># --minShapes=tensor-0:16x320x256,tensor-1:16x320,tensor-2:16</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 03-用上面的 .onnx 构建一个 TensorRT 引擎并作推理，使用 FP16 模式</span></span><br><span class="line">trtexec \</span><br><span class="line">    --onnx=model.onnx \</span><br><span class="line">    --minShapes=tensor-0:1x1x28x28 \</span><br><span class="line">    --optShapes=tensor-0:4x1x28x28 \</span><br><span class="line">    --maxShapes=tensor-0:16x1x28x28 \</span><br><span class="line">    --memPoolSize=workspace:1024MiB \</span><br><span class="line">    --saveEngine=model-FP16.plan \</span><br><span class="line">    --shapes=tensor-0:4x1x28x28 \</span><br><span class="line">    --verbose \</span><br><span class="line">    --fp16 \</span><br><span class="line">    &gt; result-FP16.<span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 04-读取上面构建的 result-FP32.plan 并作推理</span></span><br><span class="line">trtexec \</span><br><span class="line">    --loadEngine=./model-FP32.plan \</span><br><span class="line">    --shapes=tensor-0:4x1x28x28 \</span><br><span class="line">    --verbose \</span><br><span class="line">    &gt; result-loadAndInference.<span class="built_in">log</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 05-读取上面构建的 result-FP32.plan 打印引擎的详细信息（since TRT8.4）</span></span><br><span class="line">trtexec \</span><br><span class="line">    --loadEngine=./model-FP32.plan \</span><br><span class="line">    --shapes=tensor-0:4x1x28x28 \</span><br><span class="line">    --verbose \</span><br><span class="line">    --dumpLayerInfo \</span><br><span class="line">    --exportLayerInfo=<span class="string">&quot;./modelInformation.log&quot;</span> \</span><br><span class="line">    &gt; result-PrintInformation.<span class="built_in">log</span></span><br></pre></td></tr></table></figure>
<h2 id="2-2-trtexec-常用选项"><a href="#2-2-trtexec-常用选项" class="headerlink" title="2.2 trtexec 常用选项"></a>2.2 trtexec 常用选项</h2><p>我这里列举出来tensorRT8.4在orin上的help</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line">root@tegra-ubuntu:~<span class="comment"># trtexec --help</span></span><br><span class="line">&amp;&amp;&amp;&amp; RUNNING TensorRT.trtexec [TensorRT v8412] <span class="comment"># trtexec --help</span></span><br><span class="line">=== Model Options ===</span><br><span class="line">  --uff=&lt;file&gt;                UFF model</span><br><span class="line">  --onnx=&lt;file&gt;               ONNX model</span><br><span class="line">  --model=&lt;file&gt;              Caffe model (default = no model, random weights used)</span><br><span class="line">  --deploy=&lt;file&gt;             Caffe prototxt file</span><br><span class="line">  --output=&lt;name&gt;[,&lt;name&gt;]*   Output names (it can be specified multiple <span class="built_in">times</span>); at least one output is required <span class="keyword">for</span> UFF and Caffe</span><br><span class="line">  --uffInput=&lt;name&gt;,X,Y,Z     Input blob name and its dimensions (X,Y,Z=C,H,W), it can be specified multiple <span class="built_in">times</span>; at least one is required <span class="keyword">for</span> UFF models</span><br><span class="line">  --uffNHWC                   Set <span class="keyword">if</span> inputs are <span class="keyword">in</span> the NHWC layout instead of NCHW (use X,Y,Z=H,W,C order <span class="keyword">in</span> --uffInput)</span><br><span class="line"></span><br><span class="line">=== Build Options ===</span><br><span class="line">  --maxBatch                  Set max batch size and build an implicit batch engine (default = same size as --batch)</span><br><span class="line">                              This option should not be used when the input model is ONNX or when dynamic shapes are provided.</span><br><span class="line">  --minShapes=spec            Build with dynamic shapes using a profile with the min shapes provided</span><br><span class="line">  --optShapes=spec            Build with dynamic shapes using a profile with the opt shapes provided</span><br><span class="line">  --maxShapes=spec            Build with dynamic shapes using a profile with the max shapes provided</span><br><span class="line">  --minShapesCalib=spec       Calibrate with dynamic shapes using a profile with the min shapes provided</span><br><span class="line">  --optShapesCalib=spec       Calibrate with dynamic shapes using a profile with the opt shapes provided</span><br><span class="line">  --maxShapesCalib=spec       Calibrate with dynamic shapes using a profile with the max shapes provided</span><br><span class="line">                              Note: All three of min, opt and max shapes must be supplied.</span><br><span class="line">                                    However, <span class="keyword">if</span> only opt shapes is supplied <span class="keyword">then</span> it will be expanded so</span><br><span class="line">                                    that min shapes and max shapes are <span class="built_in">set</span> to the same values as opt shapes.</span><br><span class="line">                                    Input names can be wrapped with escaped single quotes (ex: \<span class="string">&#x27;Input:0\&#x27;</span>).</span><br><span class="line">                              Example input shapes spec: input0:1x3x256x256,input1:1x3x128x128</span><br><span class="line">                              Each input shape is supplied as a key-value pair <span class="built_in">where</span> key is the input name and</span><br><span class="line">                              value is the dimensions (including the batch dimension) to be used <span class="keyword">for</span> that input.</span><br><span class="line">                              Each key-value pair has the key and value separated using a colon (:).</span><br><span class="line">                              Multiple input shapes can be provided via comma-separated key-value pairs.</span><br><span class="line">  --inputIOFormats=spec       Type and format of each of the input tensors (default = all inputs <span class="keyword">in</span> fp32:chw)</span><br><span class="line">                              See --outputIOFormats <span class="built_in">help</span> <span class="keyword">for</span> the grammar of <span class="built_in">type</span> and format list.</span><br><span class="line">                              Note: If this option is specified, please <span class="built_in">set</span> comma-separated types and formats <span class="keyword">for</span> all</span><br><span class="line">                                    inputs following the same order as network inputs ID (even <span class="keyword">if</span> only one input</span><br><span class="line">                                    needs specifying IO format) or <span class="built_in">set</span> the <span class="built_in">type</span> and format once <span class="keyword">for</span> broadcasting.</span><br><span class="line">  --outputIOFormats=spec      Type and format of each of the output tensors (default = all outputs <span class="keyword">in</span> fp32:chw)</span><br><span class="line">                              Note: If this option is specified, please <span class="built_in">set</span> comma-separated types and formats <span class="keyword">for</span> all</span><br><span class="line">                                    outputs following the same order as network outputs ID (even <span class="keyword">if</span> only one output</span><br><span class="line">                                    needs specifying IO format) or <span class="built_in">set</span> the <span class="built_in">type</span> and format once <span class="keyword">for</span> broadcasting.</span><br><span class="line">                              IO Formats: spec  ::= IOfmt[<span class="string">&quot;,&quot;</span>spec]</span><br><span class="line">                                          IOfmt ::= <span class="built_in">type</span>:<span class="built_in">fmt</span></span><br><span class="line">                                          <span class="built_in">type</span>  ::= <span class="string">&quot;fp32&quot;</span>|<span class="string">&quot;fp16&quot;</span>|<span class="string">&quot;int32&quot;</span>|<span class="string">&quot;int8&quot;</span></span><br><span class="line">                                          <span class="built_in">fmt</span>   ::= (<span class="string">&quot;chw&quot;</span>|<span class="string">&quot;chw2&quot;</span>|<span class="string">&quot;chw4&quot;</span>|<span class="string">&quot;hwc8&quot;</span>|<span class="string">&quot;chw16&quot;</span>|<span class="string">&quot;chw32&quot;</span>|<span class="string">&quot;dhwc8&quot;</span>|</span><br><span class="line">                                                     <span class="string">&quot;cdhw32&quot;</span>|<span class="string">&quot;hwc&quot;</span>|<span class="string">&quot;dla_linear&quot;</span>|<span class="string">&quot;dla_hwc4&quot;</span>)[<span class="string">&quot;+&quot;</span><span class="built_in">fmt</span>]</span><br><span class="line">  --workspace=N               Set workspace size <span class="keyword">in</span> MiB.</span><br><span class="line">  --memPoolSize=poolspec      Specify the size constraints of the designated memory pool(s) <span class="keyword">in</span> MiB.</span><br><span class="line">                              Note: Also accepts decimal sizes, e.g. 0.25MiB. Will be rounded down to the nearest <span class="built_in">integer</span> bytes.</span><br><span class="line">                              Pool constraint: poolspec ::= poolfmt[<span class="string">&quot;,&quot;</span>poolspec]</span><br><span class="line">                                               poolfmt ::= pool:sizeInMiB</span><br><span class="line">                                               pool ::= <span class="string">&quot;workspace&quot;</span>|<span class="string">&quot;dlaSRAM&quot;</span>|<span class="string">&quot;dlaLocalDRAM&quot;</span>|<span class="string">&quot;dlaGlobalDRAM&quot;</span></span><br><span class="line">  --profilingVerbosity=mode   Specify profiling verbosity. mode ::= layer_names_only|detailed|none (default = layer_names_only)</span><br><span class="line">  --minTiming=M               Set the minimum number of iterations used <span class="keyword">in</span> kernel selection (default = 1)</span><br><span class="line">  --avgTiming=M               Set the number of <span class="built_in">times</span> averaged <span class="keyword">in</span> each iteration <span class="keyword">for</span> kernel selection (default = 8)</span><br><span class="line">  --refit                     Mark the engine as refittable. This will allow the inspection of refittable layers</span><br><span class="line">                              and weights within the engine.</span><br><span class="line">  --sparsity=spec             Control sparsity (default = disabled).</span><br><span class="line">                              Sparsity: spec ::= <span class="string">&quot;disable&quot;</span>, <span class="string">&quot;enable&quot;</span>, <span class="string">&quot;force&quot;</span></span><br><span class="line">                              Note: Description about each of these options is as below</span><br><span class="line">                                    <span class="built_in">disable</span> = <span class="keyword">do</span> not <span class="built_in">enable</span> sparse tactics <span class="keyword">in</span> the builder (this is the default)</span><br><span class="line">                                    <span class="built_in">enable</span>  = <span class="built_in">enable</span> sparse tactics <span class="keyword">in</span> the builder (but these tactics will only be</span><br><span class="line">                                              considered <span class="keyword">if</span> the weights have the right sparsity pattern)</span><br><span class="line">                                    force   = <span class="built_in">enable</span> sparse tactics <span class="keyword">in</span> the builder and force-overwrite the weights to have</span><br><span class="line">                                              a sparsity pattern (even <span class="keyword">if</span> you loaded a model yourself)</span><br><span class="line">  --noTF32                    Disable tf32 precision (default is to <span class="built_in">enable</span> tf32, <span class="keyword">in</span> addition to fp32)</span><br><span class="line">  --fp16                      Enable fp16 precision, <span class="keyword">in</span> addition to fp32 (default = disabled)</span><br><span class="line">  --int8                      Enable int8 precision, <span class="keyword">in</span> addition to fp32 (default = disabled)</span><br><span class="line">  --best                      Enable all precisions to achieve the best performance (default = disabled)</span><br><span class="line">  --directIO                  Avoid reformatting at network boundaries. (default = disabled)</span><br><span class="line">  --precisionConstraints=spec Control precision constraint setting. (default = none)</span><br><span class="line">                                  Precision Constaints: spec ::= <span class="string">&quot;none&quot;</span> | <span class="string">&quot;obey&quot;</span> | <span class="string">&quot;prefer&quot;</span></span><br><span class="line">                                  none = no constraints</span><br><span class="line">                                  prefer = meet precision constraints <span class="built_in">set</span> by --layerPrecisions/--layerOutputTypes <span class="keyword">if</span> possible</span><br><span class="line">                                  obey = meet precision constraints <span class="built_in">set</span> by --layerPrecisions/--layerOutputTypes or fail</span><br><span class="line">                                         otherwise</span><br><span class="line">  --layerPrecisions=spec      Control per-layer precision constraints. Effective only when precisionConstraints is <span class="built_in">set</span> to</span><br><span class="line">                              <span class="string">&quot;obey&quot;</span> or <span class="string">&quot;prefer&quot;</span>. (default = none)</span><br><span class="line">                              The specs are <span class="built_in">read</span> left-to-right, and later ones override earlier ones. <span class="string">&quot;*&quot;</span> can be used as a</span><br><span class="line">                              layerName to specify the default precision <span class="keyword">for</span> all the unspecified layers.</span><br><span class="line">                              Per-layer precision spec ::= layerPrecision[<span class="string">&quot;,&quot;</span>spec]</span><br><span class="line">                                                  layerPrecision ::= layerName<span class="string">&quot;:&quot;</span>precision</span><br><span class="line">                                                  precision ::= <span class="string">&quot;fp32&quot;</span>|<span class="string">&quot;fp16&quot;</span>|<span class="string">&quot;int32&quot;</span>|<span class="string">&quot;int8&quot;</span></span><br><span class="line">  --layerOutputTypes=spec     Control per-layer output <span class="built_in">type</span> constraints. Effective only when precisionConstraints is <span class="built_in">set</span> to</span><br><span class="line">                              <span class="string">&quot;obey&quot;</span> or <span class="string">&quot;prefer&quot;</span>. (default = none)</span><br><span class="line">                              The specs are <span class="built_in">read</span> left-to-right, and later ones override earlier ones. <span class="string">&quot;*&quot;</span> can be used as a</span><br><span class="line">                              layerName to specify the default precision <span class="keyword">for</span> all the unspecified layers. If a layer has more than</span><br><span class="line">                              one output, <span class="keyword">then</span> multiple types separated by <span class="string">&quot;+&quot;</span> can be provided <span class="keyword">for</span> this layer.</span><br><span class="line">                              Per-layer output <span class="built_in">type</span> spec ::= layerOutputTypes[<span class="string">&quot;,&quot;</span>spec]</span><br><span class="line">                                                    layerOutputTypes ::= layerName<span class="string">&quot;:&quot;</span><span class="built_in">type</span></span><br><span class="line">                                                    <span class="built_in">type</span> ::= <span class="string">&quot;fp32&quot;</span>|<span class="string">&quot;fp16&quot;</span>|<span class="string">&quot;int32&quot;</span>|<span class="string">&quot;int8&quot;</span>[<span class="string">&quot;+&quot;</span><span class="built_in">type</span>]</span><br><span class="line">  --calib=&lt;file&gt;              Read INT8 calibration cache file</span><br><span class="line">  --safe                      Enable build safety certified engine</span><br><span class="line">  --consistency               Perform consistency checking on safety certified engine</span><br><span class="line">  --restricted                Enable safety scope checking with kSAFETY_SCOPE build flag</span><br><span class="line">  --saveEngine=&lt;file&gt;         Save the serialized engine</span><br><span class="line">  --loadEngine=&lt;file&gt;         Load a serialized engine</span><br><span class="line">  --tacticSources=tactics     Specify the tactics to be used by adding (+) or removing (-) tactics from the default</span><br><span class="line">                              tactic sources (default = all available tactics).</span><br><span class="line">                              Note: Currently only cuDNN, cuBLAS, cuBLAS-LT, and edge mask convolutions are listed as optional</span><br><span class="line">                                    tactics.</span><br><span class="line">                              Tactic Sources: tactics ::= [<span class="string">&quot;,&quot;</span>tactic]</span><br><span class="line">                                              tactic  ::= (+|-)lib</span><br><span class="line">                                              lib     ::= <span class="string">&quot;CUBLAS&quot;</span>|<span class="string">&quot;CUBLAS_LT&quot;</span>|<span class="string">&quot;CUDNN&quot;</span>|<span class="string">&quot;EDGE_MASK_CONVOLUTIONS&quot;</span></span><br><span class="line">                              For example, to <span class="built_in">disable</span> cudnn and <span class="built_in">enable</span> cublas: --tacticSources=-CUDNN,+CUBLAS</span><br><span class="line">  --noBuilderCache            Disable timing cache <span class="keyword">in</span> builder (default is to <span class="built_in">enable</span> timing cache)</span><br><span class="line">  --timingCacheFile=&lt;file&gt;    Save/load the serialized global timing cache</span><br><span class="line"></span><br><span class="line">=== Inference Options ===</span><br><span class="line">  --batch=N                   Set batch size <span class="keyword">for</span> implicit batch engines (default = 1)</span><br><span class="line">                              This option should not be used when the engine is built from an ONNX model or when dynamic</span><br><span class="line">                              shapes are provided when the engine is built.</span><br><span class="line">  --shapes=spec               Set input shapes <span class="keyword">for</span> dynamic shapes inference inputs.</span><br><span class="line">                              Note: Input names can be wrapped with escaped single quotes (ex: \<span class="string">&#x27;Input:0\&#x27;</span>).</span><br><span class="line">                              Example input shapes spec: input0:1x3x256x256, input1:1x3x128x128</span><br><span class="line">                              Each input shape is supplied as a key-value pair <span class="built_in">where</span> key is the input name and</span><br><span class="line">                              value is the dimensions (including the batch dimension) to be used <span class="keyword">for</span> that input.</span><br><span class="line">                              Each key-value pair has the key and value separated using a colon (:).</span><br><span class="line">                              Multiple input shapes can be provided via comma-separated key-value pairs.</span><br><span class="line">  --loadInputs=spec           Load input values from files (default = generate random inputs). Input names can be wrapped with single quotes (ex: <span class="string">&#x27;Input:0&#x27;</span>)</span><br><span class="line">                              Input values spec ::= Ival[<span class="string">&quot;,&quot;</span>spec]</span><br><span class="line">                                           Ival ::= name<span class="string">&quot;:&quot;</span>file</span><br><span class="line">  --iterations=N              Run at least N inference iterations (default = 10)</span><br><span class="line">  --warmUp=N                  Run <span class="keyword">for</span> N milliseconds to warmup before measuring performance (default = 200)</span><br><span class="line">  --duration=N                Run performance measurements <span class="keyword">for</span> at least N seconds wallclock time (default = 3)</span><br><span class="line">  --sleepTime=N               Delay inference start with a gap of N milliseconds between launch and compute (default = 0)</span><br><span class="line">  --idleTime=N                Sleep N milliseconds between two continuous iterations(default = 0)</span><br><span class="line">  --streams=N                 Instantiate N engines to use concurrently (default = 1)</span><br><span class="line">  --exposeDMA                 Serialize DMA transfers to and from device (default = disabled).</span><br><span class="line">  --noDataTransfers           Disable DMA transfers to and from device (default = enabled).</span><br><span class="line">  --useManagedMemory          Use managed memory instead of separate host and device allocations (default = disabled).</span><br><span class="line">  --useSpinWait               Actively synchronize on GPU events. This option may decrease synchronization time but increase CPU usage and power (default = disabled)</span><br><span class="line">  --threads                   Enable multithreading to drive engines with independent threads or speed up refitting (default = disabled)</span><br><span class="line">  --useCudaGraph              Use CUDA graph to capture engine execution and <span class="keyword">then</span> launch inference (default = disabled).</span><br><span class="line">                              This flag may be ignored <span class="keyword">if</span> the graph capture fails.</span><br><span class="line">  --timeDeserialize           Time the amount of time it takes to deserialize the network and <span class="built_in">exit</span>.</span><br><span class="line">  --timeRefit                 Time the amount of time it takes to refit the engine before inference.</span><br><span class="line">  --separateProfileRun        Do not attach the profiler <span class="keyword">in</span> the benchmark run; <span class="keyword">if</span> profiling is enabled, a second profile run will be executed (default = disabled)</span><br><span class="line">  --buildOnly                 Exit after the engine has been built and skip inference perf measurement (default = disabled)</span><br><span class="line"></span><br><span class="line">=== Build and Inference Batch Options ===</span><br><span class="line">                              When using implicit batch, the max batch size of the engine, <span class="keyword">if</span> not given,</span><br><span class="line">                              is <span class="built_in">set</span> to the inference batch size;</span><br><span class="line">                              when using explicit batch, <span class="keyword">if</span> shapes are specified only <span class="keyword">for</span> inference, they</span><br><span class="line">                              will be used also as min/opt/max <span class="keyword">in</span> the build profile; <span class="keyword">if</span> shapes are</span><br><span class="line">                              specified only <span class="keyword">for</span> the build, the opt shapes will be used also <span class="keyword">for</span> inference;</span><br><span class="line">                              <span class="keyword">if</span> both are specified, they must be compatible; and <span class="keyword">if</span> explicit batch is</span><br><span class="line">                              enabled but neither is specified, the model must provide complete static</span><br><span class="line">                              dimensions, including batch size, <span class="keyword">for</span> all inputs</span><br><span class="line">                              Using ONNX models automatically forces explicit batch.</span><br><span class="line"></span><br><span class="line">=== Reporting Options ===</span><br><span class="line">  --verbose                   Use verbose logging (default = <span class="literal">false</span>)</span><br><span class="line">  --avgRuns=N                 Report performance measurements averaged over N consecutive iterations (default = 10)</span><br><span class="line">  --percentile=P              Report performance <span class="keyword">for</span> the P percentage (0&lt;=P&lt;=100, 0 representing max perf, and 100 representing min perf; (default = 99%)</span><br><span class="line">  --dumpRefit                 Print the refittable layers and weights from a refittable engine</span><br><span class="line">  --dumpOutput                Print the output tensor(s) of the last inference iteration (default = disabled)</span><br><span class="line">  --dumpProfile               Print profile information per layer (default = disabled)</span><br><span class="line">  --dumpLayerInfo             Print layer information of the engine to console (default = disabled)</span><br><span class="line">  --exportTimes=&lt;file&gt;        Write the timing results <span class="keyword">in</span> a json file (default = disabled)</span><br><span class="line">  --exportOutput=&lt;file&gt;       Write the output tensors to a json file (default = disabled)</span><br><span class="line">  --exportProfile=&lt;file&gt;      Write the profile information per layer <span class="keyword">in</span> a json file (default = disabled)</span><br><span class="line">  --exportLayerInfo=&lt;file&gt;    Write the layer information of the engine <span class="keyword">in</span> a json file (default = disabled)</span><br><span class="line"></span><br><span class="line">=== System Options ===</span><br><span class="line">  --device=N                  Select cuda device N (default = 0)</span><br><span class="line">  --useDLACore=N              Select DLA core N <span class="keyword">for</span> layers that support DLA (default = none)</span><br><span class="line">  --allowGPUFallback          When DLA is enabled, allow GPU fallback <span class="keyword">for</span> unsupported layers (default = disabled)</span><br><span class="line">  --plugins                   Plugin library (.so) to load (can be specified multiple <span class="built_in">times</span>)</span><br><span class="line"></span><br><span class="line">=== Help ===</span><br><span class="line">  --<span class="built_in">help</span>, -h                  Print this message</span><br><span class="line">root@tegra-ubuntu:~<span class="comment">#</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-2-1-构建阶段"><a href="#2-2-1-构建阶段" class="headerlink" title="2.2.1 构建阶段"></a>2.2.1 构建阶段</h3><ul>
<li><p>—onnx=./model-NCHW.onnx 指定输入模型文件名</p>
</li>
<li><p>—output=y:0 指定输出张量名（使用 ONNX 时该选项无效）</p>
</li>
<li><p>—minShapes=x:0:1x1x28x28 —optShapes=x:0:4x1x28x28 —maxShapes=x:0:16x1x28x28 指定输入形状的范围最小值、最常见值、最大值</p>
</li>
<li><p>—memPoolSize=workspace:1024MiB 优化过程可使用显存最大值 ##</p>
</li>
<li><p>—fp16，—int8，—noTF32，—best，—sparsity=… 指定引擎精度和稀疏性等属性</p>
</li>
<li><p>—saveEngine=./model.plan 指定输出引擎文件名</p>
</li>
<li><p>—skipInference 只创建引擎不运行</p>
</li>
<li><p>—verbose 打印详细日志</p>
</li>
<li><p>—preview=profileSharing0806 启用某些 preview 功能 ##</p>
</li>
<li><p>—builderOptimizationLevel=5 设置优化等级（默认值 2） ##</p>
</li>
<li><p>—timingCacheFile=timing.cache 指定输出优化计时缓存文件名[1]</p>
</li>
<li><p>—profilingVerbosity=detailed 构建期保留更多的逐层信息</p>
</li>
<li><p>—dumpLayerInfo，—exportLayerInfo=layerInfo.txt 导出引擎逐层信息，可与 </p>
</li>
</ul>
<h3 id="2-2-2-运行阶段"><a href="#2-2-2-运行阶段" class="headerlink" title="2.2.2 运行阶段"></a>2.2.2 运行阶段</h3><ul>
<li><p>—loadEngine=model.plan 读取 engine 文件，而不是输入 ONNX 文件</p>
</li>
<li><p>—shapes=x:0:1x1x28x28 指定输入张量形状</p>
</li>
<li><p>—warmUp=1000 热身阶段最短运行时间（单位：ms）</p>
</li>
<li><p>—duration=10 测试阶段最短运行时间（单位：s）</p>
</li>
<li><p>—iterations=100 指定测试阶段运行的最小迭代次数</p>
</li>
<li><p>—useCudaGraph 使用 CUDAGraph 来捕获和执行推理过程[1]</p>
</li>
<li><p>—noDataTransfers 关闭 Host 和 Device 之间的数据传输</p>
</li>
<li><p>—streams=2 使用多个 stream 来运行推理</p>
</li>
<li><p>—verbose 打印详细日志</p>
</li>
<li><p>—dumpProfile，—exportProfile=layerProfile.txt 保存逐层性能数据信息</p>
</li>
</ul>
<h3 id="2-2-3-性能测试结果"><a href="#2-2-3-性能测试结果" class="headerlink" title="2.2.3 性能测试结果"></a>2.2.3 性能测试结果</h3><img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/trtexec.png" class="" title="Performance Metrics in a Normal trtexec Run under Nsight Systems">
<p>下面是我实际一个模型的性能测试结果，其中最后还有各种延迟的解释</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[10/23/2024-07:55:37] [I] === Performance summary ===</span><br><span class="line">[10/23/2024-07:55:37] [I] Throughput: 167.872 qps</span><br><span class="line">[10/23/2024-07:55:37] [I] Latency: min = 6.00146 ms, max = 12.9489 ms, mean = 6.32138 ms, median = 6.13257 ms, percentile(99%) = 11.8965 ms</span><br><span class="line">[10/23/2024-07:55:37] [I] Enqueue Time: min = 5.64685 ms, max = 12.1282 ms, mean = 5.90171 ms, median = 5.71997 ms, percentile(99%) = 11.9158 ms</span><br><span class="line">[10/23/2024-07:55:37] [I] H2D Latency: min = 0.296387 ms, max = 0.517822 ms, mean = 0.394752 ms, median = 0.397461 ms, percentile(99%) = 0.472168 ms</span><br><span class="line">[10/23/2024-07:55:37] [I] GPU Compute Time: min = 5.37585 ms, max = 12.2018 ms, mean = 5.59932 ms, median = 5.4104 ms, percentile(99%) = 11.0095 ms</span><br><span class="line">[10/23/2024-07:55:37] [I] D2H Latency: min = 0.209717 ms, max = 0.454102 ms, mean = 0.32731 ms, median = 0.32605 ms, percentile(99%) = 0.414429 ms</span><br><span class="line">[10/23/2024-07:55:37] [I] Total Host Walltime: 2.09088 s</span><br><span class="line">[10/23/2024-07:55:37] [I] Total GPU Compute Time: 1.96536 s</span><br><span class="line">[10/23/2024-07:55:37] [W] * Throughput may be bound by Enqueue Time rather than GPU Compute and the GPU may be under-utilized.</span><br><span class="line">[10/23/2024-07:55:37] [W]   If not already <span class="keyword">in</span> use, --useCudaGraph (utilize CUDA graphs <span class="built_in">where</span> possible) may increase the throughput.</span><br><span class="line">[10/23/2024-07:55:37] [W] * GPU compute time is unstable, with coefficient of variance = 16.4497%.</span><br><span class="line">[10/23/2024-07:55:37] [W]   If not already <span class="keyword">in</span> use, locking GPU clock frequency or adding --useSpinWait may improve the stability.</span><br><span class="line">[10/23/2024-07:55:37] [I] Explanations of the performance metrics are printed <span class="keyword">in</span> the verbose logs.</span><br><span class="line">[10/23/2024-07:55:37] [V]</span><br><span class="line">[10/23/2024-07:55:37] [V] === Explanations of the performance metrics ===</span><br><span class="line">[10/23/2024-07:55:37] [V] Total Host Walltime: the host walltime from when the first query (after warmups) is enqueued to when the last query is completed.</span><br><span class="line">[10/23/2024-07:55:37] [V] GPU Compute Time: the GPU latency to execute the kernels <span class="keyword">for</span> a query.</span><br><span class="line">[10/23/2024-07:55:37] [V] Total GPU Compute Time: the summation of the GPU Compute Time of all the queries. If this is significantly shorter than Total Host Walltime, the GPU may be under-utilized because of host-side overheads or data transfers.</span><br><span class="line">[10/23/2024-07:55:37] [V] Throughput: the observed throughput computed by dividing the number of queries by the Total Host Walltime. If this is significantly lower than the reciprocal of GPU Compute Time, the GPU may be under-utilized because of host-side overheads or data transfers.</span><br><span class="line">[10/23/2024-07:55:37] [V] Enqueue Time: the host latency to enqueue a query. If this is longer than GPU Compute Time, the GPU may be under-utilized.</span><br><span class="line">[10/23/2024-07:55:37] [V] H2D Latency: the latency <span class="keyword">for</span> host-to-device data transfers <span class="keyword">for</span> input tensors of a single query.</span><br><span class="line">[10/23/2024-07:55:37] [V] D2H Latency: the latency <span class="keyword">for</span> device-to-host data transfers <span class="keyword">for</span> output tensors of a single query.</span><br><span class="line">[10/23/2024-07:55:37] [V] Latency: the summation of H2D Latency, GPU Compute Time, and D2H Latency. This is the latency to infer a single query.</span><br><span class="line">[10/23/2024-07:55:37] [I]</span><br><span class="line">&amp;&amp;&amp;&amp; PASSED TensorRT.trtexec [TensorRT v8412] <span class="comment"># trtexec --loadEngine=test.engine --verbose</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="2-3-Netron"><a href="#2-3-Netron" class="headerlink" title="2.3 Netron"></a>2.3 Netron</h2><ul>
<li><p>模型网络可视化工具</p>
</li>
<li><p>下载安装<a target="_blank" rel="noopener" href="https://github.com/lutzroeder/Netron">https://github.com/lutzroeder/Netron</a></p>
</li>
<li><p>查看网络结构</p>
</li>
<li><p>查看计算图信息</p>
</li>
<li><p>查看节点信息</p>
</li>
</ul>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241023164742091.png" class="" title="image-20241023164742091">
<h2 id="2-4-onnx-graphsurgeon"><a href="#2-4-onnx-graphsurgeon" class="headerlink" title="2.4 onnx-graphsurgeon"></a>2.4 onnx-graphsurgeon</h2><ul>
<li><p>需要手工修改网络的情形？</p>
</li>
<li><p>冗余节点</p>
</li>
<li><p>阻碍 TensorRT 融合的节点组合</p>
</li>
<li><p>可以手工模块化的节点</p>
</li>
<li>更多范例：09-BestPractice</li>
</ul>
<p>在下左图中Conv、Add、Relu节点中间存在一些Squeeze和Unsqueeze节点，这些节点的存在会阻止Conv、Add、Relu节点融合，改造为中间的图，那么中间的Conv、Add、Relu节点会中和为一个节点</p>
<p>下右图中是一个layerNorm，但是有12个节点，我们将其转换为一个plugin一个节点就可以完成。</p>
<p>因此就需要我们对原始的onnx进行修改，需要用到的就是onnx-graphsurgeon</p>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241023165629568.png" class="" title="image-20241023165629568">
<ul>
<li><p>ONNX 模型的编辑器，包含 python API（下面简称 ogs）</p>
</li>
<li><p>功能：</p>
<ul>
<li><p>修改计算图：图属性 / 节点 / 张量 / 节点和张量的连接 / 权重</p>
</li>
<li><p>修改子图：添加 / 删除 / 替换 / 隔离</p>
</li>
<li><p>优化计算图：常量折叠 / 拓扑排序 / 去除无用层</p>
</li>
<li>功能和 API 上有别于 onnx 库</li>
</ul>
</li>
<li><p>下载和参考文档</p>
<ul>
<li><p>pip install nvidia-pyindex onnx-graphsurgeon</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeon/examples">https://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeon/examples</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/onnx-graphsurgeon/docs/index.html">https://docs.nvidia.com/deeplearning/tensorrt/onnx-graphsurgeon/docs/index.html</a></p>
</li>
</ul>
</li>
</ul>
<p>范例代码</p>
<ul>
<li><p>07-Tool/OnnxGraphSurgeon，运行 test.sh</p>
</li>
<li><p>共有 9 个例子，包含创建模型、隔离子图、替换节点、常量折叠、删除节点、shape 操作</p>
</li>
</ul>
<p>Ogs 的 Node 和 Variable:类似与TensorRT的layer和Tensor的概念，有对应的上下游，我们在修改计算图时需要注意防止修改的节点存在上下游为空的情况。也就是输入输出都是可以连接上的。</p>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024110529480.png" class="" title="image-20241024110529480">
<h3 id="2-4-1-CreateModel"><a href="#2-4-1-CreateModel" class="headerlink" title="2.4.1 CreateModel"></a>2.4.1 CreateModel</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnx_graphsurgeon <span class="keyword">as</span> gs</span><br><span class="line"></span><br><span class="line">tensor0 = gs.Variable(<span class="string">&quot;tensor0&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>])  <span class="comment"># 定义张量（变量）</span></span><br><span class="line">tensor1 = gs.Variable(<span class="string">&quot;tensor1&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">tensor2 = gs.Variable(<span class="string">&quot;tensor2&quot;</span>, np.float32, <span class="literal">None</span>)  <span class="comment"># 可以不知道形状或者数据类型</span></span><br><span class="line">tensor3 = gs.Variable(<span class="string">&quot;tensor3&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">constant0 = gs.Constant(name=<span class="string">&quot;constant0&quot;</span>, values=np.ones(shape=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>], dtype=np.float32))  <span class="comment"># 定义张量（常量）</span></span><br><span class="line">constant1 = gs.Constant(name=<span class="string">&quot;constant1&quot;</span>, values=np.ones(shape=[<span class="number">1</span>], dtype=np.float32))</span><br><span class="line"></span><br><span class="line">node0 = gs.Node(<span class="string">&quot;Conv&quot;</span>, <span class="string">&quot;myConv&quot;</span>, inputs=[tensor0, constant0], outputs=[tensor1])  <span class="comment"># 定义节点，使用张量作为输入和输出</span></span><br><span class="line">node0.attrs = OrderedDict([</span><br><span class="line">    [<span class="string">&quot;dilations&quot;</span>, [<span class="number">1</span>, <span class="number">1</span>]],</span><br><span class="line">    [<span class="string">&quot;kernel_shape&quot;</span>, [<span class="number">3</span>, <span class="number">3</span>]],</span><br><span class="line">    [<span class="string">&quot;pads&quot;</span>, [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]],</span><br><span class="line">    [<span class="string">&quot;strides&quot;</span>, [<span class="number">1</span>, <span class="number">1</span>]],</span><br><span class="line">])  <span class="comment"># 节点的属性参数</span></span><br><span class="line"></span><br><span class="line">node1 = gs.Node(<span class="string">&quot;Add&quot;</span>, <span class="string">&quot;myAdd&quot;</span>, inputs=[tensor1, constant1], outputs=[tensor2])</span><br><span class="line">node2 = gs.Node(<span class="string">&quot;Relu&quot;</span>, <span class="string">&quot;myRelu&quot;</span>, inputs=[tensor2], outputs=[tensor3])</span><br><span class="line"></span><br><span class="line">graph = gs.Graph(nodes=[node0, node1, node2], inputs=[tensor0], outputs=[tensor3])  <span class="comment"># 定义计算图，要求给出所有节点和输入输出张量</span></span><br><span class="line"></span><br><span class="line">graph.cleanup().toposort()  <span class="comment"># 保存计算图前的收尾工作，详细作用见 06-Fold.py</span></span><br><span class="line">onnx.save(gs.export_onnx(graph), <span class="string">&quot;model-01.onnx&quot;</span>)</span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024100423440.png" class="" title="image-20241024100423440">
<p>对应的日志</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-01.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor0 [shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:36  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[V] Input tensor: tensor0 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:36 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:43:36  | Input metadata is: &#123;tensor0 [dtype=float32, shape=(&#x27;B&#x27;, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:36 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(4, 1, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:36  | Completed 1 iteration(s) in 19.5 ms | Average inference time: 19.5 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:36     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 32, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1434, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-01.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor0 (dtype=DataType.FLOAT, shape=(-1, 3, 64, 64)) | Setting input tensor shapes to: (min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor0&#x27;, min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.001256 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 5488</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 4 MiB</span><br><span class="line">[V] Total Activation Memory: 0</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 0.302 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:36    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:43:36     | Input metadata is: &#123;tensor0 [dtype=float32, shape=(-1, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:36    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(4, 1, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:36     | Completed 1 iteration(s) in 0.2832 ms | Average inference time: 0.2832 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:43:36&#x27;, &#x27;trt-runner-N0-10/24/24-09:43:36&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:43:36 vs. trt-runner-N0-10/24/24-09:43:36</span><br><span class="line">[I]     Comparing Output: &#x27;tensor3&#x27; (dtype=float32, shape=(4, 1, 64, 64)) with &#x27;tensor3&#x27; (dtype=float32, shape=(4, 1, 64, 64))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:43:36: tensor3 | Stats: mean=14.21, std-dev=1.8511, var=3.4266, median=14.358, min=5.0062 at (3, 0, 0, 0), max=19.679 at (3, 0, 9, 34), avg-magnitude=14.21</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range    |  Num Elems | Visualization</span><br><span class="line">                (5.01, 6.47) |          9 | </span><br><span class="line">                (6.47, 7.94) |         68 | </span><br><span class="line">                (7.94, 9.41) |        312 | <span class="params">##</span></span><br><span class="line">                (9.41, 10.9) |        510 | <span class="params">###</span></span><br><span class="line">                (10.9, 12.3) |       1156 | <span class="params">#######</span></span><br><span class="line">                (12.3, 13.8) |       3912 | <span class="params">##########################</span></span><br><span class="line">                (13.8, 15.3) |       5827 | <span class="params">########################################</span></span><br><span class="line">                (15.3, 16.7) |       3537 | <span class="params">########################</span></span><br><span class="line">                (16.7, 18.2) |        965 | <span class="params">######</span></span><br><span class="line">                (18.2, 19.7) |         88 | </span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:43:36: tensor3 | Stats: mean=14.21, std-dev=1.8511, var=3.4266, median=14.358, min=5.0062 at (3, 0, 0, 0), max=19.679 at (3, 0, 9, 34), avg-magnitude=14.21</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range    |  Num Elems | Visualization</span><br><span class="line">                (5.01, 6.47) |          9 | </span><br><span class="line">                (6.47, 7.94) |         68 | </span><br><span class="line">                (7.94, 9.41) |        312 | <span class="params">##</span></span><br><span class="line">                (9.41, 10.9) |        510 | <span class="params">###</span></span><br><span class="line">                (10.9, 12.3) |       1156 | <span class="params">#######</span></span><br><span class="line">                (12.3, 13.8) |       3912 | <span class="params">##########################</span></span><br><span class="line">                (13.8, 15.3) |       5827 | <span class="params">########################################</span></span><br><span class="line">                (15.3, 16.7) |       3537 | <span class="params">########################</span></span><br><span class="line">                (16.7, 18.2) |        965 | <span class="params">######</span></span><br><span class="line">                (18.2, 19.7) |         88 | </span><br><span class="line">[I]         Error Metrics: tensor3</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=3.8147e-06] OR [rel=2.8924e-07] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=8.4843e-07, std-dev=8.1982e-07, var=6.721e-13, median=9.5367e-07, min=0 at (0, 0, 0, 0), max=3.8147e-06 at (0, 0, 3, 31), avg-magnitude=8.4843e-07</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range            |  Num Elems | Visualization</span><br><span class="line">                    (0       , 3.81e-07) |       6227 | <span class="params">#######################################</span></span><br><span class="line">                    (3.81e-07, 7.63e-07) |         39 | </span><br><span class="line">                    (7.63e-07, 1.14e-06) |       6352 | <span class="params">########################################</span></span><br><span class="line">                    (1.14e-06, 1.53e-06) |          1 | </span><br><span class="line">                    (1.53e-06, 1.91e-06) |          0 | </span><br><span class="line">                    (1.91e-06, 2.29e-06) |       3235 | <span class="params">####################</span></span><br><span class="line">                    (2.29e-06, 2.67e-06) |          0 | </span><br><span class="line">                    (2.67e-06, 3.05e-06) |        387 | <span class="params">##</span></span><br><span class="line">                    (3.05e-06, 3.43e-06) |          0 | </span><br><span class="line">                    (3.43e-06, 3.81e-06) |        143 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=5.9694e-08, std-dev=5.6359e-08, var=3.1764e-15, median=6.4727e-08, min=0 at (0, 0, 0, 0), max=2.8924e-07 at (3, 0, 61, 31), avg-magnitude=5.9694e-08</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range            |  Num Elems | Visualization</span><br><span class="line">                    (0       , 2.89e-08) |       6227 | <span class="params">########################################</span></span><br><span class="line">                    (2.89e-08, 5.78e-08) |          0 | </span><br><span class="line">                    (5.78e-08, 8.68e-08) |       5978 | <span class="params">######################################</span></span><br><span class="line">                    (8.68e-08, 1.16e-07) |        992 | <span class="params">######</span></span><br><span class="line">                    (1.16e-07, 1.45e-07) |       2214 | <span class="params">##############</span></span><br><span class="line">                    (1.45e-07, 1.74e-07) |        405 | <span class="params">##</span></span><br><span class="line">                    (1.74e-07, 2.02e-07) |        275 | <span class="params">#</span></span><br><span class="line">                    (2.02e-07, 2.31e-07) |        196 | <span class="params">#</span></span><br><span class="line">                    (2.31e-07, 2.6e-07 ) |         86 | </span><br><span class="line">                    (2.6e-07 , 2.89e-07) |         11 | </span><br><span class="line">[I]         PASSED | Output: &#x27;tensor3&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;tensor3&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:43:36 vs. trt-runner-N0-10/24/24-09:43:36 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 5.122s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-01.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-4-2-AddNode"><a href="#2-4-2-AddNode" class="headerlink" title="2.4.2 AddNode"></a>2.4.2 AddNode</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnx_graphsurgeon <span class="keyword">as</span> gs</span><br><span class="line"></span><br><span class="line">tensor0 = gs.Variable(<span class="string">&quot;tensor0&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">tensor1 = gs.Variable(<span class="string">&quot;tensor1&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">tensor2 = gs.Variable(<span class="string">&quot;tensor2&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">node0 = gs.Node(<span class="string">&quot;Identity&quot;</span>, <span class="string">&quot;myIdentity0&quot;</span>, inputs=[tensor0], outputs=[tensor1])</span><br><span class="line">node1 = gs.Node(<span class="string">&quot;Identity&quot;</span>, <span class="string">&quot;myIdentity1&quot;</span>, inputs=[tensor1], outputs=[tensor2])</span><br><span class="line"></span><br><span class="line">graph = gs.Graph(nodes=[node0, node1], inputs=[tensor0], outputs=[tensor2])</span><br><span class="line">graph.cleanup().toposort()</span><br><span class="line">onnx.save(gs.export_onnx(graph), <span class="string">&quot;model-02-01.onnx&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> graph.nodes:</span><br><span class="line">    <span class="keyword">if</span> node.op == <span class="string">&quot;Identity&quot;</span> <span class="keyword">and</span> node.name == <span class="string">&quot;myIdentity0&quot;</span>:  <span class="comment"># 遍历计算图找到需要添加节点的位置</span></span><br><span class="line">        constant0 = gs.Constant(name=<span class="string">&quot;constant0&quot;</span>, values=np.ones(shape=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], dtype=np.float32))  <span class="comment"># 构造新节点和新张量</span></span><br><span class="line">        tensor3 = gs.Variable(<span class="string">&quot;tensor3&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">        newNode = gs.Node(<span class="string">&quot;Add&quot;</span>, <span class="string">&quot;myAdd&quot;</span>, inputs=[node.outputs[<span class="number">0</span>], constant0], outputs=[tensor3])</span><br><span class="line"></span><br><span class="line">        graph.nodes.append(newNode)  <span class="comment"># 记得把新节点加入计算图中</span></span><br><span class="line">        index = node.o().inputs.index(node.outputs[<span class="number">0</span>])  <span class="comment"># 小心地找到下一个节点中对应输入张量的位置</span></span><br><span class="line">        node.o().inputs[index] = tensor3  <span class="comment"># 替换为新张量</span></span><br><span class="line"></span><br><span class="line">graph.cleanup().toposort()</span><br><span class="line">onnx.save(gs.export_onnx(graph), <span class="string">&quot;model-02-02.onnx&quot;</span>)</span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024100731267.png" class="" title="image-20241024100731267">
<p>对应的日志</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br></pre></td><td class="code"><pre><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-02-01.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor0 [shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:41  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[V] Input tensor: tensor0 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:41 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:43:41  | Input metadata is: &#123;tensor0 [dtype=float32, shape=(&#x27;B&#x27;, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:41 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor2 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:41  | Completed 1 iteration(s) in 0.2275 ms | Average inference time: 0.2275 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:41     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 32, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1434, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-02-01.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor0 (dtype=DataType.FLOAT, shape=(-1, 3, 64, 64)) | Setting input tensor shapes to: (min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor0&#x27;, min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.000262845 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 32</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 4 MiB</span><br><span class="line">[V] [BlockAssignment] Started assigning block shifts. This will take 1 steps to complete.</span><br><span class="line">[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.024485ms to assign 1 blocks to 1 nodes requiring 512 bytes.</span><br><span class="line">[V] Total Activation Memory: 512</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 0.321 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:41    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:43:41     | Input metadata is: &#123;tensor0 [dtype=float32, shape=(-1, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:41    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor2 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:41     | Completed 1 iteration(s) in 0.2804 ms | Average inference time: 0.2804 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:43:41&#x27;, &#x27;trt-runner-N0-10/24/24-09:43:41&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:43:41 vs. trt-runner-N0-10/24/24-09:43:41</span><br><span class="line">[I]     Comparing Output: &#x27;tensor2&#x27; (dtype=float32, shape=(4, 3, 64, 64)) with &#x27;tensor2&#x27; (dtype=float32, shape=(4, 3, 64, 64))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:43:41: tensor2 | Stats: mean=0.49952, std-dev=0.2893, var=0.083692, median=0.50203, min=1.0369e-05 at (3, 2, 35, 12), max=0.99999 at (3, 0, 30, 21), avg-magnitude=0.49952</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range       |  Num Elems | Visualization</span><br><span class="line">                (1.04e-05, 0.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (0.1     , 0.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (0.2     , 0.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (0.3     , 0.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (0.4     , 0.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (0.5     , 0.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (0.6     , 0.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (0.7     , 0.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (0.8     , 0.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (0.9     , 1  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:43:41: tensor2 | Stats: mean=0.49952, std-dev=0.2893, var=0.083692, median=0.50203, min=1.0369e-05 at (3, 2, 35, 12), max=0.99999 at (3, 0, 30, 21), avg-magnitude=0.49952</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range       |  Num Elems | Visualization</span><br><span class="line">                (1.04e-05, 0.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (0.1     , 0.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (0.2     , 0.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (0.3     , 0.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (0.4     , 0.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (0.5     , 0.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (0.6     , 0.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (0.7     , 0.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (0.8     , 0.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (0.9     , 1  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         Error Metrics: tensor2</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;tensor2&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;tensor2&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:43:41 vs. trt-runner-N0-10/24/24-09:43:41 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 4.610s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-02-01.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-02-02.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor0 [shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:46  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[V] Input tensor: tensor0 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:46 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:43:46  | Input metadata is: &#123;tensor0 [dtype=float32, shape=(&#x27;B&#x27;, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:46 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor2 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:46  | Completed 1 iteration(s) in 0.2618 ms | Average inference time: 0.2618 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:46     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 32, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1434, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-02-02.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor0 (dtype=DataType.FLOAT, shape=(-1, 3, 64, 64)) | Setting input tensor shapes to: (min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor0&#x27;, min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.000311621 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 0</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 8 MiB</span><br><span class="line">[V] [BlockAssignment] Started assigning block shifts. This will take 2 steps to complete.</span><br><span class="line">[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.025079ms to assign 2 blocks to 2 nodes requiring 1024 bytes.</span><br><span class="line">[V] Total Activation Memory: 1024</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 0.514 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:46    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:43:46     | Input metadata is: &#123;tensor0 [dtype=float32, shape=(-1, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:46    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor2 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:46     | Completed 1 iteration(s) in 0.2782 ms | Average inference time: 0.2782 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:43:46&#x27;, &#x27;trt-runner-N0-10/24/24-09:43:46&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:43:46 vs. trt-runner-N0-10/24/24-09:43:46</span><br><span class="line">[I]     Comparing Output: &#x27;tensor2&#x27; (dtype=float32, shape=(4, 3, 64, 64)) with &#x27;tensor2&#x27; (dtype=float32, shape=(4, 3, 64, 64))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:43:46: tensor2 | Stats: mean=1.4995, std-dev=0.2893, var=0.083692, median=1.502, min=1 at (3, 2, 35, 12), max=2 at (3, 0, 30, 21), avg-magnitude=1.4995</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (1  , 1.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (1.1, 1.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (1.2, 1.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (1.3, 1.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (1.4, 1.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (1.5, 1.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (1.6, 1.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (1.7, 1.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (1.8, 1.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (1.9, 2  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:43:46: tensor2 | Stats: mean=1.4995, std-dev=0.2893, var=0.083692, median=1.502, min=1 at (3, 2, 35, 12), max=2 at (3, 0, 30, 21), avg-magnitude=1.4995</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (1  , 1.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (1.1, 1.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (1.2, 1.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (1.3, 1.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (1.4, 1.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (1.5, 1.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (1.6, 1.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (1.7, 1.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (1.8, 1.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (1.9, 2  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         Error Metrics: tensor2</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;tensor2&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;tensor2&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:43:46 vs. trt-runner-N0-10/24/24-09:43:46 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 4.820s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-02-02.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-4-3-RemoveNode"><a href="#2-4-3-RemoveNode" class="headerlink" title="2.4.3 RemoveNode"></a>2.4.3 RemoveNode</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnx_graphsurgeon <span class="keyword">as</span> gs</span><br><span class="line"></span><br><span class="line">tensor0 = gs.Variable(<span class="string">&quot;tensor0&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">tensor1 = gs.Variable(<span class="string">&quot;tensor1&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">tensor2 = gs.Variable(<span class="string">&quot;tensor2&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">tensor3 = gs.Variable(<span class="string">&quot;tensor3&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">constant0 = gs.Constant(name=<span class="string">&quot;constant0&quot;</span>, values=np.ones(shape=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], dtype=np.float32))</span><br><span class="line"></span><br><span class="line">node0 = gs.Node(<span class="string">&quot;Identity&quot;</span>, <span class="string">&quot;myIdentity0&quot;</span>, inputs=[tensor0], outputs=[tensor1])</span><br><span class="line">node1 = gs.Node(<span class="string">&quot;Add&quot;</span>, <span class="string">&quot;myAdd&quot;</span>, inputs=[tensor1, constant0], outputs=[tensor2])</span><br><span class="line">node2 = gs.Node(<span class="string">&quot;Identity&quot;</span>, <span class="string">&quot;myIdentity1&quot;</span>, inputs=[tensor2], outputs=[tensor3])</span><br><span class="line"></span><br><span class="line">graph = gs.Graph(nodes=[node0, node1, node2], inputs=[tensor0], outputs=[tensor3])</span><br><span class="line">graph.cleanup().toposort()</span><br><span class="line">onnx.save(gs.export_onnx(graph), <span class="string">&quot;model-03-01.onnx&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> graph.nodes:</span><br><span class="line">    <span class="keyword">if</span> node.op == <span class="string">&quot;Add&quot;</span> <span class="keyword">and</span> node.name == <span class="string">&quot;myAdd&quot;</span>:</span><br><span class="line">        index = node.o().inputs.index(node.outputs[<span class="number">0</span>])  <span class="comment"># 小心地找到下一个节点中该张量的位置</span></span><br><span class="line">        node.o().inputs[index] = node.inputs[<span class="number">0</span>]  <span class="comment"># 把下一节点的对应输入张量赋为 Add 节点的输入张量</span></span><br><span class="line">        node.outputs = []  <span class="comment"># 关键操作：将 Add 节点的输出张量设置为空，这样 Add 节点就成为无用节点，可以被自动清理删掉</span></span><br><span class="line"></span><br><span class="line">graph.cleanup().toposort()  <span class="comment"># 在清理时会自动删掉 Add 节点+</span></span><br><span class="line">onnx.save(gs.export_onnx(graph), <span class="string">&quot;model-03-02.onnx&quot;</span>)</span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024100812905.png" class="" title="image-20241024100812905">
<p>对应的日志</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br></pre></td><td class="code"><pre><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-03-01.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor0 [shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:51  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[V] Input tensor: tensor0 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:51 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:43:51  | Input metadata is: &#123;tensor0 [dtype=float32, shape=(&#x27;B&#x27;, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:51 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:51  | Completed 1 iteration(s) in 0.2489 ms | Average inference time: 0.2489 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:51     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 32, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1434, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-03-01.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor0 (dtype=DataType.FLOAT, shape=(-1, 3, 64, 64)) | Setting input tensor shapes to: (min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor0&#x27;, min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.000316171 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 0</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 8 MiB</span><br><span class="line">[V] [BlockAssignment] Started assigning block shifts. This will take 2 steps to complete.</span><br><span class="line">[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.029281ms to assign 2 blocks to 2 nodes requiring 1024 bytes.</span><br><span class="line">[V] Total Activation Memory: 1024</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 0.495 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:51    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:43:51     | Input metadata is: &#123;tensor0 [dtype=float32, shape=(-1, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:51    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:51     | Completed 1 iteration(s) in 0.3054 ms | Average inference time: 0.3054 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:43:51&#x27;, &#x27;trt-runner-N0-10/24/24-09:43:51&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:43:51 vs. trt-runner-N0-10/24/24-09:43:51</span><br><span class="line">[I]     Comparing Output: &#x27;tensor3&#x27; (dtype=float32, shape=(4, 3, 64, 64)) with &#x27;tensor3&#x27; (dtype=float32, shape=(4, 3, 64, 64))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:43:51: tensor3 | Stats: mean=1.4995, std-dev=0.2893, var=0.083692, median=1.502, min=1 at (3, 2, 35, 12), max=2 at (3, 0, 30, 21), avg-magnitude=1.4995</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (1  , 1.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (1.1, 1.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (1.2, 1.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (1.3, 1.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (1.4, 1.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (1.5, 1.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (1.6, 1.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (1.7, 1.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (1.8, 1.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (1.9, 2  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:43:51: tensor3 | Stats: mean=1.4995, std-dev=0.2893, var=0.083692, median=1.502, min=1 at (3, 2, 35, 12), max=2 at (3, 0, 30, 21), avg-magnitude=1.4995</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (1  , 1.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (1.1, 1.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (1.2, 1.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (1.3, 1.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (1.4, 1.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (1.5, 1.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (1.6, 1.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (1.7, 1.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (1.8, 1.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (1.9, 2  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         Error Metrics: tensor3</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;tensor3&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;tensor3&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:43:51 vs. trt-runner-N0-10/24/24-09:43:51 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 4.829s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-03-01.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-03-02.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor0 [shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:56  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[V] Input tensor: tensor0 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:56 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:43:56  | Input metadata is: &#123;tensor0 [dtype=float32, shape=(&#x27;B&#x27;, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:56 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:43:56  | Completed 1 iteration(s) in 0.2198 ms | Average inference time: 0.2198 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:56     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 32, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1434, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-03-02.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor0 (dtype=DataType.FLOAT, shape=(-1, 3, 64, 64)) | Setting input tensor shapes to: (min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor0&#x27;, min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.000277221 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 0</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 4 MiB</span><br><span class="line">[V] [BlockAssignment] Started assigning block shifts. This will take 1 steps to complete.</span><br><span class="line">[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.024074ms to assign 1 blocks to 1 nodes requiring 512 bytes.</span><br><span class="line">[V] Total Activation Memory: 512</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 0.151 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:56    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:43:56     | Input metadata is: &#123;tensor0 [dtype=float32, shape=(-1, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:56    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:43:56     | Completed 1 iteration(s) in 0.2995 ms | Average inference time: 0.2995 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:43:56&#x27;, &#x27;trt-runner-N0-10/24/24-09:43:56&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:43:56 vs. trt-runner-N0-10/24/24-09:43:56</span><br><span class="line">[I]     Comparing Output: &#x27;tensor3&#x27; (dtype=float32, shape=(4, 3, 64, 64)) with &#x27;tensor3&#x27; (dtype=float32, shape=(4, 3, 64, 64))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:43:56: tensor3 | Stats: mean=0.49952, std-dev=0.2893, var=0.083692, median=0.50203, min=1.0369e-05 at (3, 2, 35, 12), max=0.99999 at (3, 0, 30, 21), avg-magnitude=0.49952</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range       |  Num Elems | Visualization</span><br><span class="line">                (1.04e-05, 0.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (0.1     , 0.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (0.2     , 0.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (0.3     , 0.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (0.4     , 0.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (0.5     , 0.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (0.6     , 0.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (0.7     , 0.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (0.8     , 0.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (0.9     , 1  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:43:56: tensor3 | Stats: mean=0.49952, std-dev=0.2893, var=0.083692, median=0.50203, min=1.0369e-05 at (3, 2, 35, 12), max=0.99999 at (3, 0, 30, 21), avg-magnitude=0.49952</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range       |  Num Elems | Visualization</span><br><span class="line">                (1.04e-05, 0.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (0.1     , 0.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (0.2     , 0.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (0.3     , 0.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (0.4     , 0.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (0.5     , 0.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (0.6     , 0.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (0.7     , 0.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (0.8     , 0.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (0.9     , 1  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         Error Metrics: tensor3</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;tensor3&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;tensor3&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:43:56 vs. trt-runner-N0-10/24/24-09:43:56 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 4.452s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-03-02.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-4-4-ReplaceNode"><a href="#2-4-4-ReplaceNode" class="headerlink" title="2.4.4 ReplaceNode"></a>2.4.4 ReplaceNode</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnx_graphsurgeon <span class="keyword">as</span> gs</span><br><span class="line"></span><br><span class="line">tensor0 = gs.Variable(<span class="string">&quot;tensor0&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">tensor1 = gs.Variable(<span class="string">&quot;tensor1&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">tensor2 = gs.Variable(<span class="string">&quot;tensor2&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">tensor3 = gs.Variable(<span class="string">&quot;tensor3&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">constant0 = gs.Constant(name=<span class="string">&quot;constant0&quot;</span>, values=np.ones(shape=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], dtype=np.float32))</span><br><span class="line"></span><br><span class="line">node0 = gs.Node(<span class="string">&quot;Identity&quot;</span>, <span class="string">&quot;myIdentity0&quot;</span>, inputs=[tensor0], outputs=[tensor1])</span><br><span class="line">node1 = gs.Node(<span class="string">&quot;Add&quot;</span>, <span class="string">&quot;myAdd&quot;</span>, inputs=[tensor1, constant0], outputs=[tensor2])</span><br><span class="line">node2 = gs.Node(<span class="string">&quot;Identity&quot;</span>, <span class="string">&quot;myIdentity1&quot;</span>, inputs=[tensor2], outputs=[tensor3])</span><br><span class="line"></span><br><span class="line">graph0 = gs.Graph(nodes=[node0, node1, node2], inputs=[tensor0], outputs=[tensor3])</span><br><span class="line">graph0.cleanup().toposort()</span><br><span class="line">onnx.save(gs.export_onnx(graph0), <span class="string">&quot;model-04-01.onnx&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过修改操作类型来替换节点</span></span><br><span class="line">graph1 = graph0.copy()</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> graph1.nodes:</span><br><span class="line">    <span class="keyword">if</span> node.op == <span class="string">&quot;Add&quot;</span> <span class="keyword">and</span> node.name == <span class="string">&quot;myAdd&quot;</span>:</span><br><span class="line">        node.op = <span class="string">&quot;Sub&quot;</span></span><br><span class="line">        node.name = <span class="string">&quot;mySub&quot;</span>  <span class="comment"># 名字该改不改都行，主要是方便区分节点以及日后查找</span></span><br><span class="line"></span><br><span class="line">graph1.cleanup().toposort()</span><br><span class="line">onnx.save(gs.export_onnx(graph1), <span class="string">&quot;model-04-02.onnx&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过插入新节点来替换节点</span></span><br><span class="line">graph2 = graph0.copy()</span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> graph2.nodes:</span><br><span class="line">    <span class="keyword">if</span> node.op == <span class="string">&quot;Add&quot;</span> <span class="keyword">and</span> node.name == <span class="string">&quot;myAdd&quot;</span>:</span><br><span class="line">        newNode = gs.Node(<span class="string">&quot;Sub&quot;</span>, <span class="string">&quot;mySub&quot;</span>, inputs=node.inputs, outputs=node.outputs)  <span class="comment"># 照搬输入输出张量</span></span><br><span class="line">        graph2.nodes.append(newNode)  <span class="comment"># 把新节点加入计算图中</span></span><br><span class="line">        node.outputs = []  <span class="comment"># 将原节点的输出张量设置为空</span></span><br><span class="line"></span><br><span class="line">graph2.cleanup().toposort()</span><br><span class="line">onnx.save(gs.export_onnx(graph2), <span class="string">&quot;model-04-03.onnx&quot;</span>)</span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024101051929.png" class="" title="image-20241024101051929">
<p>对应的日志</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br></pre></td><td class="code"><pre><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-04-01.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor0 [shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:01  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[V] Input tensor: tensor0 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:01 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:44:01  | Input metadata is: &#123;tensor0 [dtype=float32, shape=(&#x27;B&#x27;, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:01 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:01  | Completed 1 iteration(s) in 0.2253 ms | Average inference time: 0.2253 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:01     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 32, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1434, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-04-01.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor0 (dtype=DataType.FLOAT, shape=(-1, 3, 64, 64)) | Setting input tensor shapes to: (min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor0&#x27;, min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.000319935 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 0</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 8 MiB</span><br><span class="line">[V] [BlockAssignment] Started assigning block shifts. This will take 2 steps to complete.</span><br><span class="line">[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.025494ms to assign 2 blocks to 2 nodes requiring 1024 bytes.</span><br><span class="line">[V] Total Activation Memory: 1024</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 0.493 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:01    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:44:01     | Input metadata is: &#123;tensor0 [dtype=float32, shape=(-1, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:01    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:01     | Completed 1 iteration(s) in 0.2902 ms | Average inference time: 0.2902 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:44:01&#x27;, &#x27;trt-runner-N0-10/24/24-09:44:01&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:44:01 vs. trt-runner-N0-10/24/24-09:44:01</span><br><span class="line">[I]     Comparing Output: &#x27;tensor3&#x27; (dtype=float32, shape=(4, 3, 64, 64)) with &#x27;tensor3&#x27; (dtype=float32, shape=(4, 3, 64, 64))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:44:01: tensor3 | Stats: mean=1.4995, std-dev=0.2893, var=0.083692, median=1.502, min=1 at (3, 2, 35, 12), max=2 at (3, 0, 30, 21), avg-magnitude=1.4995</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (1  , 1.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (1.1, 1.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (1.2, 1.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (1.3, 1.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (1.4, 1.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (1.5, 1.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (1.6, 1.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (1.7, 1.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (1.8, 1.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (1.9, 2  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:44:01: tensor3 | Stats: mean=1.4995, std-dev=0.2893, var=0.083692, median=1.502, min=1 at (3, 2, 35, 12), max=2 at (3, 0, 30, 21), avg-magnitude=1.4995</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (1  , 1.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (1.1, 1.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (1.2, 1.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (1.3, 1.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (1.4, 1.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (1.5, 1.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (1.6, 1.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (1.7, 1.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (1.8, 1.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (1.9, 2  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         Error Metrics: tensor3</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;tensor3&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;tensor3&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:44:01 vs. trt-runner-N0-10/24/24-09:44:01 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 4.800s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-04-01.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-04-02.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor0 [shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:06  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[V] Input tensor: tensor0 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:06 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:44:06  | Input metadata is: &#123;tensor0 [dtype=float32, shape=(&#x27;B&#x27;, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:06 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:06  | Completed 1 iteration(s) in 0.2508 ms | Average inference time: 0.2508 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:06     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 32, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1434, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-04-02.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor0 (dtype=DataType.FLOAT, shape=(-1, 3, 64, 64)) | Setting input tensor shapes to: (min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor0&#x27;, min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.000436975 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 256</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 16 MiB</span><br><span class="line">[V] [BlockAssignment] Started assigning block shifts. This will take 2 steps to complete.</span><br><span class="line">[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.024297ms to assign 2 blocks to 2 nodes requiring 1024 bytes.</span><br><span class="line">[V] Total Activation Memory: 1024</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 7.949 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:06    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:44:06     | Input metadata is: &#123;tensor0 [dtype=float32, shape=(-1, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:06    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:06     | Completed 1 iteration(s) in 0.2778 ms | Average inference time: 0.2778 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:44:06&#x27;, &#x27;trt-runner-N0-10/24/24-09:44:06&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:44:06 vs. trt-runner-N0-10/24/24-09:44:06</span><br><span class="line">[I]     Comparing Output: &#x27;tensor3&#x27; (dtype=float32, shape=(4, 3, 64, 64)) with &#x27;tensor3&#x27; (dtype=float32, shape=(4, 3, 64, 64))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:44:06: tensor3 | Stats: mean=-0.50048, std-dev=0.2893, var=0.083692, median=-0.49797, min=-0.99999 at (3, 2, 35, 12), max=-9.7156e-06 at (3, 0, 30, 21), avg-magnitude=0.50048</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range              |  Num Elems | Visualization</span><br><span class="line">                (-1       , -0.9     ) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (-0.9     , -0.8     ) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (-0.8     , -0.7     ) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (-0.7     , -0.6     ) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (-0.6     , -0.5     ) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (-0.5     , -0.4     ) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (-0.4     , -0.3     ) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (-0.3     , -0.2     ) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (-0.2     , -0.1     ) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (-0.1     , -9.72e-06) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:44:06: tensor3 | Stats: mean=-0.50048, std-dev=0.2893, var=0.083692, median=-0.49797, min=-0.99999 at (3, 2, 35, 12), max=-9.7156e-06 at (3, 0, 30, 21), avg-magnitude=0.50048</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range              |  Num Elems | Visualization</span><br><span class="line">                (-1       , -0.9     ) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (-0.9     , -0.8     ) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (-0.8     , -0.7     ) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (-0.7     , -0.6     ) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (-0.6     , -0.5     ) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (-0.5     , -0.4     ) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (-0.4     , -0.3     ) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (-0.3     , -0.2     ) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (-0.2     , -0.1     ) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (-0.1     , -9.72e-06) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         Error Metrics: tensor3</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;tensor3&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;tensor3&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:44:06 vs. trt-runner-N0-10/24/24-09:44:06 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 12.231s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-04-02.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-04-03.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor0 [shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:18  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[V] Input tensor: tensor0 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:18 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:44:18  | Input metadata is: &#123;tensor0 [dtype=float32, shape=(&#x27;B&#x27;, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:18 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:18  | Completed 1 iteration(s) in 0.2246 ms | Average inference time: 0.2246 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:18     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 32, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1434, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-04-03.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor0 (dtype=DataType.FLOAT, shape=(-1, 3, 64, 64)) | Setting input tensor shapes to: (min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor0&#x27;, min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.000438549 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 256</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 16 MiB</span><br><span class="line">[V] [BlockAssignment] Started assigning block shifts. This will take 2 steps to complete.</span><br><span class="line">[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.023849ms to assign 2 blocks to 2 nodes requiring 1024 bytes.</span><br><span class="line">[V] Total Activation Memory: 1024</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 7.952 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:18    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:44:18     | Input metadata is: &#123;tensor0 [dtype=float32, shape=(-1, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:18    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:18     | Completed 1 iteration(s) in 0.303 ms | Average inference time: 0.303 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:44:18&#x27;, &#x27;trt-runner-N0-10/24/24-09:44:18&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:44:18 vs. trt-runner-N0-10/24/24-09:44:18</span><br><span class="line">[I]     Comparing Output: &#x27;tensor3&#x27; (dtype=float32, shape=(4, 3, 64, 64)) with &#x27;tensor3&#x27; (dtype=float32, shape=(4, 3, 64, 64))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:44:18: tensor3 | Stats: mean=-0.50048, std-dev=0.2893, var=0.083692, median=-0.49797, min=-0.99999 at (3, 2, 35, 12), max=-9.7156e-06 at (3, 0, 30, 21), avg-magnitude=0.50048</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range              |  Num Elems | Visualization</span><br><span class="line">                (-1       , -0.9     ) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (-0.9     , -0.8     ) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (-0.8     , -0.7     ) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (-0.7     , -0.6     ) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (-0.6     , -0.5     ) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (-0.5     , -0.4     ) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (-0.4     , -0.3     ) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (-0.3     , -0.2     ) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (-0.2     , -0.1     ) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (-0.1     , -9.72e-06) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:44:18: tensor3 | Stats: mean=-0.50048, std-dev=0.2893, var=0.083692, median=-0.49797, min=-0.99999 at (3, 2, 35, 12), max=-9.7156e-06 at (3, 0, 30, 21), avg-magnitude=0.50048</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range              |  Num Elems | Visualization</span><br><span class="line">                (-1       , -0.9     ) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (-0.9     , -0.8     ) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (-0.8     , -0.7     ) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (-0.7     , -0.6     ) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (-0.6     , -0.5     ) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (-0.5     , -0.4     ) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (-0.4     , -0.3     ) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (-0.3     , -0.2     ) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (-0.2     , -0.1     ) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (-0.1     , -9.72e-06) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         Error Metrics: tensor3</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;tensor3&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;tensor3&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:44:18 vs. trt-runner-N0-10/24/24-09:44:18 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 12.211s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-04-03.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-4-5-PrintGraphInformation"><a href="#2-4-5-PrintGraphInformation" class="headerlink" title="2.4.5 PrintGraphInformation"></a>2.4.5 PrintGraphInformation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnx_graphsurgeon <span class="keyword">as</span> gs</span><br><span class="line"></span><br><span class="line">onnxFile = <span class="string">&quot;./model-05-PrintGraphInformation.onnx&quot;</span></span><br><span class="line">nMaxAdjustNode = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 .onnx 模型文件 ------------------------------------------------------------</span></span><br><span class="line">tensor0 = gs.Variable(<span class="string">&quot;tensor-0&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line"></span><br><span class="line">constant32x1 = gs.Constant(<span class="string">&quot;constant32x1&quot;</span>, np.ascontiguousarray(np.random.rand(<span class="number">32</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>).reshape(<span class="number">32</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>).astype(np.float32) * <span class="number">2</span> - <span class="number">1</span>))</span><br><span class="line">constant32 = gs.Constant(<span class="string">&quot;constant32&quot;</span>, np.ascontiguousarray(np.random.rand(<span class="number">32</span>).reshape(<span class="number">32</span>).astype(np.float32) * <span class="number">2</span> - <span class="number">1</span>))</span><br><span class="line">constant64x32 = gs.Constant(<span class="string">&quot;constant64x32&quot;</span>, np.ascontiguousarray(np.random.rand(<span class="number">64</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">5</span>).reshape(<span class="number">64</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">5</span>).astype(np.float32) * <span class="number">2</span> - <span class="number">1</span>))</span><br><span class="line">constant64 = gs.Constant(<span class="string">&quot;constant64&quot;</span>, np.ascontiguousarray(np.random.rand(<span class="number">64</span>).reshape(<span class="number">64</span>).astype(np.float32) * <span class="number">2</span> - <span class="number">1</span>))</span><br><span class="line">constantM1Comma3136 = gs.Constant(<span class="string">&quot;constantM1Comma3136&quot;</span>, np.ascontiguousarray(np.array([-<span class="number">1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>], dtype=np.int64)))</span><br><span class="line">constant3136x1024 = gs.Constant(<span class="string">&quot;constant3136x1024&quot;</span>, np.ascontiguousarray(np.random.rand(<span class="number">3136</span>, <span class="number">1024</span>).reshape(<span class="number">3136</span>, <span class="number">1024</span>).astype(np.float32) * <span class="number">2</span> - <span class="number">1</span>))</span><br><span class="line">constant1024 = gs.Constant(<span class="string">&quot;constant1024&quot;</span>, np.ascontiguousarray(np.random.rand(<span class="number">1024</span>).reshape(<span class="number">1024</span>).astype(np.float32) * <span class="number">2</span> - <span class="number">1</span>))</span><br><span class="line">constant1024x10 = gs.Constant(<span class="string">&quot;constant1024x10&quot;</span>, np.ascontiguousarray(np.random.rand(<span class="number">1024</span>, <span class="number">10</span>).reshape(<span class="number">1024</span>, <span class="number">10</span>).astype(np.float32) * <span class="number">2</span> - <span class="number">1</span>))</span><br><span class="line">constant10 = gs.Constant(<span class="string">&quot;constant10&quot;</span>, np.ascontiguousarray(np.random.rand(<span class="number">10</span>).reshape(<span class="number">10</span>).astype(np.float32) * <span class="number">2</span> - <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">graphNodeList = []</span><br><span class="line"></span><br><span class="line">tensor1 = gs.Variable(<span class="string">&quot;tensor-1&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">node1 = gs.Node(<span class="string">&quot;Conv&quot;</span>, <span class="string">&quot;Conv-1&quot;</span>, inputs=[tensor0, constant32x1, constant32], outputs=[tensor1])</span><br><span class="line">node1.attrs = OrderedDict([[<span class="string">&quot;kernel_shape&quot;</span>, [<span class="number">5</span>, <span class="number">5</span>]], [<span class="string">&quot;pads&quot;</span>, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]]])</span><br><span class="line">graphNodeList.append(node1)</span><br><span class="line"></span><br><span class="line">tensor2 = gs.Variable(<span class="string">&quot;tensor-2&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">node2 = gs.Node(<span class="string">&quot;Relu&quot;</span>, <span class="string">&quot;ReLU-2&quot;</span>, inputs=[tensor1], outputs=[tensor2])</span><br><span class="line">graphNodeList.append(node2)</span><br><span class="line"></span><br><span class="line">tensor3 = gs.Variable(<span class="string">&quot;tensor-3&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">node3 = gs.Node(<span class="string">&quot;MaxPool&quot;</span>, <span class="string">&quot;MaxPool-3&quot;</span>, inputs=[tensor2], outputs=[tensor3])</span><br><span class="line">node3.attrs = OrderedDict([[<span class="string">&quot;kernel_shape&quot;</span>, [<span class="number">2</span>, <span class="number">2</span>]], [<span class="string">&quot;pads&quot;</span>, [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]], [<span class="string">&quot;strides&quot;</span>, [<span class="number">2</span>, <span class="number">2</span>]]])</span><br><span class="line">graphNodeList.append(node3)</span><br><span class="line"></span><br><span class="line">tensor4 = gs.Variable(<span class="string">&quot;tensor-4&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">node1 = gs.Node(<span class="string">&quot;Conv&quot;</span>, <span class="string">&quot;Conv-4&quot;</span>, inputs=[tensor3, constant64x32, constant64], outputs=[tensor4])</span><br><span class="line">node1.attrs = OrderedDict([[<span class="string">&quot;kernel_shape&quot;</span>, [<span class="number">5</span>, <span class="number">5</span>]], [<span class="string">&quot;pads&quot;</span>, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>]]])</span><br><span class="line">graphNodeList.append(node1)</span><br><span class="line"></span><br><span class="line">tensor5 = gs.Variable(<span class="string">&quot;tensor-5&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">node5 = gs.Node(<span class="string">&quot;Relu&quot;</span>, <span class="string">&quot;ReLU-5&quot;</span>, inputs=[tensor4], outputs=[tensor5])</span><br><span class="line">graphNodeList.append(node5)</span><br><span class="line"></span><br><span class="line">tensor6 = gs.Variable(<span class="string">&quot;tensor-6&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">node6 = gs.Node(<span class="string">&quot;MaxPool&quot;</span>, <span class="string">&quot;MaxPool-6&quot;</span>, inputs=[tensor5], outputs=[tensor6])</span><br><span class="line">node6.attrs = OrderedDict([[<span class="string">&quot;kernel_shape&quot;</span>, [<span class="number">2</span>, <span class="number">2</span>]], [<span class="string">&quot;pads&quot;</span>, [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]], [<span class="string">&quot;strides&quot;</span>, [<span class="number">2</span>, <span class="number">2</span>]]])</span><br><span class="line">graphNodeList.append(node6)</span><br><span class="line"></span><br><span class="line">tensor7 = gs.Variable(<span class="string">&quot;tensor-7&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">node7 = gs.Node(<span class="string">&quot;Transpose&quot;</span>, <span class="string">&quot;Transpose-7&quot;</span>, inputs=[tensor6], outputs=[tensor7], attrs=OrderedDict([(<span class="string">&quot;perm&quot;</span>, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])]))</span><br><span class="line">graphNodeList.append(node7)</span><br><span class="line"></span><br><span class="line">tensor8 = gs.Variable(<span class="string">&quot;tensor-8&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">node8 = gs.Node(<span class="string">&quot;Reshape&quot;</span>, <span class="string">&quot;Reshape-7&quot;</span>, inputs=[tensor7, constantM1Comma3136], outputs=[tensor8])</span><br><span class="line">graphNodeList.append(node8)</span><br><span class="line"></span><br><span class="line">tensor9 = gs.Variable(<span class="string">&quot;tensor-9&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">node9 = gs.Node(<span class="string">&quot;MatMul&quot;</span>, <span class="string">&quot;MatMul-9&quot;</span>, inputs=[tensor8, constant3136x1024], outputs=[tensor9])</span><br><span class="line">graphNodeList.append(node9)</span><br><span class="line"></span><br><span class="line">tensor10 = gs.Variable(<span class="string">&quot;tensor-10&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">node10 = gs.Node(<span class="string">&quot;Add&quot;</span>, <span class="string">&quot;Add-10&quot;</span>, inputs=[tensor9, constant1024], outputs=[tensor10])</span><br><span class="line">graphNodeList.append(node10)</span><br><span class="line"></span><br><span class="line">tensor11 = gs.Variable(<span class="string">&quot;tensor-11&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">node11 = gs.Node(<span class="string">&quot;Relu&quot;</span>, <span class="string">&quot;ReLU-11&quot;</span>, inputs=[tensor10], outputs=[tensor11])</span><br><span class="line">graphNodeList.append(node11)</span><br><span class="line"></span><br><span class="line">tensor12 = gs.Variable(<span class="string">&quot;tensor-12&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">node12 = gs.Node(<span class="string">&quot;MatMul&quot;</span>, <span class="string">&quot;MatMul-12&quot;</span>, inputs=[tensor11, constant1024x10], outputs=[tensor12])</span><br><span class="line">graphNodeList.append(node12)</span><br><span class="line"></span><br><span class="line">tensor13 = gs.Variable(<span class="string">&quot;tensor-13&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">node13 = gs.Node(<span class="string">&quot;Add&quot;</span>, <span class="string">&quot;Add-13&quot;</span>, inputs=[tensor12, constant10], outputs=[tensor13])</span><br><span class="line">graphNodeList.append(node13)</span><br><span class="line"></span><br><span class="line">tensor14 = gs.Variable(<span class="string">&quot;tensor-14&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">node14 = gs.Node(<span class="string">&quot;Softmax&quot;</span>, <span class="string">&quot;Softmax-14&quot;</span>, inputs=[tensor13], outputs=[tensor14], attrs=OrderedDict([(<span class="string">&quot;axis&quot;</span>, <span class="number">1</span>)]))</span><br><span class="line">graphNodeList.append(node14)</span><br><span class="line"></span><br><span class="line">tensor15 = gs.Variable(<span class="string">&quot;tensor-15&quot;</span>, np.int32, <span class="literal">None</span>)</span><br><span class="line">node15 = gs.Node(<span class="string">&quot;ArgMax&quot;</span>, <span class="string">&quot;ArgMax-15&quot;</span>, inputs=[tensor14], outputs=[tensor15], attrs=OrderedDict([(<span class="string">&quot;axis&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;keepdims&quot;</span>, <span class="number">0</span>)]))</span><br><span class="line">graphNodeList.append(node15)</span><br><span class="line"></span><br><span class="line">graph = gs.Graph(nodes=graphNodeList, inputs=[tensor0], outputs=[tensor15])</span><br><span class="line"></span><br><span class="line">graph.cleanup().toposort()</span><br><span class="line">onnx.save(gs.export_onnx(graph), onnxFile)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;# Traverse the node: ----------------------------------------------------&quot;</span>)  <span class="comment"># 遍历节点，打印：节点信息，输入张量，输出张量，父节点名，子节点名</span></span><br><span class="line"><span class="keyword">for</span> index, node <span class="keyword">in</span> <span class="built_in">enumerate</span>(graph.nodes):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Node%4d: op=%s, name=%s, attrs=%s&quot;</span> % (index, node.op, node.name, <span class="string">&quot;&quot;</span>.join([<span class="string">&quot;&#123;&quot;</span>] + [<span class="built_in">str</span>(key) + <span class="string">&quot;:&quot;</span> + <span class="built_in">str</span>(value) + <span class="string">&quot;, &quot;</span> <span class="keyword">for</span> key, value <span class="keyword">in</span> node.attrs.items()] + [<span class="string">&quot;&#125;&quot;</span>])))</span><br><span class="line">    <span class="keyword">for</span> jndex, inputTensor <span class="keyword">in</span> <span class="built_in">enumerate</span>(node.inputs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tInTensor  %d: %s&quot;</span> % (jndex, inputTensor))</span><br><span class="line">    <span class="keyword">for</span> jndex, outputTensor <span class="keyword">in</span> <span class="built_in">enumerate</span>(node.outputs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tOutTensor %d: %s&quot;</span> % (jndex, outputTensor))</span><br><span class="line"></span><br><span class="line">    fatherNodeList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nMaxAdjustNode):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            newNode = node.i(i)</span><br><span class="line">            fatherNodeList.append(newNode)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">for</span> jndex, newNode <span class="keyword">in</span> <span class="built_in">enumerate</span>(fatherNodeList):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tFatherNode%d: %s&quot;</span> % (jndex, newNode.name))</span><br><span class="line"></span><br><span class="line">    sonNodeList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nMaxAdjustNode):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            newNode = node.o(i)</span><br><span class="line">            sonNodeList.append(newNode)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">for</span> jndex, newNode <span class="keyword">in</span> <span class="built_in">enumerate</span>(sonNodeList):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tSonNode   %d: %s&quot;</span> % (jndex, newNode.name))</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    # 旧方法，需要嵌套遍历计算图</span></span><br><span class="line"><span class="string">    fatherNodeList = []</span></span><br><span class="line"><span class="string">    for newNode in graph.nodes:</span></span><br><span class="line"><span class="string">        for newOutputTensor in newNode.outputs:</span></span><br><span class="line"><span class="string">            if newOutputTensor in node.inputs:</span></span><br><span class="line"><span class="string">                fatherNodeList.append(newNode)</span></span><br><span class="line"><span class="string">    for jndex, newNode in enumerate(fatherNodeList):</span></span><br><span class="line"><span class="string">        print(&quot;\tFatherNode%d: %s&quot; % (jndex, newNode.name))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    sonNodeList = []</span></span><br><span class="line"><span class="string">    for newNode in graph.nodes:</span></span><br><span class="line"><span class="string">        for newInputTensor in newNode.inputs:</span></span><br><span class="line"><span class="string">            if newInputTensor in node.outputs:</span></span><br><span class="line"><span class="string">                sonNodeList.append(newNode)</span></span><br><span class="line"><span class="string">    for jndex, newNode in enumerate(sonNodeList):</span></span><br><span class="line"><span class="string">        print(&quot;\tSonNode   %d: %s&quot; % (jndex, newNode.name))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;# Traverse the tensor: --------------------------------------------------&quot;</span>)  <span class="comment"># 遍历张量，打印：张量信息，以本张量作为输入张量的节点名，以本张量作为输出张量的节点名，父张量信息，子张量信息</span></span><br><span class="line"><span class="keyword">for</span> index, (name, tensor) <span class="keyword">in</span> <span class="built_in">enumerate</span>(graph.tensors().items()):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Tensor%4d: name=%s, desc=%s&quot;</span> % (index, name, tensor))</span><br><span class="line">    <span class="keyword">for</span> jndex, inputNode <span class="keyword">in</span> <span class="built_in">enumerate</span>(tensor.inputs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tInNode      %d: %s&quot;</span> % (jndex, inputNode.name))</span><br><span class="line">    <span class="keyword">for</span> jndex, outputNode <span class="keyword">in</span> <span class="built_in">enumerate</span>(tensor.outputs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tOutNode     %d: %s&quot;</span> % (jndex, outputNode.name))</span><br><span class="line"></span><br><span class="line">    fatherTensorList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nMaxAdjustNode):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            newTensor = tensor.i(i)</span><br><span class="line">            fatherTensorList.append(newTensor)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">for</span> jndex, newTensor <span class="keyword">in</span> <span class="built_in">enumerate</span>(fatherTensorList):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tFatherTensor%d: %s&quot;</span> % (jndex, newTensor))</span><br><span class="line"></span><br><span class="line">    sonTensorList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nMaxAdjustNode):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            newTensor = tensor.o(i)</span><br><span class="line">            sonTensorList.append(newTensor)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">for</span> jndex, newTensor <span class="keyword">in</span> <span class="built_in">enumerate</span>(sonTensorList):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tSonTensor   %d: %s&quot;</span> % (jndex, newTensor))</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    # 旧方法，需要嵌套遍历计算图</span></span><br><span class="line"><span class="string">    fatherTensorList = []</span></span><br><span class="line"><span class="string">    for newTensor in list(graph.tensors().values()):</span></span><br><span class="line"><span class="string">        for newOutputNode in newTensor.outputs:</span></span><br><span class="line"><span class="string">            if newOutputNode in tensor.inputs:</span></span><br><span class="line"><span class="string">                fatherTensorList.append(newTensor)</span></span><br><span class="line"><span class="string">    for jndex, newTensor in enumerate(fatherTensorList):</span></span><br><span class="line"><span class="string">        print(&quot;\tFatherTensor%d: %s&quot; % (jndex, newTensor))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    sonTensorList = []</span></span><br><span class="line"><span class="string">    for newTensor in list(graph.tensors().values()):</span></span><br><span class="line"><span class="string">        for newInputNode in newTensor.inputs:</span></span><br><span class="line"><span class="string">            if newInputNode in tensor.outputs:</span></span><br><span class="line"><span class="string">                sonTensorList.append(newTensor)</span></span><br><span class="line"><span class="string">    for jndex, newTensor in enumerate(sonTensorList):</span></span><br><span class="line"><span class="string">        print(&quot;\tSonTensor   %d: %s&quot; % (jndex, newTensor))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024101313424.png" class="" title="image-20241024101313424">
<p>输出如下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Traverse the node: ----------------------------------------------------</span></span><br><span class="line">Node   0: op=Conv, name=Conv-1, attrs=&#123;kernel_shape:[5, 5], pads:[2, 2, 2, 2], &#125;</span><br><span class="line">        InTensor  0: Variable (tensor-0): (shape=[<span class="string">&#x27;B&#x27;</span>, 1, 28, 28], dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InTensor  1: Constant (constant32x1): (shape=(32, 1, 5, 5), dtype=float32)</span><br><span class="line">        InTensor  2: Constant (constant32): (shape=(32,), dtype=float32)</span><br><span class="line">        OutTensor 0: Variable (tensor-1): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        SonNode   0: ReLU-2</span><br><span class="line">Node   1: op=Relu, name=ReLU-2, attrs=&#123;&#125;</span><br><span class="line">        InTensor  0: Variable (tensor-1): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        OutTensor 0: Variable (tensor-2): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherNode0: Conv-1</span><br><span class="line">        SonNode   0: MaxPool-3</span><br><span class="line">Node   2: op=MaxPool, name=MaxPool-3, attrs=&#123;kernel_shape:[2, 2], pads:[0, 0, 0, 0], strides:[2, 2], &#125;</span><br><span class="line">        InTensor  0: Variable (tensor-2): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        OutTensor 0: Variable (tensor-3): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherNode0: ReLU-2</span><br><span class="line">        SonNode   0: Conv-4</span><br><span class="line">Node   3: op=Conv, name=Conv-4, attrs=&#123;kernel_shape:[5, 5], pads:[2, 2, 2, 2], &#125;</span><br><span class="line">        InTensor  0: Variable (tensor-3): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InTensor  1: Constant (constant64x32): (shape=(64, 32, 5, 5), dtype=float32)</span><br><span class="line">        InTensor  2: Constant (constant64): (shape=(64,), dtype=float32)</span><br><span class="line">        OutTensor 0: Variable (tensor-4): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherNode0: MaxPool-3</span><br><span class="line">        SonNode   0: ReLU-5</span><br><span class="line">Node   4: op=Relu, name=ReLU-5, attrs=&#123;&#125;</span><br><span class="line">        InTensor  0: Variable (tensor-4): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        OutTensor 0: Variable (tensor-5): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherNode0: Conv-4</span><br><span class="line">        SonNode   0: MaxPool-6</span><br><span class="line">Node   5: op=MaxPool, name=MaxPool-6, attrs=&#123;kernel_shape:[2, 2], pads:[0, 0, 0, 0], strides:[2, 2], &#125;</span><br><span class="line">        InTensor  0: Variable (tensor-5): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        OutTensor 0: Variable (tensor-6): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherNode0: ReLU-5</span><br><span class="line">        SonNode   0: Transpose-7</span><br><span class="line">Node   6: op=Transpose, name=Transpose-7, attrs=&#123;perm:[0, 2, 3, 1], &#125;</span><br><span class="line">        InTensor  0: Variable (tensor-6): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        OutTensor 0: Variable (tensor-7): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherNode0: MaxPool-6</span><br><span class="line">        SonNode   0: Reshape-7</span><br><span class="line">Node   7: op=Reshape, name=Reshape-7, attrs=&#123;&#125;</span><br><span class="line">        InTensor  0: Variable (tensor-7): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InTensor  1: Constant (constantM1Comma3136): (shape=(2,), dtype=int64)</span><br><span class="line">        OutTensor 0: Variable (tensor-8): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherNode0: Transpose-7</span><br><span class="line">        SonNode   0: MatMul-9</span><br><span class="line">Node   8: op=MatMul, name=MatMul-9, attrs=&#123;&#125;</span><br><span class="line">        InTensor  0: Variable (tensor-8): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InTensor  1: Constant (constant3136x1024): (shape=(3136, 1024), dtype=float32)</span><br><span class="line">        OutTensor 0: Variable (tensor-9): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherNode0: Reshape-7</span><br><span class="line">        SonNode   0: Add-10</span><br><span class="line">Node   9: op=Add, name=Add-10, attrs=&#123;&#125;</span><br><span class="line">        InTensor  0: Variable (tensor-9): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InTensor  1: Constant (constant1024): (shape=(1024,), dtype=float32)</span><br><span class="line">        OutTensor 0: Variable (tensor-10): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherNode0: MatMul-9</span><br><span class="line">        SonNode   0: ReLU-11</span><br><span class="line">Node  10: op=Relu, name=ReLU-11, attrs=&#123;&#125;</span><br><span class="line">        InTensor  0: Variable (tensor-10): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        OutTensor 0: Variable (tensor-11): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherNode0: Add-10</span><br><span class="line">        SonNode   0: MatMul-12</span><br><span class="line">Node  11: op=MatMul, name=MatMul-12, attrs=&#123;&#125;</span><br><span class="line">        InTensor  0: Variable (tensor-11): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InTensor  1: Constant (constant1024x10): (shape=(1024, 10), dtype=float32)</span><br><span class="line">        OutTensor 0: Variable (tensor-12): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherNode0: ReLU-11</span><br><span class="line">        SonNode   0: Add-13</span><br><span class="line">Node  12: op=Add, name=Add-13, attrs=&#123;&#125;</span><br><span class="line">        InTensor  0: Variable (tensor-12): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InTensor  1: Constant (constant10): (shape=(10,), dtype=float32)</span><br><span class="line">        OutTensor 0: Variable (tensor-13): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherNode0: MatMul-12</span><br><span class="line">        SonNode   0: Softmax-14</span><br><span class="line">Node  13: op=Softmax, name=Softmax-14, attrs=&#123;axis:1, &#125;</span><br><span class="line">        InTensor  0: Variable (tensor-13): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        OutTensor 0: Variable (tensor-14): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherNode0: Add-13</span><br><span class="line">        SonNode   0: ArgMax-15</span><br><span class="line">Node  14: op=ArgMax, name=ArgMax-15, attrs=&#123;axis:1, keepdims:0, &#125;</span><br><span class="line">        InTensor  0: Variable (tensor-14): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        OutTensor 0: Variable (tensor-15): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.int32&#x27;</span>&gt;)</span><br><span class="line">        FatherNode0: Softmax-14</span><br><span class="line"><span class="comment"># Traverse the tensor: --------------------------------------------------</span></span><br><span class="line">Tensor   0: name=tensor-0, desc=Variable (tensor-0): (shape=[<span class="string">&#x27;B&#x27;</span>, 1, 28, 28], dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        OutNode     0: Conv-1</span><br><span class="line">        SonTensor   0: Variable (tensor-1): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor   1: name=constant32x1, desc=Constant (constant32x1): (shape=(32, 1, 5, 5), dtype=float32)</span><br><span class="line">        OutNode     0: Conv-1</span><br><span class="line">        SonTensor   0: Variable (tensor-1): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor   2: name=constant32, desc=Constant (constant32): (shape=(32,), dtype=float32)</span><br><span class="line">        OutNode     0: Conv-1</span><br><span class="line">        SonTensor   0: Variable (tensor-1): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor   3: name=tensor-1, desc=Variable (tensor-1): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: Conv-1</span><br><span class="line">        OutNode     0: ReLU-2</span><br><span class="line">        FatherTensor0: Variable (tensor-0): (shape=[<span class="string">&#x27;B&#x27;</span>, 1, 28, 28], dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherTensor1: Constant (constant32x1): (shape=(32, 1, 5, 5), dtype=float32)</span><br><span class="line">        FatherTensor2: Constant (constant32): (shape=(32,), dtype=float32)</span><br><span class="line">        SonTensor   0: Variable (tensor-2): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor   4: name=tensor-2, desc=Variable (tensor-2): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: ReLU-2</span><br><span class="line">        OutNode     0: MaxPool-3</span><br><span class="line">        FatherTensor0: Variable (tensor-1): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        SonTensor   0: Variable (tensor-3): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor   5: name=tensor-3, desc=Variable (tensor-3): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: MaxPool-3</span><br><span class="line">        OutNode     0: Conv-4</span><br><span class="line">        FatherTensor0: Variable (tensor-2): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        SonTensor   0: Variable (tensor-4): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor   6: name=constant64x32, desc=Constant (constant64x32): (shape=(64, 32, 5, 5), dtype=float32)</span><br><span class="line">        OutNode     0: Conv-4</span><br><span class="line">        SonTensor   0: Variable (tensor-4): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor   7: name=constant64, desc=Constant (constant64): (shape=(64,), dtype=float32)</span><br><span class="line">        OutNode     0: Conv-4</span><br><span class="line">        SonTensor   0: Variable (tensor-4): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor   8: name=tensor-4, desc=Variable (tensor-4): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: Conv-4</span><br><span class="line">        OutNode     0: ReLU-5</span><br><span class="line">        FatherTensor0: Variable (tensor-3): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherTensor1: Constant (constant64x32): (shape=(64, 32, 5, 5), dtype=float32)</span><br><span class="line">        FatherTensor2: Constant (constant64): (shape=(64,), dtype=float32)</span><br><span class="line">        SonTensor   0: Variable (tensor-5): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor   9: name=tensor-5, desc=Variable (tensor-5): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: ReLU-5</span><br><span class="line">        OutNode     0: MaxPool-6</span><br><span class="line">        FatherTensor0: Variable (tensor-4): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        SonTensor   0: Variable (tensor-6): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor  10: name=tensor-6, desc=Variable (tensor-6): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: MaxPool-6</span><br><span class="line">        OutNode     0: Transpose-7</span><br><span class="line">        FatherTensor0: Variable (tensor-5): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        SonTensor   0: Variable (tensor-7): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor  11: name=tensor-7, desc=Variable (tensor-7): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: Transpose-7</span><br><span class="line">        OutNode     0: Reshape-7</span><br><span class="line">        FatherTensor0: Variable (tensor-6): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        SonTensor   0: Variable (tensor-8): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor  12: name=constantM1Comma3136, desc=Constant (constantM1Comma3136): (shape=(2,), dtype=int64)</span><br><span class="line">        OutNode     0: Reshape-7</span><br><span class="line">        SonTensor   0: Variable (tensor-8): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor  13: name=tensor-8, desc=Variable (tensor-8): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: Reshape-7</span><br><span class="line">        OutNode     0: MatMul-9</span><br><span class="line">        FatherTensor0: Variable (tensor-7): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherTensor1: Constant (constantM1Comma3136): (shape=(2,), dtype=int64)</span><br><span class="line">        SonTensor   0: Variable (tensor-9): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor  14: name=constant3136x1024, desc=Constant (constant3136x1024): (shape=(3136, 1024), dtype=float32)</span><br><span class="line">        OutNode     0: MatMul-9</span><br><span class="line">        SonTensor   0: Variable (tensor-9): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor  15: name=tensor-9, desc=Variable (tensor-9): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: MatMul-9</span><br><span class="line">        OutNode     0: Add-10</span><br><span class="line">        FatherTensor0: Variable (tensor-8): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherTensor1: Constant (constant3136x1024): (shape=(3136, 1024), dtype=float32)</span><br><span class="line">        SonTensor   0: Variable (tensor-10): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor  16: name=constant1024, desc=Constant (constant1024): (shape=(1024,), dtype=float32)</span><br><span class="line">        OutNode     0: Add-10</span><br><span class="line">        SonTensor   0: Variable (tensor-10): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor  17: name=tensor-10, desc=Variable (tensor-10): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: Add-10</span><br><span class="line">        OutNode     0: ReLU-11</span><br><span class="line">        FatherTensor0: Variable (tensor-9): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherTensor1: Constant (constant1024): (shape=(1024,), dtype=float32)</span><br><span class="line">        SonTensor   0: Variable (tensor-11): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor  18: name=tensor-11, desc=Variable (tensor-11): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: ReLU-11</span><br><span class="line">        OutNode     0: MatMul-12</span><br><span class="line">        FatherTensor0: Variable (tensor-10): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        SonTensor   0: Variable (tensor-12): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor  19: name=constant1024x10, desc=Constant (constant1024x10): (shape=(1024, 10), dtype=float32)</span><br><span class="line">        OutNode     0: MatMul-12</span><br><span class="line">        SonTensor   0: Variable (tensor-12): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor  20: name=tensor-12, desc=Variable (tensor-12): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: MatMul-12</span><br><span class="line">        OutNode     0: Add-13</span><br><span class="line">        FatherTensor0: Variable (tensor-11): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherTensor1: Constant (constant1024x10): (shape=(1024, 10), dtype=float32)</span><br><span class="line">        SonTensor   0: Variable (tensor-13): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor  21: name=constant10, desc=Constant (constant10): (shape=(10,), dtype=float32)</span><br><span class="line">        OutNode     0: Add-13</span><br><span class="line">        SonTensor   0: Variable (tensor-13): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor  22: name=tensor-13, desc=Variable (tensor-13): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: Add-13</span><br><span class="line">        OutNode     0: Softmax-14</span><br><span class="line">        FatherTensor0: Variable (tensor-12): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        FatherTensor1: Constant (constant10): (shape=(10,), dtype=float32)</span><br><span class="line">        SonTensor   0: Variable (tensor-14): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">Tensor  23: name=tensor-14, desc=Variable (tensor-14): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: Softmax-14</span><br><span class="line">        OutNode     0: ArgMax-15</span><br><span class="line">        FatherTensor0: Variable (tensor-13): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line">        SonTensor   0: Variable (tensor-15): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.int32&#x27;</span>&gt;)</span><br><span class="line">Tensor  24: name=tensor-15, desc=Variable (tensor-15): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.int32&#x27;</span>&gt;)</span><br><span class="line">        InNode      0: ArgMax-15</span><br><span class="line">        FatherTensor0: Variable (tensor-14): (shape=None, dtype=&lt;class <span class="string">&#x27;numpy.float32&#x27;</span>&gt;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-4-6-Fold"><a href="#2-4-6-Fold" class="headerlink" title="2.4.6 Fold"></a>2.4.6 Fold</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnx_graphsurgeon <span class="keyword">as</span> gs</span><br><span class="line"></span><br><span class="line">tensor0 = gs.Variable(<span class="string">&quot;tensor0&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>])  <span class="comment"># 三个真正有用的张量</span></span><br><span class="line">tensor1 = gs.Variable(<span class="string">&quot;tensor1&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">tensor2 = gs.Variable(<span class="string">&quot;tensor2&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">tensor3 = gs.Variable(<span class="string">&quot;tensor3&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>])  <span class="comment"># 一个假输入张量</span></span><br><span class="line">tensor4 = gs.Variable(<span class="string">&quot;tensor4&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>])  <span class="comment"># 一个假输出张量</span></span><br><span class="line">tensor5 = gs.Variable(<span class="string">&quot;tensor5&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>])  <span class="comment"># 两个无用张量</span></span><br><span class="line">tensor6 = gs.Variable(<span class="string">&quot;tensor6&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">tensor7 = gs.Variable(<span class="string">&quot;tensor7&quot;</span>, np.float32, <span class="literal">None</span>)  <span class="comment"># 中间结果张量</span></span><br><span class="line">tensor8 = gs.Variable(<span class="string">&quot;tensor8&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">constant0 = gs.Constant(name=<span class="string">&quot;w&quot;</span>, values=np.ones(shape=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], dtype=np.float32))</span><br><span class="line"></span><br><span class="line">node0 = gs.Node(<span class="string">&quot;Add&quot;</span>, <span class="string">&quot;myAdd0&quot;</span>, inputs=[constant0, constant0], outputs=[tensor7])</span><br><span class="line">node1 = gs.Node(<span class="string">&quot;Add&quot;</span>, <span class="string">&quot;myAdd1&quot;</span>, inputs=[tensor7, constant0], outputs=[tensor8])</span><br><span class="line">node2 = gs.Node(<span class="string">&quot;Add&quot;</span>, <span class="string">&quot;myAdd2&quot;</span>, inputs=[tensor0, tensor8], outputs=[tensor1])  <span class="comment"># 有效节点</span></span><br><span class="line">node3 = gs.Node(<span class="string">&quot;Add&quot;</span>, <span class="string">&quot;myAdd3&quot;</span>, inputs=[tensor1, constant0], outputs=[tensor2])  <span class="comment"># 有效节点</span></span><br><span class="line">node4 = gs.Node(<span class="string">&quot;Add&quot;</span>, <span class="string">&quot;myAdd4&quot;</span>, inputs=[tensor5, constant0], outputs=[tensor6])  <span class="comment"># 无效节点</span></span><br><span class="line"></span><br><span class="line">graph = gs.Graph(nodes=[node4, node3, node2, node1, node0], inputs=[tensor0, tensor3], outputs=[tensor2, tensor4])</span><br><span class="line"></span><br><span class="line">onnx.save(gs.export_onnx(graph), <span class="string">&quot;model-06-01.onnx&quot;</span>)  <span class="comment"># 原始计算图，可见 4 个无边张量和 1 个无边的节点，还有 1 个常数计算链</span></span><br><span class="line">onnx.save(gs.export_onnx(graph.fold_constants()), <span class="string">&quot;model-06-02.onnx&quot;</span>)  <span class="comment"># 常数折叠后的计算图，常数计算链合并到主链中，多出 2 个无边 Add 节点，注意常数折叠并不做节点融合的工作，主链上两个 Add 没有合并掉</span></span><br><span class="line">onnx.save(gs.export_onnx(graph.fold_constants().cleanup()), <span class="string">&quot;model-06-03.onnx&quot;</span>)  <span class="comment"># 打扫后的计算图，可见 3 个无用的 Add 节点被清除</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Before toposort:&quot;</span>)  <span class="comment"># 原始节点顺序</span></span><br><span class="line"><span class="keyword">for</span> index, node <span class="keyword">in</span> <span class="built_in">enumerate</span>(graph.nodes):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;No.%d-&gt;%s&quot;</span> % (index, node.name))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;After toposort:&quot;</span>)  <span class="comment"># 拓扑排序后的节点顺序，节点基本按照计算图的计算顺序进行排列</span></span><br><span class="line">graph.cleanup().toposort()</span><br><span class="line"><span class="keyword">for</span> index, node <span class="keyword">in</span> <span class="built_in">enumerate</span>(graph.nodes):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;No.%d-&gt;%s&quot;</span> % (index, node.name))</span><br><span class="line"></span><br><span class="line">graph.inputs = [tensor0]</span><br><span class="line">graph.outputs = [tensor2]</span><br><span class="line">onnx.save(gs.export_onnx(graph), <span class="string">&quot;model-06-04.onnx&quot;</span>)  <span class="comment"># 去掉多与输入输出的计算图，才能正确被 TensorRT 处理</span></span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024101511418.png" class="" title="image-20241024101511418">
<p>对应的日志</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line">Before toposort:</span><br><span class="line">No.0-&gt;myAdd3</span><br><span class="line">No.1-&gt;myAdd2</span><br><span class="line">After toposort:</span><br><span class="line">No.0-&gt;myAdd2</span><br><span class="line">No.1-&gt;myAdd3</span><br><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-06-04.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor0 [shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:31  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[V] Input tensor: tensor0 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:31 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:44:31  | Input metadata is: &#123;tensor0 [dtype=float32, shape=(&#x27;B&#x27;, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:31 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor2 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:31  | Completed 1 iteration(s) in 0.3281 ms | Average inference time: 0.3281 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:31     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 32, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1434, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-06-04.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor0 (dtype=DataType.FLOAT, shape=(-1, 3, 64, 64)) | Setting input tensor shapes to: (min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor0&#x27;, min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.000276867 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 0</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 8 MiB</span><br><span class="line">[V] Total Activation Memory: 0</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 0.062 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:31    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:44:31     | Input metadata is: &#123;tensor0 [dtype=float32, shape=(-1, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:31    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor2 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:31     | Completed 1 iteration(s) in 0.3433 ms | Average inference time: 0.3433 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:44:31&#x27;, &#x27;trt-runner-N0-10/24/24-09:44:31&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:44:31 vs. trt-runner-N0-10/24/24-09:44:31</span><br><span class="line">[I]     Comparing Output: &#x27;tensor2&#x27; (dtype=float32, shape=(4, 3, 64, 64)) with &#x27;tensor2&#x27; (dtype=float32, shape=(4, 3, 64, 64))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:44:31: tensor2 | Stats: mean=4.4995, std-dev=0.2893, var=0.083692, median=4.502, min=4 at (3, 2, 35, 12), max=5 at (3, 0, 30, 21), avg-magnitude=4.4995</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (4  , 4.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (4.1, 4.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (4.2, 4.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (4.3, 4.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (4.4, 4.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (4.5, 4.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (4.6, 4.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (4.7, 4.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (4.8, 4.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (4.9, 5  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:44:31: tensor2 | Stats: mean=4.4995, std-dev=0.2893, var=0.083692, median=4.502, min=4 at (3, 2, 35, 12), max=5 at (3, 0, 30, 21), avg-magnitude=4.4995</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (4  , 4.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (4.1, 4.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (4.2, 4.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (4.3, 4.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (4.4, 4.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (4.5, 4.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (4.6, 4.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (4.7, 4.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (4.8, 4.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (4.9, 5  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         Error Metrics: tensor2</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=4.7684e-07] OR [rel=1.1921e-07] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=7.9677e-08, std-dev=1.7789e-07, var=3.1644e-14, median=0, min=0 at (0, 0, 0, 0), max=4.7684e-07 at (0, 0, 0, 3), avg-magnitude=7.9677e-08</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range            |  Num Elems | Visualization</span><br><span class="line">                    (0       , 4.77e-08) |      40939 | <span class="params">########################################</span></span><br><span class="line">                    (4.77e-08, 9.54e-08) |          0 | </span><br><span class="line">                    (9.54e-08, 1.43e-07) |          0 | </span><br><span class="line">                    (1.43e-07, 1.91e-07) |          0 | </span><br><span class="line">                    (1.91e-07, 2.38e-07) |          0 | </span><br><span class="line">                    (2.38e-07, 2.86e-07) |          0 | </span><br><span class="line">                    (2.86e-07, 3.34e-07) |          0 | </span><br><span class="line">                    (3.34e-07, 3.81e-07) |          0 | </span><br><span class="line">                    (3.81e-07, 4.29e-07) |          0 | </span><br><span class="line">                    (4.29e-07, 4.77e-07) |       8213 | <span class="params">########</span></span><br><span class="line">[I]             Relative Difference | Stats: mean=1.8076e-08, std-dev=4.0456e-08, var=1.6367e-15, median=0, min=0 at (0, 0, 0, 0), max=1.1921e-07 at (3, 2, 50, 63), avg-magnitude=1.8076e-08</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range            |  Num Elems | Visualization</span><br><span class="line">                    (0       , 1.19e-08) |      40939 | <span class="params">########################################</span></span><br><span class="line">                    (1.19e-08, 2.38e-08) |          0 | </span><br><span class="line">                    (2.38e-08, 3.58e-08) |          0 | </span><br><span class="line">                    (3.58e-08, 4.77e-08) |          0 | </span><br><span class="line">                    (4.77e-08, 5.96e-08) |          0 | </span><br><span class="line">                    (5.96e-08, 7.15e-08) |          0 | </span><br><span class="line">                    (7.15e-08, 8.34e-08) |          0 | </span><br><span class="line">                    (8.34e-08, 9.54e-08) |          0 | </span><br><span class="line">                    (9.54e-08, 1.07e-07) |       3547 | <span class="params">###</span></span><br><span class="line">                    (1.07e-07, 1.19e-07) |       4666 | <span class="params">####</span></span><br><span class="line">[I]         PASSED | Output: &#x27;tensor2&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;tensor2&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:44:31 vs. trt-runner-N0-10/24/24-09:44:31 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 4.330s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-06-04.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-4-7-ShapeOperationAndSimplify"><a href="#2-4-7-ShapeOperationAndSimplify" class="headerlink" title="2.4.7 ShapeOperationAndSimplify"></a>2.4.7 ShapeOperationAndSimplify</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnx_graphsurgeon <span class="keyword">as</span> gs</span><br><span class="line"></span><br><span class="line">tensor0 = gs.Variable(<span class="string">&quot;tensor0&quot;</span>, np.float32, [<span class="string">&quot;A&quot;</span>, <span class="number">3</span>, <span class="string">&quot;B&quot;</span>, <span class="number">5</span>])</span><br><span class="line">tensor1 = gs.Variable(<span class="string">&quot;tensor1&quot;</span>, np.int64, <span class="literal">None</span>)</span><br><span class="line">tensor2 = gs.Variable(<span class="string">&quot;tensor2&quot;</span>, np.int64, <span class="literal">None</span>)</span><br><span class="line">tensor3 = gs.Variable(<span class="string">&quot;tensor3&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line">tensor4 = gs.Variable(<span class="string">&quot;tensor4&quot;</span>, np.int64, <span class="literal">None</span>)</span><br><span class="line">tensor5 = gs.Variable(<span class="string">&quot;tensor5&quot;</span>, np.int64, <span class="literal">None</span>)</span><br><span class="line">tensor6 = gs.Variable(<span class="string">&quot;tensor6&quot;</span>, np.int64, <span class="literal">None</span>)</span><br><span class="line">tensor7 = gs.Variable(<span class="string">&quot;tensor7&quot;</span>, np.int64, <span class="literal">None</span>)</span><br><span class="line">tensor8 = gs.Variable(<span class="string">&quot;tensor8&quot;</span>, np.float32, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">constant0 = gs.Constant(<span class="string">&quot;constant0&quot;</span>, values=np.array([<span class="number">0</span>, <span class="number">1</span>], dtype=np.int32))  <span class="comment"># 定义张量（常量）</span></span><br><span class="line">constant1 = gs.Constant(<span class="string">&quot;constant1&quot;</span>, values=np.array([<span class="number">2</span>, <span class="number">3</span>], dtype=np.int32))</span><br><span class="line"></span><br><span class="line">node0 = gs.Node(<span class="string">&quot;Shape&quot;</span>, <span class="string">&quot;myShape&quot;</span>, inputs=[tensor0], outputs=[tensor1])  <span class="comment"># value=(A,3,B,5), shape=(4,)</span></span><br><span class="line">node1 = gs.Node(<span class="string">&quot;ReduceProd&quot;</span>, <span class="string">&quot;myReduceProd0&quot;</span>, inputs=[tensor1], attrs=&#123;<span class="string">&quot;axes&quot;</span>: [<span class="number">0</span>], <span class="string">&quot;keepdims&quot;</span>: <span class="built_in">int</span>(<span class="literal">True</span>)&#125;, outputs=[tensor2])  <span class="comment"># value=(A*3*B*5), shape=()</span></span><br><span class="line">node2 = gs.Node(<span class="string">&quot;Reshape&quot;</span>, <span class="string">&quot;myReshape0&quot;</span>, inputs=[tensor0, tensor2], outputs=[tensor3])  <span class="comment"># shape=(A*3*B*5,)</span></span><br><span class="line"></span><br><span class="line">node3 = gs.Node(<span class="string">&quot;Gather&quot;</span>, <span class="string">&quot;myGather0&quot;</span>, inputs=[tensor1, constant0], outputs=[tensor4])  <span class="comment"># value=(A,3), shape=(2,)</span></span><br><span class="line">node4 = gs.Node(<span class="string">&quot;Gather&quot;</span>, <span class="string">&quot;myGather1&quot;</span>, inputs=[tensor1, constant1], outputs=[tensor5])  <span class="comment"># value=(B,5), shape=(2,)</span></span><br><span class="line">node5 = gs.Node(<span class="string">&quot;ReduceProd&quot;</span>, <span class="string">&quot;myReduceProd1&quot;</span>, inputs=[tensor5], attrs=&#123;<span class="string">&quot;axes&quot;</span>: [<span class="number">0</span>], <span class="string">&quot;keepdims&quot;</span>: <span class="built_in">int</span>(<span class="literal">True</span>)&#125;, outputs=[tensor6])  <span class="comment"># value=(B*5), shape=()</span></span><br><span class="line">node6 = gs.Node(<span class="string">&quot;Concat&quot;</span>, <span class="string">&quot;myConcat&quot;</span>, inputs=[tensor4, tensor6], attrs=&#123;<span class="string">&quot;axis&quot;</span>: <span class="number">0</span>&#125;, outputs=[tensor7])  <span class="comment"># value=(A,3,B*5), shape=()</span></span><br><span class="line">node7 = gs.Node(<span class="string">&quot;Reshape&quot;</span>, <span class="string">&quot;myReshape1&quot;</span>, inputs=[tensor0, tensor7], outputs=[tensor8])  <span class="comment"># shape=(A*3*B*5,)</span></span><br><span class="line"></span><br><span class="line">graph = gs.Graph(nodes=[node0, node1, node2, node3, node4, node5, node6, node7], inputs=[tensor0], outputs=[tensor3, tensor8])</span><br><span class="line"></span><br><span class="line">graph.cleanup().toposort()</span><br><span class="line">onnx.save(gs.export_onnx(graph), <span class="string">&quot;model-07-01.onnx&quot;</span>)</span><br><span class="line"></span><br><span class="line">graph.inputs[<span class="number">0</span>].shape = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]  <span class="comment"># 如果是 static shape，则 fold_constants 可以化简大量形状相关的计算节点</span></span><br><span class="line">graph.fold_constants().cleanup().toposort()</span><br><span class="line">onnx.save(gs.export_onnx(graph), <span class="string">&quot;model-07-02.onnx&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024101612206.png" class="" title="image-20241024101612206">
<p>对应的日志</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br></pre></td><td class="code"><pre><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-07-01.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,1,5] --trt-opt-shapes tensor0:[2,3,4,5] --trt-max-shapes tensor0:[4,3,16,5] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor0 [shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:36  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[W] Input tensor: tensor0 [shape=BoundedShape([&#x27;A&#x27;, 3, &#x27;B&#x27;, 5], min=None, max=None)] | Cannot use provided custom shape: BoundedShape([4, 3, 64, 64], min=None, max=None) to override tensor shape. Will use default shape instead.</span><br><span class="line">[W] Input tensor: tensor0 [shape=BoundedShape([&#x27;A&#x27;, 3, &#x27;B&#x27;, 5], min=None, max=None)] | Will generate data of shape: [1, 3, 1, 5].</span><br><span class="line">    If this is incorrect, please provide a custom data loader.</span><br><span class="line">[V] Input tensor: tensor0 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:36 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(1, 3, 1, 5)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:44:36  | Input metadata is: &#123;tensor0 [dtype=float32, shape=(&#x27;A&#x27;, 3, &#x27;B&#x27;, 5)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:36 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(15,)],</span><br><span class="line">     tensor8 [dtype=float32, shape=(1, 3, 5)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:36  | Completed 1 iteration(s) in 0.2027 ms | Average inference time: 0.2027 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:36     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +13, GPU +0, now: CPU 31, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1435, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-07-01.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor0 (dtype=DataType.FLOAT, shape=(-1, 3, -1, 5)) | Setting input tensor shapes to: (min=[1, 3, 1, 5], opt=[2, 3, 4, 5], max=[4, 3, 16, 5])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor0&#x27;, min=[1, 3, 1, 5], opt=[2, 3, 4, 5], max=[4, 3, 16, 5])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.000201152 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 2 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 0</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 4 MiB</span><br><span class="line">[V] [BlockAssignment] Started assigning block shifts. This will take 1 steps to complete.</span><br><span class="line">[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.022452ms to assign 1 blocks to 1 nodes requiring 512 bytes.</span><br><span class="line">[V] Total Activation Memory: 512</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 0.218 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:36    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(1, 3, 1, 5)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:44:36     | Input metadata is: &#123;tensor0 [dtype=float32, shape=(-1, 3, -1, 5)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:36    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(15,)],</span><br><span class="line">     tensor8 [dtype=float32, shape=(1, 3, 5)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:36     | Completed 1 iteration(s) in 0.3252 ms | Average inference time: 0.3252 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:44:36&#x27;, &#x27;trt-runner-N0-10/24/24-09:44:36&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:44:36 vs. trt-runner-N0-10/24/24-09:44:36</span><br><span class="line">[I]     Comparing Output: &#x27;tensor3&#x27; (dtype=float32, shape=(15,)) with &#x27;tensor3&#x27; (dtype=float32, shape=(15,))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:44:36: tensor3 | Stats: mean=0.35738, std-dev=0.25288, var=0.063947, median=0.34556, min=0.00011437 at (2,), max=0.87812 at (13,), avg-magnitude=0.35738</span><br><span class="line">[V]             ---- Values ----</span><br><span class="line">                    [4.17021990e-01 7.20324516e-01 1.14374816e-04 3.02332580e-01</span><br><span class="line">                     1.46755889e-01 9.23385918e-02 1.86260208e-01 3.45560730e-01</span><br><span class="line">                     3.96767467e-01 5.38816750e-01 4.19194520e-01 6.85219526e-01</span><br><span class="line">                     2.04452246e-01 8.78117442e-01 2.73875929e-02]</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range          |  Num Elems | Visualization</span><br><span class="line">                (0.000114, 0.0879) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.0879  , 0.176 ) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.176   , 0.264 ) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.264   , 0.351 ) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.351   , 0.439 ) |          3 | <span class="params">########################################</span></span><br><span class="line">                (0.439   , 0.527 ) |          0 | </span><br><span class="line">                (0.527   , 0.615 ) |          1 | <span class="params">#############</span></span><br><span class="line">                (0.615   , 0.703 ) |          1 | <span class="params">#############</span></span><br><span class="line">                (0.703   , 0.79  ) |          1 | <span class="params">#############</span></span><br><span class="line">                (0.79    , 0.878 ) |          1 | <span class="params">#############</span></span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:44:36: tensor3 | Stats: mean=0.35738, std-dev=0.25288, var=0.063947, median=0.34556, min=0.00011437 at (2,), max=0.87812 at (13,), avg-magnitude=0.35738</span><br><span class="line">[V]             ---- Values ----</span><br><span class="line">                    [4.17021990e-01 7.20324516e-01 1.14374816e-04 3.02332580e-01</span><br><span class="line">                     1.46755889e-01 9.23385918e-02 1.86260208e-01 3.45560730e-01</span><br><span class="line">                     3.96767467e-01 5.38816750e-01 4.19194520e-01 6.85219526e-01</span><br><span class="line">                     2.04452246e-01 8.78117442e-01 2.73875929e-02]</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range          |  Num Elems | Visualization</span><br><span class="line">                (0.000114, 0.0879) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.0879  , 0.176 ) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.176   , 0.264 ) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.264   , 0.351 ) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.351   , 0.439 ) |          3 | <span class="params">########################################</span></span><br><span class="line">                (0.439   , 0.527 ) |          0 | </span><br><span class="line">                (0.527   , 0.615 ) |          1 | <span class="params">#############</span></span><br><span class="line">                (0.615   , 0.703 ) |          1 | <span class="params">#############</span></span><br><span class="line">                (0.703   , 0.79  ) |          1 | <span class="params">#############</span></span><br><span class="line">                (0.79    , 0.878 ) |          1 | <span class="params">#############</span></span><br><span class="line">[I]         Error Metrics: tensor3</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0,), max=0 at (0,), avg-magnitude=0</span><br><span class="line">[V]                 ---- Values ----</span><br><span class="line">                        [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |         15 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0,), max=0 at (0,), avg-magnitude=0</span><br><span class="line">[V]                 ---- Values ----</span><br><span class="line">                        [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |         15 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;tensor3&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     Comparing Output: &#x27;tensor8&#x27; (dtype=float32, shape=(1, 3, 5)) with &#x27;tensor8&#x27; (dtype=float32, shape=(1, 3, 5))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:44:36: tensor8 | Stats: mean=0.35738, std-dev=0.25288, var=0.063947, median=0.34556, min=0.00011437 at (0, 0, 2), max=0.87812 at (0, 2, 3), avg-magnitude=0.35738</span><br><span class="line">[V]             ---- Values ----</span><br><span class="line">                    [[[4.17021990e-01 7.20324516e-01 1.14374816e-04 3.02332580e-01</span><br><span class="line">                       1.46755889e-01]</span><br><span class="line">                      [9.23385918e-02 1.86260208e-01 3.45560730e-01 3.96767467e-01</span><br><span class="line">                       5.38816750e-01]</span><br><span class="line">                      [4.19194520e-01 6.85219526e-01 2.04452246e-01 8.78117442e-01</span><br><span class="line">                       2.73875929e-02]]]</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range          |  Num Elems | Visualization</span><br><span class="line">                (0.000114, 0.0879) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.0879  , 0.176 ) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.176   , 0.264 ) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.264   , 0.351 ) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.351   , 0.439 ) |          3 | <span class="params">########################################</span></span><br><span class="line">                (0.439   , 0.527 ) |          0 | </span><br><span class="line">                (0.527   , 0.615 ) |          1 | <span class="params">#############</span></span><br><span class="line">                (0.615   , 0.703 ) |          1 | <span class="params">#############</span></span><br><span class="line">                (0.703   , 0.79  ) |          1 | <span class="params">#############</span></span><br><span class="line">                (0.79    , 0.878 ) |          1 | <span class="params">#############</span></span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:44:36: tensor8 | Stats: mean=0.35738, std-dev=0.25288, var=0.063947, median=0.34556, min=0.00011437 at (0, 0, 2), max=0.87812 at (0, 2, 3), avg-magnitude=0.35738</span><br><span class="line">[V]             ---- Values ----</span><br><span class="line">                    [[[4.17021990e-01 7.20324516e-01 1.14374816e-04 3.02332580e-01</span><br><span class="line">                       1.46755889e-01]</span><br><span class="line">                      [9.23385918e-02 1.86260208e-01 3.45560730e-01 3.96767467e-01</span><br><span class="line">                       5.38816750e-01]</span><br><span class="line">                      [4.19194520e-01 6.85219526e-01 2.04452246e-01 8.78117442e-01</span><br><span class="line">                       2.73875929e-02]]]</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range          |  Num Elems | Visualization</span><br><span class="line">                (0.000114, 0.0879) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.0879  , 0.176 ) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.176   , 0.264 ) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.264   , 0.351 ) |          2 | <span class="params">##########################</span></span><br><span class="line">                (0.351   , 0.439 ) |          3 | <span class="params">########################################</span></span><br><span class="line">                (0.439   , 0.527 ) |          0 | </span><br><span class="line">                (0.527   , 0.615 ) |          1 | <span class="params">#############</span></span><br><span class="line">                (0.615   , 0.703 ) |          1 | <span class="params">#############</span></span><br><span class="line">                (0.703   , 0.79  ) |          1 | <span class="params">#############</span></span><br><span class="line">                (0.79    , 0.878 ) |          1 | <span class="params">#############</span></span><br><span class="line">[I]         Error Metrics: tensor8</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0), max=0 at (0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Values ----</span><br><span class="line">                        [[[0. 0. 0. 0. 0.]</span><br><span class="line">                          [0. 0. 0. 0. 0.]</span><br><span class="line">                          [0. 0. 0. 0. 0.]]]</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |         15 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0), max=0 at (0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Values ----</span><br><span class="line">                        [[[0. 0. 0. 0. 0.]</span><br><span class="line">                          [0. 0. 0. 0. 0.]</span><br><span class="line">                          [0. 0. 0. 0. 0.]]]</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |         15 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;tensor8&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;tensor3&#x27;, &#x27;tensor8&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:44:36 vs. trt-runner-N0-10/24/24-09:44:36 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 4.488s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-07-01.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,1,5] --trt-opt-shapes tensor0:[2,3,4,5] --trt-max-shapes tensor0:[4,3,16,5] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-07-02.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --input-shapes tensor0:[2,3,4,5]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor0 [shape=(2, 3, 4, 5)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:41  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[V] Input tensor: tensor0 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:41 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(2, 3, 4, 5)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:44:41  | Input metadata is: &#123;tensor0 [dtype=float32, shape=(2, 3, 4, 5)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:41 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(120,)],</span><br><span class="line">     tensor8 [dtype=float32, shape=(2, 3, 20)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:41  | Completed 1 iteration(s) in 0.1762 ms | Average inference time: 0.1762 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:41     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +13, GPU +0, now: CPU 31, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1435, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-07-02.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[W] onnx2trt<span class="built_in">_</span>utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor0 (dtype=DataType.FLOAT, shape=(2, 3, 4, 5)) | Setting input tensor shapes to: (min=[2, 3, 4, 5], opt=[2, 3, 4, 5], max=[2, 3, 4, 5])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor0&#x27;, min=[2, 3, 4, 5], opt=[2, 3, 4, 5], max=[2, 3, 4, 5])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.000190046 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 2 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 0</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 4 MiB</span><br><span class="line">[V] [BlockAssignment] Started assigning block shifts. This will take 1 steps to complete.</span><br><span class="line">[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.023994ms to assign 1 blocks to 1 nodes requiring 512 bytes.</span><br><span class="line">[V] Total Activation Memory: 512</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 0.222 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:41    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(2, 3, 4, 5)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:44:41     | Input metadata is: &#123;tensor0 [dtype=float32, shape=(2, 3, 4, 5)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:41    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(120,)],</span><br><span class="line">     tensor8 [dtype=float32, shape=(2, 3, 20)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:41     | Completed 1 iteration(s) in 0.2961 ms | Average inference time: 0.2961 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:44:41&#x27;, &#x27;trt-runner-N0-10/24/24-09:44:41&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:44:41 vs. trt-runner-N0-10/24/24-09:44:41</span><br><span class="line">[I]     Comparing Output: &#x27;tensor3&#x27; (dtype=float32, shape=(120,)) with &#x27;tensor3&#x27; (dtype=float32, shape=(120,))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:44:41: tensor3 | Stats: mean=0.50103, std-dev=0.30166, var=0.090998, median=0.53011, min=0.00011437 at (2,), max=0.99732 at (109,), avg-magnitude=0.50103</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range          |  Num Elems | Visualization</span><br><span class="line">                (0.000114, 0.0998) |         13 | <span class="params">################################</span></span><br><span class="line">                (0.0998  , 0.2   ) |         16 | <span class="params">########################################</span></span><br><span class="line">                (0.2     , 0.299 ) |          8 | <span class="params">####################</span></span><br><span class="line">                (0.299   , 0.399 ) |         10 | <span class="params">#########################</span></span><br><span class="line">                (0.399   , 0.499 ) |         11 | <span class="params">###########################</span></span><br><span class="line">                (0.499   , 0.598 ) |         11 | <span class="params">###########################</span></span><br><span class="line">                (0.598   , 0.698 ) |         15 | <span class="params">#####################################</span></span><br><span class="line">                (0.698   , 0.798 ) |         11 | <span class="params">###########################</span></span><br><span class="line">                (0.798   , 0.898 ) |         10 | <span class="params">#########################</span></span><br><span class="line">                (0.898   , 0.997 ) |         15 | <span class="params">#####################################</span></span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:44:41: tensor3 | Stats: mean=0.50103, std-dev=0.30166, var=0.090998, median=0.53011, min=0.00011437 at (2,), max=0.99732 at (109,), avg-magnitude=0.50103</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range          |  Num Elems | Visualization</span><br><span class="line">                (0.000114, 0.0998) |         13 | <span class="params">################################</span></span><br><span class="line">                (0.0998  , 0.2   ) |         16 | <span class="params">########################################</span></span><br><span class="line">                (0.2     , 0.299 ) |          8 | <span class="params">####################</span></span><br><span class="line">                (0.299   , 0.399 ) |         10 | <span class="params">#########################</span></span><br><span class="line">                (0.399   , 0.499 ) |         11 | <span class="params">###########################</span></span><br><span class="line">                (0.499   , 0.598 ) |         11 | <span class="params">###########################</span></span><br><span class="line">                (0.598   , 0.698 ) |         15 | <span class="params">#####################################</span></span><br><span class="line">                (0.698   , 0.798 ) |         11 | <span class="params">###########################</span></span><br><span class="line">                (0.798   , 0.898 ) |         10 | <span class="params">#########################</span></span><br><span class="line">                (0.898   , 0.997 ) |         15 | <span class="params">#####################################</span></span><br><span class="line">[I]         Error Metrics: tensor3</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0,), max=0 at (0,), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |        120 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0,), max=0 at (0,), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |        120 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;tensor3&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     Comparing Output: &#x27;tensor8&#x27; (dtype=float32, shape=(2, 3, 20)) with &#x27;tensor8&#x27; (dtype=float32, shape=(2, 3, 20))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:44:41: tensor8 | Stats: mean=0.50103, std-dev=0.30166, var=0.090998, median=0.53011, min=0.00011437 at (0, 0, 2), max=0.99732 at (1, 2, 9), avg-magnitude=0.50103</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range          |  Num Elems | Visualization</span><br><span class="line">                (0.000114, 0.0998) |         13 | <span class="params">################################</span></span><br><span class="line">                (0.0998  , 0.2   ) |         16 | <span class="params">########################################</span></span><br><span class="line">                (0.2     , 0.299 ) |          8 | <span class="params">####################</span></span><br><span class="line">                (0.299   , 0.399 ) |         10 | <span class="params">#########################</span></span><br><span class="line">                (0.399   , 0.499 ) |         11 | <span class="params">###########################</span></span><br><span class="line">                (0.499   , 0.598 ) |         11 | <span class="params">###########################</span></span><br><span class="line">                (0.598   , 0.698 ) |         15 | <span class="params">#####################################</span></span><br><span class="line">                (0.698   , 0.798 ) |         11 | <span class="params">###########################</span></span><br><span class="line">                (0.798   , 0.898 ) |         10 | <span class="params">#########################</span></span><br><span class="line">                (0.898   , 0.997 ) |         15 | <span class="params">#####################################</span></span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:44:41: tensor8 | Stats: mean=0.50103, std-dev=0.30166, var=0.090998, median=0.53011, min=0.00011437 at (0, 0, 2), max=0.99732 at (1, 2, 9), avg-magnitude=0.50103</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range          |  Num Elems | Visualization</span><br><span class="line">                (0.000114, 0.0998) |         13 | <span class="params">################################</span></span><br><span class="line">                (0.0998  , 0.2   ) |         16 | <span class="params">########################################</span></span><br><span class="line">                (0.2     , 0.299 ) |          8 | <span class="params">####################</span></span><br><span class="line">                (0.299   , 0.399 ) |         10 | <span class="params">#########################</span></span><br><span class="line">                (0.399   , 0.499 ) |         11 | <span class="params">###########################</span></span><br><span class="line">                (0.499   , 0.598 ) |         11 | <span class="params">###########################</span></span><br><span class="line">                (0.598   , 0.698 ) |         15 | <span class="params">#####################################</span></span><br><span class="line">                (0.698   , 0.798 ) |         11 | <span class="params">###########################</span></span><br><span class="line">                (0.798   , 0.898 ) |         10 | <span class="params">#########################</span></span><br><span class="line">                (0.898   , 0.997 ) |         15 | <span class="params">#####################################</span></span><br><span class="line">[I]         Error Metrics: tensor8</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0), max=0 at (0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |        120 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0), max=0 at (0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |        120 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;tensor8&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;tensor3&#x27;, &#x27;tensor8&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:44:41 vs. trt-runner-N0-10/24/24-09:44:41 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 4.486s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-07-02.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --input-shapes tensor0:[2,3,4,5]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-4-8-IsolateSubgraph"><a href="#2-4-8-IsolateSubgraph" class="headerlink" title="2.4.8 IsolateSubgraph"></a>2.4.8 IsolateSubgraph</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnx_graphsurgeon <span class="keyword">as</span> gs</span><br><span class="line"></span><br><span class="line">tensor0 = gs.Variable(<span class="string">&quot;tensor0&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">tensor1 = gs.Variable(<span class="string">&quot;tensor1&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">tensor2 = gs.Variable(<span class="string">&quot;tensor2&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line">tensor3 = gs.Variable(<span class="string">&quot;tensor3&quot;</span>, np.float32, [<span class="string">&quot;B&quot;</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>])</span><br><span class="line"></span><br><span class="line">constant0 = gs.Constant(name=<span class="string">&quot;constant0&quot;</span>, values=np.ones(shape=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], dtype=np.float32))</span><br><span class="line"></span><br><span class="line">node0 = gs.Node(<span class="string">&quot;Identity&quot;</span>, <span class="string">&quot;myIdentity0&quot;</span>, inputs=[tensor0], outputs=[tensor1])</span><br><span class="line">node1 = gs.Node(<span class="string">&quot;Add&quot;</span>, <span class="string">&quot;myAdd&quot;</span>, inputs=[tensor1, constant0], outputs=[tensor2])</span><br><span class="line">node2 = gs.Node(<span class="string">&quot;Identity&quot;</span>, <span class="string">&quot;myIdentity1&quot;</span>, inputs=[tensor2], outputs=[tensor3])</span><br><span class="line"></span><br><span class="line">graph = gs.Graph(nodes=[node0, node1, node2], inputs=[tensor0], outputs=[tensor3])</span><br><span class="line">graph.cleanup().toposort()</span><br><span class="line">onnx.save(gs.export_onnx(graph), <span class="string">&quot;model-08-01.onnx&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> graph.nodes:</span><br><span class="line">    <span class="keyword">if</span> node.op == <span class="string">&quot;Add&quot;</span> <span class="keyword">and</span> node.name == <span class="string">&quot;myAdd&quot;</span>:</span><br><span class="line">        graph.inputs = [node.inputs[<span class="number">0</span>]]</span><br><span class="line">        graph.outputs = node.outputs</span><br><span class="line"></span><br><span class="line">graph.cleanup().toposort()</span><br><span class="line">onnx.save(gs.export_onnx(graph), <span class="string">&quot;model-08-02.onnx&quot;</span>)</span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024101641401.png" class="" title="image-20241024101641401">
<p>对应的日志</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br></pre></td><td class="code"><pre><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-08-01.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor0 [shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:45  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[V] Input tensor: tensor0 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:45 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:44:45  | Input metadata is: &#123;tensor0 [dtype=float32, shape=(&#x27;B&#x27;, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:45 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:45  | Completed 1 iteration(s) in 0.2596 ms | Average inference time: 0.2596 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:45     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 32, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1434, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-08-01.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor0 (dtype=DataType.FLOAT, shape=(-1, 3, 64, 64)) | Setting input tensor shapes to: (min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor0&#x27;, min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.000325907 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 0</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 8 MiB</span><br><span class="line">[V] [BlockAssignment] Started assigning block shifts. This will take 2 steps to complete.</span><br><span class="line">[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.024943ms to assign 2 blocks to 2 nodes requiring 1024 bytes.</span><br><span class="line">[V] Total Activation Memory: 1024</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 0.491 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:45    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:44:45     | Input metadata is: &#123;tensor0 [dtype=float32, shape=(-1, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:45    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor3 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:45     | Completed 1 iteration(s) in 0.2978 ms | Average inference time: 0.2978 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:44:45&#x27;, &#x27;trt-runner-N0-10/24/24-09:44:45&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:44:45 vs. trt-runner-N0-10/24/24-09:44:45</span><br><span class="line">[I]     Comparing Output: &#x27;tensor3&#x27; (dtype=float32, shape=(4, 3, 64, 64)) with &#x27;tensor3&#x27; (dtype=float32, shape=(4, 3, 64, 64))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:44:45: tensor3 | Stats: mean=1.4995, std-dev=0.2893, var=0.083692, median=1.502, min=1 at (3, 2, 35, 12), max=2 at (3, 0, 30, 21), avg-magnitude=1.4995</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (1  , 1.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (1.1, 1.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (1.2, 1.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (1.3, 1.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (1.4, 1.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (1.5, 1.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (1.6, 1.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (1.7, 1.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (1.8, 1.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (1.9, 2  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:44:45: tensor3 | Stats: mean=1.4995, std-dev=0.2893, var=0.083692, median=1.502, min=1 at (3, 2, 35, 12), max=2 at (3, 0, 30, 21), avg-magnitude=1.4995</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (1  , 1.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (1.1, 1.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (1.2, 1.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (1.3, 1.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (1.4, 1.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (1.5, 1.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (1.6, 1.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (1.7, 1.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (1.8, 1.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (1.9, 2  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         Error Metrics: tensor3</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;tensor3&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;tensor3&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:44:45 vs. trt-runner-N0-10/24/24-09:44:45 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 4.765s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-08-01.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor0:[1,3,64,64] --trt-opt-shapes tensor0:[4,3,64,64] --trt-max-shapes tensor0:[16,3,64,64] --input-shapes tensor0:[4,3,64,64]</span><br><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-08-02.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor1:[1,3,64,64] --trt-opt-shapes tensor1:[4,3,64,64] --trt-max-shapes tensor1:[16,3,64,64] --input-shapes tensor1:[4,3,64,64]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor1 [shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:50  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[V] Input tensor: tensor1 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:50 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor1 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:44:50  | Input metadata is: &#123;tensor1 [dtype=float32, shape=(&#x27;B&#x27;, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:50 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor2 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:50  | Completed 1 iteration(s) in 0.2503 ms | Average inference time: 0.2503 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:50     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 32, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1434, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-08-02.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor1 (dtype=DataType.FLOAT, shape=(-1, 3, 64, 64)) | Setting input tensor shapes to: (min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor1&#x27;, min=[1, 3, 64, 64], opt=[4, 3, 64, 64], max=[16, 3, 64, 64])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.000202157 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 0</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 8 MiB</span><br><span class="line">[V] Total Activation Memory: 0</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 0.047 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:50    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor1 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:44:50     | Input metadata is: &#123;tensor1 [dtype=float32, shape=(-1, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:50    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;tensor2 [dtype=float32, shape=(4, 3, 64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:50     | Completed 1 iteration(s) in 0.3438 ms | Average inference time: 0.3438 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:44:50&#x27;, &#x27;trt-runner-N0-10/24/24-09:44:50&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:44:50 vs. trt-runner-N0-10/24/24-09:44:50</span><br><span class="line">[I]     Comparing Output: &#x27;tensor2&#x27; (dtype=float32, shape=(4, 3, 64, 64)) with &#x27;tensor2&#x27; (dtype=float32, shape=(4, 3, 64, 64))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:44:50: tensor2 | Stats: mean=1.4995, std-dev=0.2893, var=0.083692, median=1.502, min=1 at (3, 2, 35, 12), max=2 at (3, 0, 30, 21), avg-magnitude=1.4995</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (1  , 1.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (1.1, 1.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (1.2, 1.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (1.3, 1.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (1.4, 1.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (1.5, 1.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (1.6, 1.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (1.7, 1.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (1.8, 1.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (1.9, 2  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:44:50: tensor2 | Stats: mean=1.4995, std-dev=0.2893, var=0.083692, median=1.502, min=1 at (3, 2, 35, 12), max=2 at (3, 0, 30, 21), avg-magnitude=1.4995</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (1  , 1.1) |       4986 | <span class="params">#######################################</span></span><br><span class="line">                (1.1, 1.2) |       4912 | <span class="params">#######################################</span></span><br><span class="line">                (1.2, 1.3) |       4978 | <span class="params">#######################################</span></span><br><span class="line">                (1.3, 1.4) |       4840 | <span class="params">######################################</span></span><br><span class="line">                (1.4, 1.5) |       4785 | <span class="params">######################################</span></span><br><span class="line">                (1.5, 1.6) |       5011 | <span class="params">########################################</span></span><br><span class="line">                (1.6, 1.7) |       4908 | <span class="params">#######################################</span></span><br><span class="line">                (1.7, 1.8) |       4915 | <span class="params">#######################################</span></span><br><span class="line">                (1.8, 1.9) |       4899 | <span class="params">#######################################</span></span><br><span class="line">                (1.9, 2  ) |       4918 | <span class="params">#######################################</span></span><br><span class="line">[I]         Error Metrics: tensor2</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0, 0, 0), max=0 at (0, 0, 0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |      49152 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;tensor2&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;tensor2&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:44:50 vs. trt-runner-N0-10/24/24-09:44:50 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 4.326s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-08-02.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --trt-min-shapes tensor1:[1,3,64,64] --trt-opt-shapes tensor1:[4,3,64,64] --trt-max-shapes tensor1:[16,3,64,64] --input-shapes tensor1:[4,3,64,64]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="2-4-9-BuildModelWithAPI"><a href="#2-4-9-BuildModelWithAPI" class="headerlink" title="2.4.9 BuildModelWithAPI"></a>2.4.9 BuildModelWithAPI</h3><p>用的不多</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnx_graphsurgeon <span class="keyword">as</span> gs</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建节点</span></span><br><span class="line"><span class="comment"># 使用 onnx_graphsurgeon.Graph.register() 将一个函数注册为计算图</span></span><br><span class="line"><span class="meta">@gs.Graph.register()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> self.layer(op=<span class="string">&quot;Add&quot;</span>, inputs=[a, b], outputs=[<span class="string">&quot;myAdd&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">@gs.Graph.register()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mul</span>(<span class="params">self, a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> self.layer(op=<span class="string">&quot;Mul&quot;</span>, inputs=[a, b], outputs=[<span class="string">&quot;myMul&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">@gs.Graph.register()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gemm</span>(<span class="params">self, a, b, isTransposeA=<span class="literal">False</span>, isTransposeB=<span class="literal">False</span></span>):</span><br><span class="line">    attrs = &#123;<span class="string">&quot;transA&quot;</span>: <span class="built_in">int</span>(isTransposeA), <span class="string">&quot;transB&quot;</span>: <span class="built_in">int</span>(isTransposeB)&#125;</span><br><span class="line">    <span class="keyword">return</span> self.layer(op=<span class="string">&quot;Gemm&quot;</span>, inputs=[a, b], outputs=[<span class="string">&quot;myGgemm&quot;</span>], attrs=attrs)</span><br><span class="line"></span><br><span class="line"><span class="meta">@gs.Graph.register()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">min</span>(<span class="params">self, *args</span>):</span><br><span class="line">    <span class="keyword">return</span> self.layer(op=<span class="string">&quot;Min&quot;</span>, inputs=args, outputs=[<span class="string">&quot;myMin&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">@gs.Graph.register()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">max</span>(<span class="params">self, *args</span>):</span><br><span class="line">    <span class="keyword">return</span> self.layer(op=<span class="string">&quot;Max&quot;</span>, inputs=args, outputs=[<span class="string">&quot;myMax&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册函数时注明其版本</span></span><br><span class="line"><span class="meta">@gs.Graph.register(<span class="params">opsets=[<span class="number">11</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">self, a</span>):</span><br><span class="line">    <span class="keyword">return</span> self.layer(op=<span class="string">&quot;Relu&quot;</span>, inputs=[a], outputs=[<span class="string">&quot;myReLU&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注册其他版本的同名函数，在 graph 创建时只会选用指定版本的函数</span></span><br><span class="line"><span class="meta">@gs.Graph.register(<span class="params">opsets=[<span class="number">1</span>]</span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">self, a</span>):</span><br><span class="line">    <span class="keyword">raise</span> NotImplementedError(<span class="string">&quot;This function has not been implemented!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建计算图</span></span><br><span class="line">graph = gs.Graph(opset=<span class="number">11</span>)</span><br><span class="line">tensor0 = gs.Variable(name=<span class="string">&quot;tensor0&quot;</span>, shape=[<span class="number">64</span>, <span class="number">64</span>], dtype=np.float32)</span><br><span class="line"><span class="comment">#tensor1 = np.ones(shape=(64, 64), dtype=np.float32) # 可以直接使用 np.array，但是张量名字会由 onnx 自动生成</span></span><br><span class="line">tensor1 = gs.Constant(name=<span class="string">&quot;tensor1&quot;</span>, values=np.ones(shape=(<span class="number">64</span>, <span class="number">64</span>), dtype=np.float32))</span><br><span class="line">tensor2 = gs.Constant(name=<span class="string">&quot;tensor2&quot;</span>, values=np.ones((<span class="number">64</span>, <span class="number">64</span>), dtype=np.float32) * <span class="number">0.5</span>)</span><br><span class="line">tensor3 = gs.Constant(name=<span class="string">&quot;tensor3&quot;</span>, values=np.ones(shape=[<span class="number">64</span>, <span class="number">64</span>], dtype=np.float32))</span><br><span class="line">tensor4 = gs.Constant(name=<span class="string">&quot;tensor4&quot;</span>, values=np.array([<span class="number">3</span>], dtype=np.float32))</span><br><span class="line">tensor5 = gs.Constant(name=<span class="string">&quot;tensor5&quot;</span>, values=np.array([-<span class="number">3</span>], dtype=np.float32))</span><br><span class="line"></span><br><span class="line">node0 = graph.gemm(tensor1, tensor0, isTransposeB=<span class="literal">True</span>)</span><br><span class="line">node1 = graph.add(*node0, tensor2)</span><br><span class="line">node2 = graph.relu(*node1)</span><br><span class="line">node3 = graph.mul(*node2, tensor3)</span><br><span class="line">node4 = graph.<span class="built_in">min</span>(*node3, tensor4)</span><br><span class="line">node5 = graph.<span class="built_in">max</span>(*node4, tensor5)</span><br><span class="line"></span><br><span class="line">graph.inputs = [tensor0]</span><br><span class="line">graph.outputs = [node5[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">graph.inputs[<span class="number">0</span>].dtype = np.dtype(np.float32)</span><br><span class="line">graph.outputs[<span class="number">0</span>].dtype = np.dtype(np.float32)</span><br><span class="line"></span><br><span class="line">onnx.save(gs.export_onnx(graph), <span class="string">&quot;model-09-01.onnx&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@gs.Graph.register()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">replaceWithClip</span>(<span class="params">self, inputs, outputs</span>):</span><br><span class="line">    <span class="comment"># 砍掉末尾节点的输出张量和头节点的输入张量</span></span><br><span class="line">    <span class="keyword">for</span> inp <span class="keyword">in</span> inputs:</span><br><span class="line">        inp.outputs.clear()</span><br><span class="line">    <span class="keyword">for</span> out <span class="keyword">in</span> outputs:</span><br><span class="line">        out.inputs.clear()</span><br><span class="line">    <span class="comment"># 插入新节点</span></span><br><span class="line">    <span class="keyword">return</span> self.layer(op=<span class="string">&quot;Clip&quot;</span>, inputs=inputs, outputs=outputs)</span><br><span class="line"></span><br><span class="line">tmap = graph.tensors()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手工找出要砍掉的输入和输出张量，交给 replaceWithClip 函数</span></span><br><span class="line">inputs = [tmap[<span class="string">&quot;myMul_6&quot;</span>], tmap[<span class="string">&quot;tensor5&quot;</span>], tmap[<span class="string">&quot;tensor4&quot;</span>]]</span><br><span class="line">outputs = [tmap[<span class="string">&quot;myMax_10&quot;</span>]]</span><br><span class="line">graph.replaceWithClip(inputs, outputs)</span><br><span class="line"></span><br><span class="line">graph.cleanup().toposort()</span><br><span class="line">onnx.save(gs.export_onnx(graph), <span class="string">&quot;model-09-02.onnx&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024101707816.png" class="" title="image-20241024101707816">
<p>对应的日志</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br></pre></td><td class="code"><pre><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-09-01.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --input-shapes tensor0:[64,64]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor0 [shape=(64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:55  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[V] Input tensor: tensor0 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:55 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(64, 64)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:44:55  | Input metadata is: &#123;tensor0 [dtype=float32, shape=(64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:55 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;myMax<span class="built_in">_</span>10 [dtype=float32, shape=(64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:44:55  | Completed 1 iteration(s) in 8.818 ms | Average inference time: 8.818 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:55     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +13, GPU +0, now: CPU 31, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1435, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-09-01.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor0 (dtype=DataType.FLOAT, shape=(64, 64)) | Setting input tensor shapes to: (min=[64, 64], opt=[64, 64], max=[64, 64])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor0&#x27;, min=[64, 64], opt=[64, 64], max=[64, 64])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.00119907 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 7648</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 8 MiB</span><br><span class="line">[V] [BlockAssignment] Started assigning block shifts. This will take 1 steps to complete.</span><br><span class="line">[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.023593ms to assign 1 blocks to 1 nodes requiring 16384 bytes.</span><br><span class="line">[V] Total Activation Memory: 16384</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 1.608 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:55    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(64, 64)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:44:55     | Input metadata is: &#123;tensor0 [dtype=float32, shape=(64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:55    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;myMax<span class="built_in">_</span>10 [dtype=float32, shape=(64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:44:55     | Completed 1 iteration(s) in 0.2885 ms | Average inference time: 0.2885 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:44:55&#x27;, &#x27;trt-runner-N0-10/24/24-09:44:55&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:44:55 vs. trt-runner-N0-10/24/24-09:44:55</span><br><span class="line">[I]     Comparing Output: &#x27;myMax<span class="built_in">_</span>10&#x27; (dtype=float32, shape=(64, 64)) with &#x27;myMax<span class="built_in">_</span>10&#x27; (dtype=float32, shape=(64, 64))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:44:55: myMax<span class="built_in">_</span>10 | Stats: mean=3, std-dev=0, var=0, median=3, min=3 at (0, 0), max=3 at (0, 0), avg-magnitude=3</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (2.5, 2.6) |          0 | </span><br><span class="line">                (2.6, 2.7) |          0 | </span><br><span class="line">                (2.7, 2.8) |          0 | </span><br><span class="line">                (2.8, 2.9) |          0 | </span><br><span class="line">                (2.9, 3  ) |          0 | </span><br><span class="line">                (3  , 3.1) |       4096 | <span class="params">########################################</span></span><br><span class="line">                (3.1, 3.2) |          0 | </span><br><span class="line">                (3.2, 3.3) |          0 | </span><br><span class="line">                (3.3, 3.4) |          0 | </span><br><span class="line">                (3.4, 3.5) |          0 | </span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:44:55: myMax<span class="built_in">_</span>10 | Stats: mean=3, std-dev=0, var=0, median=3, min=3 at (0, 0), max=3 at (0, 0), avg-magnitude=3</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (2.5, 2.6) |          0 | </span><br><span class="line">                (2.6, 2.7) |          0 | </span><br><span class="line">                (2.7, 2.8) |          0 | </span><br><span class="line">                (2.8, 2.9) |          0 | </span><br><span class="line">                (2.9, 3  ) |          0 | </span><br><span class="line">                (3  , 3.1) |       4096 | <span class="params">########################################</span></span><br><span class="line">                (3.1, 3.2) |          0 | </span><br><span class="line">                (3.2, 3.3) |          0 | </span><br><span class="line">                (3.3, 3.4) |          0 | </span><br><span class="line">                (3.4, 3.5) |          0 | </span><br><span class="line">[I]         Error Metrics: myMax<span class="built_in">_</span>10</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0), max=0 at (0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |       4096 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0), max=0 at (0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |       4096 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;myMax<span class="built_in">_</span>10&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;myMax<span class="built_in">_</span>10&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:44:55 vs. trt-runner-N0-10/24/24-09:44:55 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 5.898s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-09-01.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --input-shapes tensor0:[64,64]</span><br><span class="line">[W] --workspace is deprecated and will be removed in Polygraphy 0.48.0. Use --pool-limit workspace:1000000000 instead.</span><br><span class="line">[I] RUNNING | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-09-02.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --input-shapes tensor0:[64,64]</span><br><span class="line">[V] Loaded Module: polygraphy | Version: 0.47.1 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/polygraphy&#x27;]</span><br><span class="line">[V] Loaded extension modules: []</span><br><span class="line">[V] Loaded Module: tensorrt | Version: 8.6.0 | Path: [&#x27;/home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/lib/python3.8/site-packages/tensorrt&#x27;]</span><br><span class="line">[I] Will generate inference input data according to provided TensorMetadata: &#123;tensor0 [shape=(64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:45:01  | Activating and starting inference</span><br><span class="line">[V] Loaded Module: onnxruntime | Version: 1.19.0 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/onnxruntime&#x27;]</span><br><span class="line">[I] Creating ONNX-Runtime Inference Session with providers: [&#x27;CPUExecutionProvider&#x27;]</span><br><span class="line">[V] Loaded Module: numpy | Version: 1.24.4 | Path: [&#x27;/home/zhaoyidong/.local/lib/python3.8/site-packages/numpy&#x27;]</span><br><span class="line">[V] Loading inputs from data loader</span><br><span class="line">[V] Generating data using numpy seed: 1</span><br><span class="line">[V] Input tensor: tensor0 | Generating input data in range: [0.0, 1.0]</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:45:01 </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(64, 64)]&#125;</span><br><span class="line">[V] onnxrt-runner-N0-10/24/24-09:45:01  | Input metadata is: &#123;tensor0 [dtype=float32, shape=(64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:45:01 </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;myMax<span class="built_in">_</span>10 [dtype=float32, shape=(64, 64)]&#125;</span><br><span class="line">[I] onnxrt-runner-N0-10/24/24-09:45:01  | Completed 1 iteration(s) in 0.2558 ms | Average inference time: 0.2558 ms.</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:45:01     | Activating and starting inference</span><br><span class="line">[V] [MemUsageChange] Init CUDA: CPU +13, GPU +0, now: CPU 31, GPU 848 (MiB)</span><br><span class="line">[V] [MemUsageChange] Init builder kernel library: CPU +1435, GPU +266, now: CPU 1542, GPU 1114 (MiB)</span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V] Input filename:   /media/byd/disk38/media<span class="built_in">_</span>zhaoyidong/test/OnnxGraphSurgeon/model-09-02.onnx</span><br><span class="line">[V] ONNX IR version:  0.0.9</span><br><span class="line">[V] Opset version:    11</span><br><span class="line">[V] Producer name:    </span><br><span class="line">[V] Producer version: </span><br><span class="line">[V] Domain:           </span><br><span class="line">[V] Model version:    0</span><br><span class="line">[V] Doc string:       </span><br><span class="line">[V] ----------------------------------------------------------------</span><br><span class="line">[V]     Setting TensorRT Optimization Profiles</span><br><span class="line">[V]     Input tensor: tensor0 (dtype=DataType.FLOAT, shape=(64, 64)) | Setting input tensor shapes to: (min=[64, 64], opt=[64, 64], max=[64, 64])</span><br><span class="line">[I]     Configuring with profiles: [Profile().add(&#x27;tensor0&#x27;, min=[64, 64], opt=[64, 64], max=[64, 64])]</span><br><span class="line">[I] Building engine with configuration:</span><br><span class="line">    Flags                  | []</span><br><span class="line">    Engine Capability      | EngineCapability.DEFAULT</span><br><span class="line">    Memory Pools           | [WORKSPACE: 953.67 MiB, TACTIC<span class="built_in">_</span>DRAM: 24209.12 MiB]</span><br><span class="line">    Tactic Sources         | [CUBLAS, CUBLAS<span class="built_in">_</span>LT, CUDNN, EDGE<span class="built_in">_</span>MASK<span class="built_in">_</span>CONVOLUTIONS, JIT<span class="built_in">_</span>CONVOLUTIONS]</span><br><span class="line">    Profiling Verbosity    | ProfilingVerbosity.DETAILED</span><br><span class="line">    Preview Features       | [FASTER<span class="built_in">_</span>DYNAMIC<span class="built_in">_</span>SHAPES<span class="built_in">_</span>0805, DISABLE<span class="built_in">_</span>EXTERNAL<span class="built_in">_</span>TACTIC<span class="built_in">_</span>SOURCES<span class="built_in">_</span>FOR<span class="built_in">_</span>CORE<span class="built_in">_</span>0805]</span><br><span class="line">[V] Graph optimization time: 0.000733574 seconds.</span><br><span class="line">[V] Global timing cache in use. Profiling results in this builder pass will be stored.</span><br><span class="line">[V] Detected 1 inputs and 1 output network tensors.</span><br><span class="line">[V] Total Host Persistent Memory: 7648</span><br><span class="line">[V] Total Device Persistent Memory: 0</span><br><span class="line">[V] Total Scratch Memory: 0</span><br><span class="line">[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 8 MiB</span><br><span class="line">[V] [BlockAssignment] Started assigning block shifts. This will take 1 steps to complete.</span><br><span class="line">[V] [BlockAssignment] Algorithm ShiftNTopDown took 0.024197ms to assign 1 blocks to 1 nodes requiring 16384 bytes.</span><br><span class="line">[V] Total Activation Memory: 16384</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +4, now: CPU 0, GPU 4 (MiB)</span><br><span class="line">[I] Finished engine building in 1.614 seconds</span><br><span class="line">[V] Loaded engine size: 0 MiB</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +0, now: CPU 0, GPU 0 (MiB)</span><br><span class="line">[V] Found candidate CUDA libraries: [&#x27;/usr/local/cuda/lib64/libcudart.so&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.0&#x27;, &#x27;/usr/local/cuda/lib64/libcudart.so.11.4.43&#x27;]</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:45:01    </span><br><span class="line">    ---- Inference Input(s) ----</span><br><span class="line">    &#123;tensor0 [dtype=float32, shape=(64, 64)]&#125;</span><br><span class="line">[V] trt-runner-N0-10/24/24-09:45:01     | Input metadata is: &#123;tensor0 [dtype=float32, shape=(64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:45:01    </span><br><span class="line">    ---- Inference Output(s) ----</span><br><span class="line">    &#123;myMax<span class="built_in">_</span>10 [dtype=float32, shape=(64, 64)]&#125;</span><br><span class="line">[I] trt-runner-N0-10/24/24-09:45:01     | Completed 1 iteration(s) in 0.2401 ms | Average inference time: 0.2401 ms.</span><br><span class="line">[V] Successfully ran: [&#x27;onnxrt-runner-N0-10/24/24-09:45:01&#x27;, &#x27;trt-runner-N0-10/24/24-09:45:01&#x27;]</span><br><span class="line">[I] Accuracy Comparison | onnxrt-runner-N0-10/24/24-09:45:01 vs. trt-runner-N0-10/24/24-09:45:01</span><br><span class="line">[I]     Comparing Output: &#x27;myMax<span class="built_in">_</span>10&#x27; (dtype=float32, shape=(64, 64)) with &#x27;myMax<span class="built_in">_</span>10&#x27; (dtype=float32, shape=(64, 64))</span><br><span class="line">[I]         Tolerance: [abs=0.001, rel=0.001] | Checking elemwise error</span><br><span class="line">[I]         onnxrt-runner-N0-10/24/24-09:45:01: myMax<span class="built_in">_</span>10 | Stats: mean=3, std-dev=0, var=0, median=3, min=3 at (0, 0), max=3 at (0, 0), avg-magnitude=3</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (2.5, 2.6) |          0 | </span><br><span class="line">                (2.6, 2.7) |          0 | </span><br><span class="line">                (2.7, 2.8) |          0 | </span><br><span class="line">                (2.8, 2.9) |          0 | </span><br><span class="line">                (2.9, 3  ) |          0 | </span><br><span class="line">                (3  , 3.1) |       4096 | <span class="params">########################################</span></span><br><span class="line">                (3.1, 3.2) |          0 | </span><br><span class="line">                (3.2, 3.3) |          0 | </span><br><span class="line">                (3.3, 3.4) |          0 | </span><br><span class="line">                (3.4, 3.5) |          0 | </span><br><span class="line">[I]         trt-runner-N0-10/24/24-09:45:01: myMax<span class="built_in">_</span>10 | Stats: mean=3, std-dev=0, var=0, median=3, min=3 at (0, 0), max=3 at (0, 0), avg-magnitude=3</span><br><span class="line">[V]             ---- Histogram ----</span><br><span class="line">                Bin Range  |  Num Elems | Visualization</span><br><span class="line">                (2.5, 2.6) |          0 | </span><br><span class="line">                (2.6, 2.7) |          0 | </span><br><span class="line">                (2.7, 2.8) |          0 | </span><br><span class="line">                (2.8, 2.9) |          0 | </span><br><span class="line">                (2.9, 3  ) |          0 | </span><br><span class="line">                (3  , 3.1) |       4096 | <span class="params">########################################</span></span><br><span class="line">                (3.1, 3.2) |          0 | </span><br><span class="line">                (3.2, 3.3) |          0 | </span><br><span class="line">                (3.3, 3.4) |          0 | </span><br><span class="line">                (3.4, 3.5) |          0 | </span><br><span class="line">[I]         Error Metrics: myMax<span class="built_in">_</span>10</span><br><span class="line">[I]             Minimum Required Tolerance: elemwise error | [abs=0] OR [rel=0] (requirements may be lower if both abs/rel tolerances are set)</span><br><span class="line">[I]             Absolute Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0), max=0 at (0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |       4096 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]             Relative Difference | Stats: mean=0, std-dev=0, var=0, median=0, min=0 at (0, 0), max=0 at (0, 0), avg-magnitude=0</span><br><span class="line">[V]                 ---- Histogram ----</span><br><span class="line">                    Bin Range    |  Num Elems | Visualization</span><br><span class="line">                    (-0.5, -0.4) |          0 | </span><br><span class="line">                    (-0.4, -0.3) |          0 | </span><br><span class="line">                    (-0.3, -0.2) |          0 | </span><br><span class="line">                    (-0.2, -0.1) |          0 | </span><br><span class="line">                    (-0.1, 0   ) |          0 | </span><br><span class="line">                    (0   , 0.1 ) |       4096 | <span class="params">########################################</span></span><br><span class="line">                    (0.1 , 0.2 ) |          0 | </span><br><span class="line">                    (0.2 , 0.3 ) |          0 | </span><br><span class="line">                    (0.3 , 0.4 ) |          0 | </span><br><span class="line">                    (0.4 , 0.5 ) |          0 | </span><br><span class="line">[I]         PASSED | Output: &#x27;myMax<span class="built_in">_</span>10&#x27; | Difference is within tolerance (rel=0.001, abs=0.001)</span><br><span class="line">[I]     PASSED | All outputs matched | Outputs: [&#x27;myMax<span class="built_in">_</span>10&#x27;]</span><br><span class="line">[I] Accuracy Summary | onnxrt-runner-N0-10/24/24-09:45:01 vs. trt-runner-N0-10/24/24-09:45:01 | Passed: 1/1 iterations | Pass Rate: 100.0<span class="comment">%</span></span><br><span class="line">[I] PASSED | Runtime: 5.899s | Command: /home/zhaoyidong/anaconda3/envs/zyd<span class="built_in">_</span>env/bin/polygraphy run model-09-02.onnx --onnxrt --trt --workspace 1000000000 --atol 1e-3 --rtol 1e-3 --verbose --input-shapes tensor0:[64,64]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="2-5-polygraphy"><a href="#2-5-polygraphy" class="headerlink" title="2.5 polygraphy"></a>2.5 <strong>polygraphy</strong></h2><p>深度学习模型调试器，包含 python API 和命令行工具（这里只介绍命令行）</p>
<ul>
<li><p>功能：</p>
<ul>
<li><p>使用多种后端运行推理计算，包括 TensorRT，onnxruntime，TensorFlow</p>
</li>
<li><p>比较不同后端的逐层计算结果</p>
</li>
<li><p>由模型文件生成 TensorRT 引擎并序列化为 .plan</p>
</li>
<li><p>查看模型网络的逐层信息</p>
</li>
<li><p>修改 Onnx 模型，如提取子图，计算图化简</p>
</li>
<li><p>分析 Onnx 转 TensorRT 失败原因，将原计算图中可以 / 不可以转 TensorRT 的子图分割保存</p>
</li>
<li><p>隔离 TensorRT 中的错误 tactic</p>
</li>
</ul>
</li>
<li><p>下载和参考文档：</p>
<ul>
<li><p>pip install polygraphy</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/polygraphy/docs/index.html">https://docs.nvidia.com/deeplearning/tensorrt/polygraphy/docs/index.html</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31695/（onnx-graphsurgeon">https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31695/（onnx-graphsurgeon</a> 和 polygraphy 的一个视频教程）</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">polygraphy --<span class="built_in">help</span></span><br><span class="line">usage: polygraphy [-h] [-v] &#123;run,convert,inspect,surgeon,template,debug,data&#125; ...</span><br><span class="line"></span><br><span class="line">Polygraphy: A Deep Learning Debugging Toolkit</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --<span class="built_in">help</span>            show this <span class="built_in">help</span> message and <span class="built_in">exit</span></span><br><span class="line">  -v, --version         show program<span class="string">&#x27;s version number and exit</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Tools:</span></span><br><span class="line"><span class="string">  &#123;run,convert,inspect,surgeon,template,debug,data&#125;</span></span><br><span class="line"><span class="string">    run                 Run inference and compare results across backends.</span></span><br><span class="line"><span class="string">    convert             Convert models to other formats.</span></span><br><span class="line"><span class="string">    inspect             View information about various types of files.</span></span><br><span class="line"><span class="string">    surgeon             Modify ONNX models.</span></span><br><span class="line"><span class="string">    template            [EXPERIMENTAL] Generate template files.</span></span><br><span class="line"><span class="string">    debug               [EXPERIMENTAL] Debug a wide variety of model issues.</span></span><br><span class="line"><span class="string">    data                Manipulate input and output data generated by other Polygraphy subtools.</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>
<h3 id="2-5-1-run模式"><a href="#2-5-1-run模式" class="headerlink" title="2.5.1 run模式"></a>2.5.1 run模式</h3><p>Run 模式</p>
<ul>
<li><p>范例代码07-Tool/polygraphy/runExample，运行 ./command.sh</p>
</li>
<li><p>首先生成一个 .onnx 文件（同前面基于 MNIST 的模型）</p>
</li>
<li><p>其次使用 polygraphy 生成一个 FP16 的 TRT 引擎，并对比使用 onnxruntime 和 TensorRT 的计算结果</p>
</li>
<li><p>然后使用 polygraphy 生成一个 FP32 的 TRT 引擎，将网络中所有层都标记为输出，并对比使用 onnxruntime 和TensorRT 的计算结果（逐层结果对比）</p>
</li>
<li><p>最后（右图中没有）使用 polygraphy 生成一个“用于生成FP32 的 TRT 引擎”的 python 脚本，可用于理解polygraphy 的 API 和原理，这里不展开</p>
</li>
</ul>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024091135885.png" class="" title="image-20241024091135885">
<p>输出结果示例</p>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024091234428.png" class="" title="image-20241024091234428">
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 从 TensorFlow 创建一个 .onnx 用来做 polygraphy 的输入文件</span></span><br><span class="line">python3 getOnnxModel.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># 01 用上面的 .onnx 构建一个 TensorRT 引擎，使用 FP16精度，同时在 onnxruntime 和 TensorRT 中运行，对比结果</span></span><br><span class="line">polygraphy run model.onnx \</span><br><span class="line">    --onnxrt --trt \</span><br><span class="line">    --workspace 1000000000 \</span><br><span class="line">    --save-engine=model-FP16.plan \</span><br><span class="line">    --atol 1e-3 --rtol 1e-3 \</span><br><span class="line">    --fp16 \</span><br><span class="line">    --verbose \</span><br><span class="line">    --trt-min-shapes <span class="string">&#x27;tensor-0:[1,1,28,28]&#x27;</span> \</span><br><span class="line">    --trt-opt-shapes <span class="string">&#x27;tensor-0:[4,1,28,28]&#x27;</span> \</span><br><span class="line">    --trt-max-shapes <span class="string">&#x27;tensor-0:[16,1,28,28]&#x27;</span> \</span><br><span class="line">    --input-shapes   <span class="string">&#x27;tensor-0:[4,1,28,28]&#x27;</span> \</span><br><span class="line">    &gt; result-run-FP16.<span class="built_in">log</span> 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意参数名和格式跟 trtexec 不一样，多个形状之间用空格分隔，如：</span></span><br><span class="line"><span class="comment"># --trt-max-shapes &#x27;input0:[16,320,256]&#x27; &#x27;input1:[16，320]&#x27; &#x27;input2:[16]&#x27;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 02 用上面的 .onnx 构建一个 TensorRT 引擎，使用 FP32 精度，输出所有层的计算结果作对比</span></span><br><span class="line">polygraphy run model.onnx \</span><br><span class="line">    --onnxrt --trt \</span><br><span class="line">    --workspace 1000000000 \</span><br><span class="line">    --save-engine=model-FP32-MarkAll.plan \</span><br><span class="line">    --atol 1e-3 --rtol 1e-3 \</span><br><span class="line">    --verbose \</span><br><span class="line">    --onnx-outputs mark all \</span><br><span class="line">    --trt-outputs mark all \</span><br><span class="line">    --trt-min-shapes <span class="string">&#x27;tensor-0:[1,1,28,28]&#x27;</span> \</span><br><span class="line">    --trt-opt-shapes <span class="string">&#x27;tensor-0:[4,1,28,28]&#x27;</span> \</span><br><span class="line">    --trt-max-shapes <span class="string">&#x27;tensor-0:[16,1,28,28]&#x27;</span> \</span><br><span class="line">    --input-shapes   <span class="string">&#x27;tensor-0:[4,1,28,28]&#x27;</span></span><br><span class="line">    &gt; result-run-FP32-MarkAll.<span class="built_in">log</span> 2&gt;&amp;1</span><br></pre></td></tr></table></figure>
<p><strong>Run 模式常用选项</strong></p>
<ul>
<li><p>—model-type ：输入模型文件类型，可用 frozen, keras, ckpt, onnx, engine 等</p>
</li>
<li><p>—verbose ：打印详细日志</p>
</li>
<li><p>—trt —tf –onnxrt ：选用的后端（可以指定多个）</p>
</li>
<li><p>—input-shapes “x:[4,1,28,28]” “y:[4,8]” ：做推理时的实际数据形状，注意跟 trtexec 格式不同</p>
</li>
<li><p>—ckpt “./model.ckpt” —save-pb “./model.pb” —freeze-graph TensorFlow相关的模型在载入和保存</p>
</li>
<li><p>—save-onnx “.model.onnx” —opset 13 ：指定导出 Onnx 文件名和算子集编号</p>
</li>
<li><p>—shape-inference ：启用 Onnx 的形状推理功能</p>
</li>
<li><p>—trt-min-shapes ‘x:[1,1,28,28]’ ‘y:[1,8]’ —trt-opt-shapes ‘x:[4,1,28,28]’ ‘y:[4,8]’ —trt-max-shapes ‘x:[16,1,28,28]’ ‘y:[16,8]’：设定 TensorRT Dynamic Shape 的输入性状范围</p>
</li>
<li><p>—fp16 —int8 —tf32 —sparse-weights ：指定 TensorRT 中网络计算精度和稀疏性</p>
</li>
<li><p>—workspace 1G ：指定 TensorRT 工作空间大小，可用 M 或者 G 做后缀</p>
</li>
<li><p>—save-engine “./model.plan” ：指定 TensorRT 导出的 .plan 文件名</p>
</li>
<li><p>—rtol 1e-6 —atol 1e-6 —validate ：比较计算结果时的相对误差/绝对误差上限，并检查 NaN 和 INF</p>
</li>
</ul>
<h3 id="2-5-2-inspect-模式"><a href="#2-5-2-inspect-模式" class="headerlink" title="2.5.2 inspect 模式"></a>2.5.2 inspect 模式</h3><ul>
<li><p>范例代码07-Tool/polygraphy/inspectExample，运行 ./command.sh</p>
</li>
<li><p>首先生成一个 .onnx 文件（同前面基于 MNIST 的模型）</p>
</li>
<li><p>其次使用 polygraphy 导出 .onnx 详细信息（计算图属性、逐层属性、权重信息等）</p>
</li>
<li><p>然后生成一个 FP32 的 TRT 引擎，使用 polygraphy 导出 .plan 详细信息（网络属性、引擎binding信息，显存需求等）</p>
</li>
<li><p>使用 polygraphy 判断一个 .onnx 是否完整被 TensorRT 支持</p>
</li>
<li><p>故意生成一个 TensorRT 不完整支持的 .onnx（含一个 NonZero 节点），再次做上述判断</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">clear</span><br><span class="line"></span><br><span class="line"><span class="built_in">rm</span> -rf ./*.onnx ./*.plan ./result*.<span class="built_in">log</span> ./polygraphy_capability_dumps/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 .onnx 模型</span></span><br><span class="line">python3 getOnnxModel.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导出上面 .onnx 的详细信息</span></span><br><span class="line">polygraphy inspect model model.onnx \</span><br><span class="line">    --mode=full \</span><br><span class="line">    &gt; result-inspectOnnxModel.<span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用上面 .onnx 生成一个 .plan 及其相应的 tactics 用于后续分析</span></span><br><span class="line">polygraphy run model.onnx \</span><br><span class="line">    --trt \</span><br><span class="line">    --workspace 1000000000 \</span><br><span class="line">    --save-engine=<span class="string">&quot;./model.plan&quot;</span> \</span><br><span class="line">    --save-tactics=<span class="string">&quot;./model.tactic&quot;</span> \</span><br><span class="line">    --save-inputs=<span class="string">&quot;./model-input.log&quot;</span> \</span><br><span class="line">    --save-outputs=<span class="string">&quot;./model-output.log&quot;</span> \</span><br><span class="line">    --silent \</span><br><span class="line">    --trt-min-shapes <span class="string">&#x27;tensor-0:[1,1,28,28]&#x27;</span> \</span><br><span class="line">    --trt-opt-shapes <span class="string">&#x27;tensor-0:[4,1,28,28]&#x27;</span> \</span><br><span class="line">    --trt-max-shapes <span class="string">&#x27;tensor-0:[16,1,28,28]&#x27;</span> \</span><br><span class="line">    --input-shapes   <span class="string">&#x27;tensor-0:[4,1,28,28]&#x27;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 导出上面 .plan 的详细信息（since TensorRT 8.2）</span></span><br><span class="line">polygraphy inspect model model.plan \</span><br><span class="line">    --mode=full \</span><br><span class="line">    &gt; result-inspectPlanModel.<span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导出上面 .tactic 的信息，就是 json 转 text</span></span><br><span class="line">polygraphy inspect tactics model.tactic \</span><br><span class="line">    &gt; result-inspectPlanTactic.<span class="built_in">log</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 导出上面 .plan 输入输出数据信息</span></span><br><span class="line">polygraphy inspect data model-input.log \</span><br><span class="line">    &gt; result-inspectPlanInputData.<span class="built_in">log</span></span><br><span class="line"></span><br><span class="line">polygraphy inspect data model-output.log \</span><br><span class="line">    &gt; result-inspectPlanOutputData.<span class="built_in">log</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># 确认 TensorRT 是否完全原生支持该 .onnx</span></span><br><span class="line">polygraphy inspect capability model.onnx &gt; result-inspectCapability.<span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个含有 TensorRT 不原生支持的 .onnx，再次用 inspect capability 来确认</span></span><br><span class="line">python3 getOnnxModel-NonZero.py</span><br><span class="line"></span><br><span class="line">polygraphy inspect capability model-NonZero.onnx &gt; result-NonZero.<span class="built_in">log</span></span><br><span class="line"><span class="comment"># 产生目录 .results，包含网络分析信息和支持的子图(.onnx)、不支持的子图(.onnx)</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>输出结果示例</p>
<p>下图中polygraphy 的inspect 还帮我们切割了模型，因为中间的TensorRT不支持的节点NonZero，原模型被切割为三个部分。</p>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024092022891.png" class="" title="image-20241024092022891">
<h3 id="2-5-3-surgeon-模式"><a href="#2-5-3-surgeon-模式" class="headerlink" title="2.5.3 surgeon 模式"></a>2.5.3 surgeon 模式</h3><ul>
<li><p>范例代码08-Tool/polygraphy/surgeonExample，运行 ./command.sh</p>
</li>
<li><p>首先生成一个 .onnx 文件（一个 shape 操作相关的网络）</p>
</li>
<li><p>其次使用 polygraphy 优化其计算图</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">clear</span><br><span class="line"></span><br><span class="line"><span class="built_in">rm</span> -rf ./*.onnx ./*.plan ./result-*.<span class="built_in">log</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从 TensorFlow 创建一个 .onnx 用来做 polygraphy 的输入文件</span></span><br><span class="line">python3 getShapeOperateOnnxModel.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># 01 对上面的 .onnx 进行图优化</span></span><br><span class="line">polygraphy surgeon sanitize model.onnx \</span><br><span class="line">    --fold-constant \</span><br><span class="line">    -o model-foldConstant.onnx \</span><br><span class="line">    &gt; result-surgeon.log</span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024092200932.png" class="" title="image-20241024092200932">
<h3 id="2-5-4-其他模式"><a href="#2-5-4-其他模式" class="headerlink" title="2.5.4 其他模式"></a>2.5.4 其他模式</h3><ul>
<li><p>convert 模式</p>
<ul>
<li><p>基本同 run 模式</p>
</li>
<li><p>范例代码：08-Tool\Polygraphy\convertExample</p>
</li>
</ul>
</li>
<li><p>debug 模式</p>
<ul>
<li><p>检查模型转 TensorRT 时的错误并分离出一个可以运行的最大子图</p>
</li>
<li><p>范例代码：08-Tool\Polygraphy\debugExample</p>
</li>
</ul>
</li>
<li><p>data 模式</p>
<ul>
<li>调整和分析用于运行模型的输入输出、权重数据</li>
</ul>
</li>
<li><p>template 模式</p>
<ul>
<li><p>用于生成 polygraphy 的 python 脚本，使用脚本对模型进行调整</p>
</li>
<li><p>范例代码：08-Tool\Polygraphy\templateExample</p>
</li>
</ul>
</li>
</ul>
<h2 id="2-6-nsight-systems"><a href="#2-6-nsight-systems" class="headerlink" title="2.6 nsight systems"></a>2.6 nsight systems</h2><p>性能调试器</p>
<ul>
<li><p>随 CUDA 安装或独立安装，位于 /usr/local/cuda/bin/ 下的 nsys 和 nsys-ui</p>
</li>
<li><p>替代旧工具 nvprof 和 nvvp</p>
</li>
<li><p>首先命令行运行 nsys profile XXX，获得 .qdrep 或 .qdrep-nsys 文件</p>
</li>
<li><p>然后打开 nsys-ui，将上述文件拖入即可观察 timeline</p>
</li>
</ul>
<p>建议</p>
<ul>
<li><p>只计量运行阶段，而不分析构建期</p>
</li>
<li><p>构建期打开 profiling 以便获得关于 Layer 的更多信息</p>
</li>
<li><p>builder_config.profilling_verbosity = trt.ProfilingVerbosity.DETAILED</p>
</li>
<li><p>范例代码 08-Advance\ProfilingVerbosity</p>
</li>
<li><p>可以搭配 trtexec 使用：nsys profile -o myProfile trtexec —loadEngine=model.plan —warmUp=0 —duration=0 —iterations=50</p>
</li>
<li><p>也可以配合自己的 script 使用：nsys profile -o myProfile python myScript.py</p>
</li>
<li><p>务必将 nsys 更新到最新版，nsys 不能前向兼容</p>
</li>
</ul>
<p>范例代码08-Tool\NsightSystem，运行 ./command.sh</p>
<p>范例在 TensorFlow 上训练一个 MNIST 手写识别模型，然后在 TensorRT中用 API 重新搭建该模型，并加载训练好的权重，最后进行 30 次推理</p>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024093553154.png" class="" title="image-20241024093553154">
<ul>
<li>整个 timeline</li>
<li>建议使用 nsys 只计量 TensoRT 运行部分的时间（上面 timeline 包含 TF 训练、TRT 构建和 TRT 运行</li>
</ul>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024093636233.png" class="" title="image-20241024093636233">
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024093656626.png" class="" title="image-20241024093656626">
<ul>
<li>计算部分的 timeline（GPU timeline 要从左边 CUDA HW 行展开，Threads 下面的是 CPU 端的情况）</li>
<li>一共有 31 次 ExecutionContext::enqueue（橘黄色），对应 31 次 context.execute 调用</li>
<li>下面的黄色矩形对应网络中各层，蓝色矩形为相应的 cuda kernel 的调用</li>
<li>首次推理（warming up）为完整推理过程{，前后包含一些 memcpy，后续 30 次只做计算不做 memcpy</li>
</ul>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024093756458.png" class="" title="image-20241024093756458">
<ul>
<li>单次 inference 的 timeline，展开 GPU部分</li>
<li>9 个黄矩形代表网络的 9 个层：<ul>
<li>Reformat、Conv+ReLU、Pooling、Conv+ReLU、Pooling、Shuffle、FullyConnected+ReLU、 FullyConnected+ReLU、softmax+TopK</li>
</ul>
</li>
<li>CPU 端调用 cuda API 与 GPU 端实际执行 kernel 之间存在滞后</li>
</ul>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024093940935.png" class="" title="image-20241024093940935">
<p>列出 GPU kernel 运行情况（在 [All Streams] 那一行右键选择 “show in events”）</p>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024094013676.png" class="" title="image-20241024094013676">
<p>列出 TensorRT 的 NVTX（在 TensorRT 那一行右键选择 “show in events”）</p>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/image-20241024094039101.png" class="" title="image-20241024094039101">
<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><ul>
<li><p>Netron <a target="_blank" rel="noopener" href="https://github.com/lutzroeder/Netron">https://github.com/lutzroeder/Netron</a></p>
</li>
<li><p>onnx-graphsurgeon</p>
<ul>
<li><p>pip install nvidia-pyindex onnx-graphsurgeon</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeon/examples">https://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeon/examples</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/onnx-graphsurgeon/docs/index.html">https://docs.nvidia.com/deeplearning/tensorrt/onnx-graphsurgeon/docs/index.html</a></p>
</li>
</ul>
</li>
<li><p>polygraphy</p>
<ul>
<li><p>pip install polygraphy</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/polygraphy/docs/index.html">https://docs.nvidia.com/deeplearning/tensorrt/polygraphy/docs/index.html</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31695/（onnx-graphsurgeon">https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31695/（onnx-graphsurgeon</a> 和 polygraphy 的一个视频教程）</p>
</li>
</ul>
</li>
<li><p>nsight system</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/nsight-systems">https://developer.nvidia.com/nsight-systems</a> </li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html">https://docs.nvidia.com/nsight-systems/UserGuide/index.html</a></li>
</ul>
</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>奔跑的IC
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/" title="Part2-TensorRT开发辅助工具">http://example.com/TensorRT/TensorRT教程-基于8.6.1/Part2-TensorRT开发辅助工具/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/git/" rel="tag"><i class="fa fa-tag"></i> git</a>
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C</a>
              <a href="/tags/GPU/" rel="tag"><i class="fa fa-tag"></i> GPU</a>
              <a href="/tags/Tensorrt/" rel="tag"><i class="fa fa-tag"></i> Tensorrt</a>
              <a href="/tags/DeepLearning/" rel="tag"><i class="fa fa-tag"></i> DeepLearning</a>
              <a href="/tags/Plugin/" rel="tag"><i class="fa fa-tag"></i> Plugin</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part1-TensorRT%E7%AE%80%E4%BB%8B/" rel="prev" title="Part1-TensorRT简介">
                  <i class="fa fa-chevron-left"></i> Part1-TensorRT简介
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/" rel="next" title="Part4-TensorRT高级用法">
                  Part4-TensorRT高级用法 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">奔跑的IC</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"zmurder","repo":"zmurder.github.io","client_id":"cf2343f27b6c29efe0bc","client_secret":"3268a1fa92706c7358d5421f88f76a0f7ada3188","admin_user":"zmurder","distraction_free_mode":true,"proxy":"https://strong-caramel-969805.netlify.app/github_access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"4a2b0f12745d48d8ba08599396a1e50b"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
