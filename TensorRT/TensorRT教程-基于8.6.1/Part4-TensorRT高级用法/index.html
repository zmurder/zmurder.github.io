<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
  <meta name="msvalidate.01" content="7EC20DBC74B004C2782077570E15C280">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
    <meta name="description" content="1 背景本文档是记录学习Nvidia官方B站的视频，参考对应的PDF文件 TensorRTTraining-TRT8.6.1-Part5-V1.1.pdf 的记录。对应的官方代码[trt-samples-for-hackathon-cn]">
<meta property="og:type" content="article">
<meta property="og:title" content="Part4-TensorRT高级用法">
<meta property="og:url" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/index.html">
<meta property="og:site_name" content="奔跑的IC">
<meta property="og:description" content="1 背景本文档是记录学习Nvidia官方B站的视频，参考对应的PDF文件 TensorRTTraining-TRT8.6.1-Part5-V1.1.pdf 的记录。对应的官方代码[trt-samples-for-hackathon-cn]">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241022204349489.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105162200691.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105163248361.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105163300023.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105165151479.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105165353944.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105170228979.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105171938919.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105173548603.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105173713886.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105173918880.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105174150765.png">
<meta property="article:published_time" content="2024-12-01T10:13:45.482Z">
<meta property="article:modified_time" content="2024-12-01T10:13:45.482Z">
<meta property="article:author" content="奔跑的IC">
<meta property="article:tag" content="git">
<meta property="article:tag" content="C">
<meta property="article:tag" content="GPU">
<meta property="article:tag" content="Tensorrt">
<meta property="article:tag" content="DeepLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241022204349489.png">


<link rel="canonical" href="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/","path":"TensorRT/TensorRT教程-基于8.6.1/Part4-TensorRT高级用法/","title":"Part4-TensorRT高级用法"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Part4-TensorRT高级用法 | 奔跑的IC</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">奔跑的IC</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-reorder fa-fw"></i>文章列表</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E8%83%8C%E6%99%AF"><span class="nav-text">1 背景</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95"><span class="nav-text">2 TensorRT高级用法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E5%A4%9A-OptimizationProfile"><span class="nav-text">2.1 多 OptimizationProfile</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E5%A4%9Astream"><span class="nav-text">2.2 多stream</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-%E5%A4%9AContext"><span class="nav-text">2.3 多Context</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-4-CUDA-Graph"><span class="nav-text">2.4 CUDA Graph</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-5-Timing-Cache"><span class="nav-text">2.5 Timing Cache</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-6-Algorithm-Selector"><span class="nav-text">2.6 Algorithm Selector</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-7-Refit"><span class="nav-text">2.7 Refit</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-8-Tactic-Source"><span class="nav-text">2.8 Tactic Source</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-9-%E7%A1%AC%E4%BB%B6%E5%85%BC%E5%AE%B9"><span class="nav-text">2.9 硬件兼容</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-text">附录</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="奔跑的IC"
      src="/images/zyd.gif">
  <p class="site-author-name" itemprop="name">奔跑的IC</p>
  <div class="site-description" itemprop="description">死磕牛角的IT农民工</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">174</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zyd.gif">
      <meta itemprop="name" content="奔跑的IC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="奔跑的IC">
      <meta itemprop="description" content="死磕牛角的IT农民工">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Part4-TensorRT高级用法 | 奔跑的IC">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Part4-TensorRT高级用法
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-01 18:13:45" itemprop="dateCreated datePublished" datetime="2024-12-01T18:13:45+08:00">2024-12-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TensorRT/" itemprop="url" rel="index"><span itemprop="name">TensorRT</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8-6-1/" itemprop="url" rel="index"><span itemprop="name">TensorRT教程-基于8.6.1</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h1><p>本文档是记录学习Nvidia官方B站的<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1jj411Z7wG?spm_id_from=333.788.videopod.sections&amp;vd_source=cde2e7b9bca1a7048a13eaf0b48210b6">视频</a>，参考对应的PDF文件 TensorRTTraining-TRT8.6.1-Part5-V1.1.pdf 的记录。对应的官方代码<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/trt-samples-for-hackathon-cn">[trt-samples-for-hackathon-cn]</a></p>
<p>官方的视频教程基于TensorRT8.6.1版本。但是官方代码没有对应的tag。只有8.4、8.5和截至目前最新的8.10（master分支）。因此我这里参考的都是8.4分支的代码。</p>
<ul>
<li>part1 TensorRT简介</li>
<li>part2 开发辅助工具</li>
<li>part3 插件书写</li>
<li>part4 TensorRT高级用法</li>
<li>part5 常见优化策略</li>
</ul>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241022204349489.png" class="" title="image-20241022204349489">
<h1 id="2-TensorRT高级用法"><a href="#2-TensorRT高级用法" class="headerlink" title="2 TensorRT高级用法"></a>2 TensorRT高级用法</h1><ul>
<li>多 Optimization Profile</li>
<li>多 Context 与多 Stream</li>
<li>CUDA Graph</li>
<li>Timing Cache</li>
<li>Algorithm Selector</li>
<li>Refit</li>
<li>Tactic Source</li>
<li>Hardware compatibility 和 Version compatibility</li>
<li>更多工具</li>
</ul>
<ul>
<li>希望解决的问题<ul>
<li>Dynamic Shape 模式在 min-opt-max 跨度较大时性能下降？</li>
<li>怎样重叠计算和数据拷贝的时间，增加GPU利用率？</li>
<li>怎样使一个 engine 供多个线程使用？</li>
<li>怎么样优化 Kernel 的调用，减少 Launch Bound 的发生？</li>
<li>engine 构建时间太长，怎么节约多次构建的时间？</li>
<li>某些 Layer 的算法导致较大的误差，能不能屏蔽掉该选择？</li>
<li>想更新模型的权重，但又不想重新构建 engine？</li>
<li>构建期/运行期显存占用过大，怎么减少？</li>
<li>能不能跨硬件或 TensorRT 版本运行 engine？</li>
</ul>
</li>
</ul>
<p>因为开发环境变化，这里使用的tensorrt都是TODO版本</p>
<h2 id="2-1-多-OptimizationProfile"><a href="#2-1-多-OptimizationProfile" class="headerlink" title="2.1 多 OptimizationProfile"></a>2.1 多 OptimizationProfile</h2><p>Dynamic Shape 模式在 min-opt-max 跨度较大时性能下降</p>
<p>解决方法：造多个 OptimizationProfile</p>
<p>范例代码08-Advance\MultiProfile</p>
<ul>
<li>要点<ul>
<li>缩小每个 Profile 范围，方便 TensorRT 自动优化，多个profile的动态范围可以重叠</li>
<li>推理时，根据数据形状选择相应 Profile</li>
<li>注意输入输出数据的绑定位置</li>
</ul>
</li>
<li>间接作用<ul>
<li>多 Context 的基础工作</li>
<li>增加显存占用、引擎尺寸和 .plan 尺寸</li>
</ul>
</li>
</ul>
<p>TODO 本机测试并验证，没看明白</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br><span class="line"><span class="keyword">from</span> cuda <span class="keyword">import</span> cudart</span><br><span class="line"></span><br><span class="line">shape = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">nProfile = <span class="number">2</span>  <span class="comment"># count of OptimizationProfile</span></span><br><span class="line">np.random.seed(<span class="number">31193</span>)</span><br><span class="line">np.set_printoptions(precision=<span class="number">3</span>, linewidth=<span class="number">200</span>, suppress=<span class="literal">True</span>)</span><br><span class="line">cudart.cudaDeviceSynchronize()</span><br><span class="line"></span><br><span class="line">logger = trt.Logger(trt.Logger.ERROR)</span><br><span class="line">builder = trt.Builder(logger)</span><br><span class="line">network = builder.create_network(<span class="number">1</span> &lt;&lt; <span class="built_in">int</span>(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))</span><br><span class="line">profileList = [builder.create_optimization_profile() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(nProfile)]</span><br><span class="line">config = builder.create_builder_config()</span><br><span class="line"></span><br><span class="line">inputT0 = network.add_input(<span class="string">&quot;inputT0&quot;</span>, trt.float32, [-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line">inputT1 = network.add_input(<span class="string">&quot;inputT1&quot;</span>, trt.float32, [-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> profile <span class="keyword">in</span> profileList:</span><br><span class="line">    profile.set_shape(inputT0.name, shape, shape, [k * nProfile <span class="keyword">for</span> k <span class="keyword">in</span> shape])  <span class="comment"># &quot;* nProfile&quot; is just for this example, not required in real use case</span></span><br><span class="line">    profile.set_shape(inputT1.name, shape, shape, [k * nProfile <span class="keyword">for</span> k <span class="keyword">in</span> shape])</span><br><span class="line">    config.add_optimization_profile(profile)</span><br><span class="line"></span><br><span class="line">layer = network.add_elementwise(inputT0, inputT1, trt.ElementWiseOperation.SUM)</span><br><span class="line">network.mark_output(layer.get_output(<span class="number">0</span>))</span><br><span class="line">engineString = builder.build_serialized_network(network, config)</span><br><span class="line">engine = trt.Runtime(logger).deserialize_cuda_engine(engineString)</span><br><span class="line">nIO = engine.num_io_tensors</span><br><span class="line">lTensorName = [engine.get_tensor_name(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nIO)]</span><br><span class="line">nInput = [engine.get_tensor_mode(lTensorName[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nIO)].count(trt.TensorIOMode.INPUT)</span><br><span class="line"></span><br><span class="line">context = engine.create_execution_context()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(nProfile):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Use Profile %d&quot;</span> % index)</span><br><span class="line">    context.set_optimization_profile_async(index, <span class="number">0</span>)  <span class="comment"># use default stream</span></span><br><span class="line">    inputShape = [k * (index + <span class="number">1</span>) <span class="keyword">for</span> k <span class="keyword">in</span> shape]  <span class="comment"># we use different shape for various context in this example, not required in real use case</span></span><br><span class="line">    context.set_input_shape(lTensorName[<span class="number">0</span>], inputShape)</span><br><span class="line">    context.set_input_shape(lTensorName[<span class="number">1</span>], inputShape)</span><br><span class="line">    bufferH = []  <span class="comment"># use respective buffers for different Optimization Profile</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput):</span><br><span class="line">        bufferH.append(np.arange(np.prod(inputShape)).astype(np.float32).reshape(inputShape))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput, nIO):</span><br><span class="line">        bufferH.append(np.empty(context.get_tensor_shape(lTensorName[i]), dtype=trt.nptype(engine.get_tensor_dtype(lTensorName[i]))))</span><br><span class="line"></span><br><span class="line">    bufferD = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(bufferH)):</span><br><span class="line">        bufferD.append(cudart.cudaMalloc(bufferH[i].nbytes)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nIO):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[%2d]%s-&gt;&quot;</span> % (i, <span class="string">&quot;Input &quot;</span> <span class="keyword">if</span> i &lt; nInput <span class="keyword">else</span> <span class="string">&quot;Output&quot;</span>), engine.get_tensor_dtype(lTensorName[i]), engine.get_tensor_shape(lTensorName[i]), context.get_tensor_shape(lTensorName[i]), lTensorName[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput):</span><br><span class="line">        cudart.cudaMemcpyAsync(bufferD[i], bufferH[i].ctypes.data, bufferH[i].nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nIO):</span><br><span class="line">        context.set_tensor_address(lTensorName[i], <span class="built_in">int</span>(bufferD[i]))</span><br><span class="line"></span><br><span class="line">    context.execute_async_v3(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput, nIO):</span><br><span class="line">        cudart.cudaMemcpyAsync(bufferH[i].ctypes.data, bufferD[i], bufferH[i].nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;check result of OptimizationProfile %d: %s&quot;</span> % (index, np.<span class="built_in">all</span>(bufferH[<span class="number">2</span>] == bufferH[<span class="number">0</span>] + bufferH[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> bufferD:</span><br><span class="line">        cudart.cudaFree(b)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="2-2-多stream"><a href="#2-2-多stream" class="headerlink" title="2.2 多stream"></a>2.2 多stream</h2><p>CUDA 编程经典话题：重叠计算和数据拷贝</p>
<ul>
<li>解决方法：恰当使用 Stream</li>
<li>与GPU相关的异步操作，包括异步内存申请释放、kernel执行等，都可以放在一个或者多个stream中，同一个stream的函数调用会根据函数的加入顺序执行，不同的stream之间独立（不使用event时）</li>
<li>范例代码：08-Advance\MultiStream</li>
</ul>
<ul>
<li><p>使用 CUDA event 和 CUDA stream</p>
<p>下图中stream0执行 kernel0-kernel1-eventRecord。同时stream2执行kernel2-StreamWaitEvent（等待stream0的eventRecord）-kernel3。</p>
<p>相当与kernel3必须在kernel0  1 2执行完成后再执行。</p>
</li>
</ul>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105162200691.png" class="" title="image-20241105162200691">
<ul>
<li><p>使用 pinned-memory</p>
<p>通常我们使用的CPU内存是可分页的，可以被交换到文件中以减少内存的使用量，但是pinned memory是不可分页的，一定在内存中。我们使用cuda的异步函数调用是一定要使用pinned memory。因为例如异步内存拷贝等都是异步的，真正执行的时候可能需要的内存被交换出去了，又要再换回来，浪费时间。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br><span class="line"><span class="keyword">from</span> cuda <span class="keyword">import</span> cudart</span><br><span class="line"></span><br><span class="line">trtFile = <span class="string">&quot;./model.plan&quot;</span></span><br><span class="line">np.random.seed(<span class="number">31193</span>)</span><br><span class="line"></span><br><span class="line">nWarmUp = <span class="number">10</span></span><br><span class="line">nTest = <span class="number">30</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># There are 3 scenarios of the inference</span></span><br><span class="line"><span class="comment"># 1. HtoD-bound</span></span><br><span class="line"></span><br><span class="line">nB, nC, nH, nW = <span class="number">8</span>, <span class="number">64</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line">nCOut, nKernelHeight, nKernelWidth = <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Calculation-bound</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">nB,nC,nH,nW = 8,64,128,128</span></span><br><span class="line"><span class="string">nCOut,nKernelHeight,nKernelWidth    = 64,9,9</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 3. DtoH-bound</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">nB,nC,nH,nW = 8,64,128,128</span></span><br><span class="line"><span class="string">nCOut,nKernelHeight,nKernelWidth    = 256,3,3</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getEngine</span>():</span><br><span class="line">    logger = trt.Logger(trt.Logger.ERROR)</span><br><span class="line">    <span class="keyword">if</span> os.path.isfile(trtFile):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(trtFile, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            engineString = f.read()</span><br><span class="line">        <span class="keyword">if</span> engineString == <span class="literal">None</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Failed getting serialized engine!&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Succeeded getting serialized engine!&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        builder = trt.Builder(logger)</span><br><span class="line">        network = builder.create_network(<span class="number">1</span> &lt;&lt; <span class="built_in">int</span>(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))</span><br><span class="line">        profile = builder.create_optimization_profile()</span><br><span class="line">        config = builder.create_builder_config()</span><br><span class="line"></span><br><span class="line">        inputTensor = network.add_input(<span class="string">&quot;inputT0&quot;</span>, trt.float32, [-<span class="number">1</span>, nC, nH, nW])</span><br><span class="line">        profile.set_shape(inputTensor.name, [<span class="number">1</span>, nC, nH, nW], [nB, nC, nH, nW], [nB * <span class="number">2</span>, nC, nH, nW])</span><br><span class="line">        config.add_optimization_profile(profile)</span><br><span class="line"></span><br><span class="line">        w = np.ascontiguousarray(np.random.rand(nCOut, nC, nKernelHeight, nKernelWidth).astype(np.float32) * <span class="number">2</span> - <span class="number">1</span>)</span><br><span class="line">        b = np.ascontiguousarray(np.random.rand(nCOut).astype(np.float32) * <span class="number">2</span> - <span class="number">1</span>)</span><br><span class="line">        _0 = network.add_convolution_nd(inputTensor, nCOut, [nKernelHeight, nKernelWidth], trt.Weights(w), trt.Weights(b))</span><br><span class="line">        _0.padding_nd = (nKernelHeight // <span class="number">2</span>, nKernelWidth // <span class="number">2</span>)</span><br><span class="line">        _1 = network.add_activation(_0.get_output(<span class="number">0</span>), trt.ActivationType.RELU)</span><br><span class="line"></span><br><span class="line">        network.mark_output(_1.get_output(<span class="number">0</span>))</span><br><span class="line">        engineString = builder.build_serialized_network(network, config)</span><br><span class="line">        <span class="keyword">if</span> engineString == <span class="literal">None</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Failed building serialized engine!&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Succeeded building serialized engine!&quot;</span>)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(trtFile, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(engineString)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Succeeded saving .plan file!&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> trt.Runtime(logger).deserialize_cuda_engine(engineString)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run1</span>(<span class="params">engine</span>):</span><br><span class="line">    context = engine.create_execution_context()</span><br><span class="line">    context.set_binding_shape(<span class="number">0</span>, [nB, nC, nH, nW])</span><br><span class="line">    _, stream = cudart.cudaStreamCreate()</span><br><span class="line"></span><br><span class="line">    data = np.random.rand(nB * nC * nH * nW).astype(np.float32).reshape(nB, nC, nH, nW)</span><br><span class="line">    inputH0 = np.ascontiguousarray(data.reshape(-<span class="number">1</span>))</span><br><span class="line">    outputH0 = np.empty(context.get_binding_shape(<span class="number">1</span>), dtype=trt.nptype(engine.get_binding_dtype(<span class="number">1</span>)))</span><br><span class="line">    _, inputD0 = cudart.cudaMallocAsync(inputH0.nbytes, stream)</span><br><span class="line">    _, outputD0 = cudart.cudaMallocAsync(outputH0.nbytes, stream)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># do a complete inference</span></span><br><span class="line">    cudart.cudaMemcpyAsync(inputD0, inputH0.ctypes.data, inputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)</span><br><span class="line">    context.execute_async_v2([<span class="built_in">int</span>(inputD0), <span class="built_in">int</span>(outputD0)], stream)</span><br><span class="line">    cudart.cudaMemcpyAsync(outputH0.ctypes.data, outputD0, outputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)</span><br><span class="line">    cudart.cudaStreamSynchronize(stream)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Count time of memory copy from host to device</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nWarmUp):</span><br><span class="line">        cudart.cudaMemcpyAsync(inputD0, inputH0.ctypes.data, inputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)</span><br><span class="line"></span><br><span class="line">    trtTimeStart = time()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nTest):</span><br><span class="line">        cudart.cudaMemcpyAsync(inputD0, inputH0.ctypes.data, inputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)</span><br><span class="line">    cudart.cudaStreamSynchronize(stream)</span><br><span class="line">    trtTimeEnd = time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%6.3fms - 1 stream, DataCopyHtoD&quot;</span> % ((trtTimeEnd - trtTimeStart) / nTest * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Count time of inference</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nWarmUp):</span><br><span class="line">        context.execute_async_v2([<span class="built_in">int</span>(inputD0), <span class="built_in">int</span>(outputD0)], stream)</span><br><span class="line"></span><br><span class="line">    trtTimeStart = time()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nTest):</span><br><span class="line">        context.execute_async_v2([<span class="built_in">int</span>(inputD0), <span class="built_in">int</span>(outputD0)], stream)</span><br><span class="line">    cudart.cudaStreamSynchronize(stream)</span><br><span class="line">    trtTimeEnd = time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%6.3fms - 1 stream, Inference&quot;</span> % ((trtTimeEnd - trtTimeStart) / nTest * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Count time of memory copy from device to host</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nWarmUp):</span><br><span class="line">        cudart.cudaMemcpyAsync(outputH0.ctypes.data, outputD0, outputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)</span><br><span class="line"></span><br><span class="line">    trtTimeStart = time()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nTest):</span><br><span class="line">        cudart.cudaMemcpyAsync(outputH0.ctypes.data, outputD0, outputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)</span><br><span class="line">    cudart.cudaStreamSynchronize(stream)</span><br><span class="line">    trtTimeEnd = time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%6.3fms - 1 stream, DataCopyDtoH&quot;</span> % ((trtTimeEnd - trtTimeStart) / nTest * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Count time of end to end</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nWarmUp):</span><br><span class="line">        context.execute_async_v2([<span class="built_in">int</span>(inputD0), <span class="built_in">int</span>(outputD0)], stream)</span><br><span class="line"></span><br><span class="line">    trtTimeStart = time()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nTest):</span><br><span class="line">        cudart.cudaMemcpyAsync(inputD0, inputH0.ctypes.data, inputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)</span><br><span class="line">        context.execute_async_v2([<span class="built_in">int</span>(inputD0), <span class="built_in">int</span>(outputD0)], stream)</span><br><span class="line">        cudart.cudaMemcpyAsync(outputH0.ctypes.data, outputD0, outputH0.nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)</span><br><span class="line">    cudart.cudaStreamSynchronize(stream)</span><br><span class="line">    trtTimeEnd = time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%6.3fms - 1 stream, DataCopy + Inference&quot;</span> % ((trtTimeEnd - trtTimeStart) / nTest * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    cudart.cudaStreamDestroy(stream)</span><br><span class="line">    cudart.cudaFree(inputD0)</span><br><span class="line">    cudart.cudaFree(outputD0)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run2</span>(<span class="params">engine</span>):</span><br><span class="line">    context = engine.create_execution_context()</span><br><span class="line">    context.set_binding_shape(<span class="number">0</span>, [nB, nC, nH, nW])</span><br><span class="line">    _, stream0 = cudart.cudaStreamCreate()</span><br><span class="line">    _, stream1 = cudart.cudaStreamCreate()</span><br><span class="line">    _, event0 = cudart.cudaEventCreate()</span><br><span class="line">    _, event1 = cudart.cudaEventCreate()</span><br><span class="line"></span><br><span class="line">    data = np.random.rand(nB * nC * nH * nW).astype(np.float32).reshape(nB, nC, nH, nW)</span><br><span class="line">    inputSize = trt.volume(context.get_binding_shape(<span class="number">0</span>)) * np.array([<span class="number">0</span>], dtype=trt.nptype(engine.get_binding_dtype(<span class="number">0</span>))).nbytes</span><br><span class="line">    outputSize = trt.volume(context.get_binding_shape(<span class="number">1</span>)) * np.array([<span class="number">0</span>], dtype=trt.nptype(engine.get_binding_dtype(<span class="number">1</span>))).nbytes</span><br><span class="line">    _, inputH0 = cudart.cudaHostAlloc(inputSize, cudart.cudaHostAllocWriteCombined)</span><br><span class="line">    _, inputH1 = cudart.cudaHostAlloc(inputSize, cudart.cudaHostAllocWriteCombined)</span><br><span class="line">    _, outputH0 = cudart.cudaHostAlloc(outputSize, cudart.cudaHostAllocWriteCombined)</span><br><span class="line">    _, outputH1 = cudart.cudaHostAlloc(outputSize, cudart.cudaHostAllocWriteCombined)</span><br><span class="line">    _, inputD0 = cudart.cudaMallocAsync(inputSize, stream0)</span><br><span class="line">    _, inputD1 = cudart.cudaMallocAsync(inputSize, stream1)</span><br><span class="line">    _, outputD0 = cudart.cudaMallocAsync(outputSize, stream0)</span><br><span class="line">    _, outputD1 = cudart.cudaMallocAsync(outputSize, stream1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Count time of end to end</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nWarmUp):</span><br><span class="line">        context.execute_async_v2([<span class="built_in">int</span>(inputD0), <span class="built_in">int</span>(outputD0)], stream0)</span><br><span class="line"></span><br><span class="line">    trtTimeStart = time()</span><br><span class="line">    cudart.cudaEventRecord(event1, stream1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nTest):</span><br><span class="line">        inputH, outputH = [inputH1, outputH1] <span class="keyword">if</span> i &amp; <span class="number">1</span> <span class="keyword">else</span> [inputH0, outputH0]</span><br><span class="line">        inputD, outputD = [inputD1, outputD1] <span class="keyword">if</span> i &amp; <span class="number">1</span> <span class="keyword">else</span> [inputD0, outputD0]</span><br><span class="line">        eventBefore, eventAfter = [event0, event1] <span class="keyword">if</span> i &amp; <span class="number">1</span> <span class="keyword">else</span> [event1, event0]</span><br><span class="line">        stream = stream1 <span class="keyword">if</span> i &amp; <span class="number">1</span> <span class="keyword">else</span> stream0</span><br><span class="line"></span><br><span class="line">        cudart.cudaMemcpyAsync(inputD, inputH, inputSize, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream)</span><br><span class="line">        cudart.cudaStreamWaitEvent(stream, eventBefore, cudart.cudaEventWaitDefault)</span><br><span class="line">        context.execute_async_v2([<span class="built_in">int</span>(inputD), <span class="built_in">int</span>(outputD)], stream)</span><br><span class="line">        cudart.cudaEventRecord(eventAfter, stream)</span><br><span class="line">        cudart.cudaMemcpyAsync(outputH, outputD, outputSize, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream)</span><br><span class="line">    <span class="string">&quot;&quot;&quot;# split the loop into odd and even iterations</span></span><br><span class="line"><span class="string">    for i in range(nTest//2):</span></span><br><span class="line"><span class="string">        cudart.cudaMemcpyAsync(inputD0, inputH0, inputSize, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream0)</span></span><br><span class="line"><span class="string">        cudart.cudaStreamWaitEvent(stream0,event1,cudart.cudaEventWaitDefault)</span></span><br><span class="line"><span class="string">        context.execute_async_v2([int(inputD0), int(outputD0)], stream0)</span></span><br><span class="line"><span class="string">        cudart.cudaEventRecord(event0,stream0)</span></span><br><span class="line"><span class="string">        cudart.cudaMemcpyAsync(outputH0, outputD0, outputSize, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream0)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        cudart.cudaMemcpyAsync(inputD1, inputH1, inputSize, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, stream1)</span></span><br><span class="line"><span class="string">        cudart.cudaStreamWaitEvent(stream1,event0,cudart.cudaEventWaitDefault)</span></span><br><span class="line"><span class="string">        context.execute_async_v2([int(inputD1), int(outputD1)], stream1)</span></span><br><span class="line"><span class="string">        cudart.cudaEventRecord(event1,stream1)</span></span><br><span class="line"><span class="string">        cudart.cudaMemcpyAsync(outputH1, outputD1, outputSize, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, stream1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    cudart.cudaEventSynchronize(event1)</span><br><span class="line">    trtTimeEnd = time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%6.3fms - 2 stream, DataCopy + Inference&quot;</span> % ((trtTimeEnd - trtTimeStart) / nTest * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    os.system(<span class="string">&quot;rm -rf ./*.plan&quot;</span>)</span><br><span class="line">    cudart.cudaDeviceSynchronize()</span><br><span class="line">    engine = getEngine()  <span class="comment"># build TensorRT engine</span></span><br><span class="line">    run1(engine)  <span class="comment"># do inference with single stream</span></span><br><span class="line">    run2(engine)  <span class="comment"># do inference with double stream</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105163248361.png" class="" title="image-20241105163248361">
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105163300023.png" class="" title="image-20241105163300023">
<h2 id="2-3-多Context"><a href="#2-3-多Context" class="headerlink" title="2.3 多Context"></a>2.3 多Context</h2><p>一个 Engine 供多个 Context 共用，之占用一个engine的显存，供多个线程推理计算。</p>
<ul>
<li>范例代码08-Advance\MultiContext</li>
<li>要点<ul>
<li>从 engine 中创建多个 context<ul>
<li>contextList = [engine.create_execution_context() for index in range(nContext)</li>
</ul>
</li>
<li>不再需要使用多个 OptimizationProfile （从TensorRT8.6开始），可以从示例代码中看细节</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br><span class="line"><span class="keyword">from</span> cuda <span class="keyword">import</span> cudart</span><br><span class="line"></span><br><span class="line">shape = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">nProfile = <span class="number">2</span>  <span class="comment"># count of OptimizationProfile</span></span><br><span class="line">np.random.seed(<span class="number">31193</span>)</span><br><span class="line">np.set_printoptions(precision=<span class="number">3</span>, linewidth=<span class="number">200</span>, suppress=<span class="literal">True</span>)</span><br><span class="line">cudart.cudaDeviceSynchronize()</span><br><span class="line"></span><br><span class="line">logger = trt.Logger(trt.Logger.ERROR)</span><br><span class="line">builder = trt.Builder(logger)</span><br><span class="line">network = builder.create_network(<span class="number">1</span> &lt;&lt; <span class="built_in">int</span>(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))</span><br><span class="line">profileList = [builder.create_optimization_profile() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(nProfile)]</span><br><span class="line">config = builder.create_builder_config()</span><br><span class="line"></span><br><span class="line">inputT0 = network.add_input(<span class="string">&quot;inputT0&quot;</span>, trt.float32, [-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line">inputT1 = network.add_input(<span class="string">&quot;inputT1&quot;</span>, trt.float32, [-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> profile <span class="keyword">in</span> profileList:</span><br><span class="line">    profile.set_shape(inputT0.name, shape, shape, [k * nProfile <span class="keyword">for</span> k <span class="keyword">in</span> shape])  <span class="comment"># &quot;* nProfile&quot; is just for this example, not required in real use case</span></span><br><span class="line">    profile.set_shape(inputT1.name, shape, shape, [k * nProfile <span class="keyword">for</span> k <span class="keyword">in</span> shape])</span><br><span class="line">    config.add_optimization_profile(profile)</span><br><span class="line"></span><br><span class="line">layer = network.add_elementwise(inputT0, inputT1, trt.ElementWiseOperation.SUM)</span><br><span class="line">network.mark_output(layer.get_output(<span class="number">0</span>))</span><br><span class="line">engineString = builder.build_serialized_network(network, config)</span><br><span class="line">engine = trt.Runtime(logger).deserialize_cuda_engine(engineString)</span><br><span class="line">nIO = engine.num_io_tensors</span><br><span class="line">lTensorName = [engine.get_tensor_name(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nIO)]</span><br><span class="line">nInput = [engine.get_tensor_mode(lTensorName[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nIO)].count(trt.TensorIOMode.INPUT)</span><br><span class="line"></span><br><span class="line">context = engine.create_execution_context()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(nProfile):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Use Profile %d&quot;</span> % index)</span><br><span class="line">    context.set_optimization_profile_async(index, <span class="number">0</span>)  <span class="comment"># use default stream</span></span><br><span class="line">    inputShape = [k * (index + <span class="number">1</span>) <span class="keyword">for</span> k <span class="keyword">in</span> shape]  <span class="comment"># we use different shape for various context in this example, not required in real use case</span></span><br><span class="line">    context.set_input_shape(lTensorName[<span class="number">0</span>], inputShape)</span><br><span class="line">    context.set_input_shape(lTensorName[<span class="number">1</span>], inputShape)</span><br><span class="line">    bufferH = []  <span class="comment"># use respective buffers for different Optimization Profile</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput):</span><br><span class="line">        bufferH.append(np.arange(np.prod(inputShape)).astype(np.float32).reshape(inputShape))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput, nIO):</span><br><span class="line">        bufferH.append(np.empty(context.get_tensor_shape(lTensorName[i]), dtype=trt.nptype(engine.get_tensor_dtype(lTensorName[i]))))</span><br><span class="line"></span><br><span class="line">    bufferD = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(bufferH)):</span><br><span class="line">        bufferD.append(cudart.cudaMalloc(bufferH[i].nbytes)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nIO):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[%2d]%s-&gt;&quot;</span> % (i, <span class="string">&quot;Input &quot;</span> <span class="keyword">if</span> i &lt; nInput <span class="keyword">else</span> <span class="string">&quot;Output&quot;</span>), engine.get_tensor_dtype(lTensorName[i]), engine.get_tensor_shape(lTensorName[i]), context.get_tensor_shape(lTensorName[i]), lTensorName[i])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput):</span><br><span class="line">        cudart.cudaMemcpyAsync(bufferD[i], bufferH[i].ctypes.data, bufferH[i].nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nIO):</span><br><span class="line">        context.set_tensor_address(lTensorName[i], <span class="built_in">int</span>(bufferD[i]))</span><br><span class="line"></span><br><span class="line">    context.execute_async_v3(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput, nIO):</span><br><span class="line">        cudart.cudaMemcpyAsync(bufferH[i].ctypes.data, bufferD[i], bufferH[i].nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;check result of OptimizationProfile %d: %s&quot;</span> % (index, np.<span class="built_in">all</span>(bufferH[<span class="number">2</span>] == bufferH[<span class="number">0</span>] + bufferH[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> bufferD:</span><br><span class="line">        cudart.cudaFree(b)</span><br></pre></td></tr></table></figure>
<h2 id="2-4-CUDA-Graph"><a href="#2-4-CUDA-Graph" class="headerlink" title="2.4 CUDA Graph"></a>2.4 CUDA Graph</h2><p>利用cuda Graph优化 Kernel 的调用，减少 Launch Bound 的发生</p>
<ul>
<li><p>范例代码 08-Advance\CudaGraph</p>
</li>
<li><p>优点：</p>
<ul>
<li>降低 CPU Launch cost</li>
<li>CUDA 工作流优化</li>
<li>缓解大量 kernel 调用时的 Launch Bound</li>
</ul>
</li>
<li>要点<ul>
<li>步骤：Graph 定义， Graph 实例化， Graph 执行（捕获 CUDA Graph 之前要运行一次推理）</li>
<li>Dynamic Shape 模式中，实际数据形状发生改变时（调用 context.set_binding_shape），<br>要先跑一遍 context.execute 再重新捕获 Graph，最后再实例化和执行 Graph</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> * Copyright (c) <span class="number">2021</span>-<span class="number">2023</span>, NVIDIA CORPORATION. All rights reserved.</span><br><span class="line"></span><br><span class="line"> *</span><br><span class="line"> * Licensed under the Apache License, Version <span class="number">2.0</span> (the <span class="string">&quot;License&quot;</span>);</span><br><span class="line"> * you may <span class="keyword">not</span> use this file <span class="keyword">except</span> <span class="keyword">in</span> compliance <span class="keyword">with</span> the License.</span><br><span class="line"> * You may obtain a copy of the License at</span><br><span class="line"> *</span><br><span class="line"> *     http://www.apache.org/licenses/LICENSE-<span class="number">2.0</span></span><br><span class="line"> *</span><br><span class="line"> * Unless required by applicable law <span class="keyword">or</span> agreed to <span class="keyword">in</span> writing, software</span><br><span class="line"> * distributed under the License <span class="keyword">is</span> distributed on an <span class="string">&quot;AS IS&quot;</span> BASIS,</span><br><span class="line"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express <span class="keyword">or</span> implied.</span><br><span class="line"> * See the License <span class="keyword">for</span> the specific language governing permissions <span class="keyword">and</span></span><br><span class="line"> * limitations under the License.</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line"><span class="comment">#include &quot;cookbookHelper.cuh&quot;</span></span><br><span class="line"></span><br><span class="line">using namespace nvinfer1;</span><br><span class="line"></span><br><span class="line">const std::string trtFile &#123;<span class="string">&quot;./model.plan&quot;</span>&#125;;</span><br><span class="line">static Logger     gLogger(ILogger::Severity::kERROR);</span><br><span class="line"></span><br><span class="line">void run()</span><br><span class="line">&#123;</span><br><span class="line">    ICudaEngine *engine = nullptr;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (access(trtFile.c_str(), F_OK) == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        std::ifstream engineFile(trtFile, std::ios::binary);</span><br><span class="line">        long <span class="built_in">int</span>      fsize = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        engineFile.seekg(<span class="number">0</span>, engineFile.end);</span><br><span class="line">        fsize = engineFile.tellg();</span><br><span class="line">        engineFile.seekg(<span class="number">0</span>, engineFile.beg);</span><br><span class="line">        std::vector&lt;char&gt; engineString(fsize);</span><br><span class="line">        engineFile.read(engineString.data(), fsize);</span><br><span class="line">        <span class="keyword">if</span> (engineString.size() == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Failed getting serialized engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Succeeded getting serialized engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">        IRuntime *runtime &#123;createInferRuntime(gLogger)&#125;;</span><br><span class="line">        engine = runtime-&gt;deserializeCudaEngine(engineString.data(), fsize);</span><br><span class="line">        <span class="keyword">if</span> (engine == nullptr)</span><br><span class="line">        &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Failed loading engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Succeeded loading engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        IBuilder             *builder = createInferBuilder(gLogger);</span><br><span class="line">        INetworkDefinition   *network = builder-&gt;createNetworkV2(1U &lt;&lt; <span class="built_in">int</span>(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH));</span><br><span class="line">        IOptimizationProfile *profile = builder-&gt;createOptimizationProfile();</span><br><span class="line">        IBuilderConfig       *config  = builder-&gt;createBuilderConfig();</span><br><span class="line">        config-&gt;setMemoryPoolLimit(MemoryPoolType::kWORKSPACE, <span class="number">1</span> &lt;&lt; <span class="number">30</span>);</span><br><span class="line"></span><br><span class="line">        ITensor *inputTensor = network-&gt;addInput(<span class="string">&quot;inputT0&quot;</span>, DataType::kFLOAT, Dims32 &#123;<span class="number">3</span>, &#123;-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>&#125;&#125;);</span><br><span class="line">        profile-&gt;setDimensions(inputTensor-&gt;getName(), OptProfileSelector::kMIN, Dims32 &#123;<span class="number">3</span>, &#123;<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>&#125;&#125;);</span><br><span class="line">        profile-&gt;setDimensions(inputTensor-&gt;getName(), OptProfileSelector::kOPT, Dims32 &#123;<span class="number">3</span>, &#123;<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;&#125;);</span><br><span class="line">        profile-&gt;setDimensions(inputTensor-&gt;getName(), OptProfileSelector::kMAX, Dims32 &#123;<span class="number">3</span>, &#123;<span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>&#125;&#125;);</span><br><span class="line">        config-&gt;addOptimizationProfile(profile);</span><br><span class="line"></span><br><span class="line">        IIdentityLayer *identityLayer = network-&gt;addIdentity(*inputTensor);</span><br><span class="line">        network-&gt;markOutput(*identityLayer-&gt;getOutput(<span class="number">0</span>));</span><br><span class="line">        IHostMemory *engineString = builder-&gt;buildSerializedNetwork(*network, *config);</span><br><span class="line">        <span class="keyword">if</span> (engineString == nullptr || engineString-&gt;size() == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Failed building serialized engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Succeeded building serialized engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">        IRuntime *runtime &#123;createInferRuntime(gLogger)&#125;;</span><br><span class="line">        engine = runtime-&gt;deserializeCudaEngine(engineString-&gt;data(), engineString-&gt;size());</span><br><span class="line">        <span class="keyword">if</span> (engine == nullptr)</span><br><span class="line">        &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Failed building engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Succeeded building engine!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">        std::ofstream engineFile(trtFile, std::ios::binary);</span><br><span class="line">        <span class="keyword">if</span> (!engineFile)</span><br><span class="line">        &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Failed opening file to write&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        engineFile.write(static_cast&lt;char *&gt;(engineString-&gt;data()), engineString-&gt;size());</span><br><span class="line">        <span class="keyword">if</span> (engineFile.fail())</span><br><span class="line">        &#123;</span><br><span class="line">            std::cout &lt;&lt; <span class="string">&quot;Failed saving .plan file!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;Succeeded saving .plan file!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    IExecutionContext *context = engine-&gt;createExecutionContext();</span><br><span class="line">    context-&gt;setBindingDimensions(<span class="number">0</span>, Dims32 &#123;<span class="number">3</span>, &#123;<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>&#125;&#125;);</span><br><span class="line">    std::cout &lt;&lt; std::string(<span class="string">&quot;Binding all? &quot;</span>) &lt;&lt; std::string(context-&gt;allInputDimensionsSpecified() ? <span class="string">&quot;Yes&quot;</span> : <span class="string">&quot;No&quot;</span>) &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">int</span> nBinding = engine-&gt;getNbBindings();</span><br><span class="line">    <span class="built_in">int</span> nInput   = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        nInput += <span class="built_in">int</span>(engine-&gt;bindingIsInput(i));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">int</span> nOutput = nBinding - nInput;</span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; std::string(<span class="string">&quot;Bind[&quot;</span>) &lt;&lt; i &lt;&lt; std::string(i &lt; nInput ? <span class="string">&quot;]:i[&quot;</span> : <span class="string">&quot;]:o[&quot;</span>) &lt;&lt; (i &lt; nInput ? i : i - nInput) &lt;&lt; std::string(<span class="string">&quot;]-&gt;&quot;</span>);</span><br><span class="line">        std::cout &lt;&lt; dataTypeToString(engine-&gt;getBindingDataType(i)) &lt;&lt; std::string(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        std::cout &lt;&lt; shapeToString(context-&gt;getBindingDimensions(i)) &lt;&lt; std::string(<span class="string">&quot; &quot;</span>);</span><br><span class="line">        std::cout &lt;&lt; engine-&gt;getBindingName(i) &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::vector&lt;<span class="built_in">int</span>&gt; vBindingSize(nBinding, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        Dims32 dim  = context-&gt;getBindingDimensions(i);</span><br><span class="line">        <span class="built_in">int</span>    size = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="built_in">int</span> j = <span class="number">0</span>; j &lt; dim.nbDims; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            size *= dim.d[j];</span><br><span class="line">        &#125;</span><br><span class="line">        vBindingSize[i] = size * dataTypeToSize(engine-&gt;getBindingDataType(i));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::vector&lt;void *&gt; vBufferH &#123;nBinding, nullptr&#125;;</span><br><span class="line">    std::vector&lt;void *&gt; vBufferD &#123;nBinding, nullptr&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        vBufferH[i] = (void *)new char[vBindingSize[i]];</span><br><span class="line">        CHECK(cudaMalloc(&amp;vBufferD[i], vBindingSize[i]));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">float</span> *pData = (<span class="built_in">float</span> *)vBufferH[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; vBindingSize[<span class="number">0</span>] / dataTypeToSize(engine-&gt;getBindingDataType(<span class="number">0</span>)); ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        pData[i] = <span class="built_in">float</span>(i);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">int</span>  inputSize = <span class="number">3</span> * <span class="number">4</span> * <span class="number">5</span>, outputSize = <span class="number">1</span>;</span><br><span class="line">    Dims outputShape = context-&gt;getBindingDimensions(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; outputShape.nbDims; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        outputSize *= outputShape.d[i];</span><br><span class="line">    &#125;</span><br><span class="line">    std::vector&lt;<span class="built_in">float</span>&gt;  inputH0(inputSize, <span class="number">1.0</span>f);</span><br><span class="line">    std::vector&lt;<span class="built_in">float</span>&gt;  outputH0(outputSize, <span class="number">0.0</span>f);</span><br><span class="line">    std::vector&lt;void *&gt; binding = &#123;nullptr, nullptr&#125;;</span><br><span class="line">    CHECK(cudaMalloc(&amp;binding[<span class="number">0</span>], sizeof(<span class="built_in">float</span>) * inputSize));</span><br><span class="line">    CHECK(cudaMalloc(&amp;binding[<span class="number">1</span>], sizeof(<span class="built_in">float</span>) * outputSize));</span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; inputSize; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        inputH0[i] = (<span class="built_in">float</span>)i;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 运行推理和使用 CUDA Graph 要用的流</span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    CHECK(cudaStreamCreate(&amp;stream));</span><br><span class="line"></span><br><span class="line">    // 捕获 CUDA Graph 之前要运行一次推理，https://docs.nvidia.com/deeplearning/tensorrt/api/c_api/classnvinfer1_1_1_i_execution_context.html<span class="comment">#a2f4429652736e8ef6e19f433400108c7</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; nInput; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        CHECK(cudaMemcpyAsync(vBufferD[i], vBufferH[i], vBindingSize[i], cudaMemcpyHostToDevice, stream));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    context-&gt;enqueueV2(vBufferD.data(), stream, nullptr);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = nInput; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        CHECK(cudaMemcpyAsync(vBufferH[i], vBufferD[i], vBindingSize[i], cudaMemcpyDeviceToHost, stream));</span><br><span class="line">    &#125;</span><br><span class="line">    cudaStreamSynchronize(stream); // 不用在 graph 内同步</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        printArrayInformation((<span class="built_in">float</span> *)vBufferH[i], context-&gt;getBindingDimensions(i), std::string(engine-&gt;getBindingName(i)), true, true);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 首次捕获 CUDA Graph 并运行推理</span><br><span class="line">    cudaGraph_t     graph;</span><br><span class="line">    cudaGraphExec_t graphExec = nullptr;</span><br><span class="line">    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);</span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; nInput; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        CHECK(cudaMemcpyAsync(vBufferD[i], vBufferH[i], vBindingSize[i], cudaMemcpyHostToDevice, stream));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    context-&gt;enqueueV2(vBufferD.data(), stream, nullptr);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = nInput; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        CHECK(cudaMemcpyAsync(vBufferH[i], vBufferD[i], vBindingSize[i], cudaMemcpyDeviceToHost, stream));</span><br><span class="line">    &#125;</span><br><span class="line">    //cudaStreamSynchronize(stream); // 不用在 graph 内同步</span><br><span class="line">    cudaStreamEndCapture(stream, &amp;graph);</span><br><span class="line">    cudaGraphInstantiate(&amp;graphExec, graph, nullptr, nullptr, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    cudaGraphLaunch(graphExec, stream);</span><br><span class="line">    cudaStreamSynchronize(stream);</span><br><span class="line"></span><br><span class="line">    // 输入尺寸改变后，也需要首先运行一次推理，然后重新捕获 CUDA Graph，最后再运行推理</span><br><span class="line">    context-&gt;setBindingDimensions(<span class="number">0</span>, Dims32 &#123;<span class="number">3</span>, &#123;<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;&#125;);</span><br><span class="line">    std::cout &lt;&lt; std::string(<span class="string">&quot;Binding all? &quot;</span>) &lt;&lt; std::string(context-&gt;allInputDimensionsSpecified() ? <span class="string">&quot;Yes&quot;</span> : <span class="string">&quot;No&quot;</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        Dims32 dim  = context-&gt;getBindingDimensions(i);</span><br><span class="line">        <span class="built_in">int</span>    size = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="built_in">int</span> j = <span class="number">0</span>; j &lt; dim.nbDims; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            size *= dim.d[j];</span><br><span class="line">        &#125;</span><br><span class="line">        vBindingSize[i] = size * dataTypeToSize(engine-&gt;getBindingDataType(i));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 这里偷懒，因为本次推理绑定的输入输出数据形状不大于上一次推理，所以这里不再重新准备所有 buffer</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; nInput; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        CHECK(cudaMemcpyAsync(vBufferD[i], vBufferH[i], vBindingSize[i], cudaMemcpyHostToDevice, stream));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    context-&gt;enqueueV2(vBufferD.data(), stream, nullptr);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = nInput; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        CHECK(cudaMemcpyAsync(vBufferH[i], vBufferD[i], vBindingSize[i], cudaMemcpyDeviceToHost, stream));</span><br><span class="line">    &#125;</span><br><span class="line">    cudaStreamSynchronize(stream); // 不用在 graph 内同步</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        printArrayInformation((<span class="built_in">float</span> *)vBufferH[i], context-&gt;getBindingDimensions(i), std::string(engine-&gt;getBindingName(i)), true, true);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 再次捕获 CUDA Graph 并运行推理</span><br><span class="line">    cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);</span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; nInput; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        CHECK(cudaMemcpyAsync(vBufferD[i], vBufferH[i], vBindingSize[i], cudaMemcpyHostToDevice, stream));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    context-&gt;enqueueV2(vBufferD.data(), stream, nullptr);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = nInput; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        CHECK(cudaMemcpyAsync(vBufferH[i], vBufferD[i], vBindingSize[i], cudaMemcpyDeviceToHost, stream));</span><br><span class="line">    &#125;</span><br><span class="line">    //cudaStreamSynchronize(stream); // 不用在 graph 内同步</span><br><span class="line">    cudaStreamEndCapture(stream, &amp;graph);</span><br><span class="line">    cudaGraphInstantiate(&amp;graphExec, graph, nullptr, nullptr, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    cudaGraphLaunch(graphExec, stream);</span><br><span class="line">    cudaStreamSynchronize(stream);</span><br><span class="line"></span><br><span class="line">    cudaStreamDestroy(stream);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; nBinding; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        delete[] vBufferH[i];</span><br><span class="line">        CHECK(cudaFree(vBufferD[i]));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">int</span> main()</span><br><span class="line">&#123;</span><br><span class="line">    CHECK(cudaSetDevice(<span class="number">0</span>));</span><br><span class="line">    run();</span><br><span class="line">    run();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105165151479.png" class="" title="image-20241105165151479">
<h2 id="2-5-Timing-Cache"><a href="#2-5-Timing-Cache" class="headerlink" title="2.5 Timing Cache"></a>2.5 Timing Cache</h2><p>engine 构建时间太长，使用timing cache节约多次构建时的时间</p>
<ul>
<li>范例代码：08-Advance\TimingCache</li>
<li>优点：<ul>
<li>优化单次引擎构建时间（模型内多个同参数的算子）</li>
<li>优化多次引擎构建时间（debug、参数更新后重新构建）</li>
<li>优化同环境下多个引擎构建时间（跨 builder 可用）</li>
<li>用于反复生成一模一样的引擎</li>
</ul>
</li>
<li>要点<ul>
<li>类似引擎序列化反序列化，将 Timing Cache 保存出来下次用</li>
<li>类似 .plan，不可跨平台和开发环境使用</li>
</ul>
</li>
</ul>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105165353944.png" class="" title="image-20241105165353944">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Copyright (c) 2021-2022, NVIDIA CORPORATION. All rights reserved.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment"># you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment"># You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> cuda <span class="keyword">import</span> cudart</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">trtFile = <span class="string">&quot;./model.plan&quot;</span></span><br><span class="line">timeCacheFile = <span class="string">&quot;./model.cache&quot;</span></span><br><span class="line">nB, nC, nH, nW = <span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span></span><br><span class="line">data = np.random.rand(nB, nC, nH, nW).astype(np.float32) * <span class="number">2</span> - <span class="number">1</span></span><br><span class="line">np.random.seed(<span class="number">31193</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">bUseTimeCache</span>):</span><br><span class="line">    logger = trt.Logger(trt.Logger.ERROR)</span><br><span class="line">    timeCache = <span class="string">b&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> bUseTimeCache <span class="keyword">and</span> os.path.isfile(timeCacheFile):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(timeCacheFile, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            timeCache = f.read()</span><br><span class="line">        <span class="keyword">if</span> timeCache == <span class="literal">None</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Failed getting serialized timing cache!&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Succeeded getting serialized timing cache!&quot;</span>)</span><br><span class="line"></span><br><span class="line">    builder = trt.Builder(logger)</span><br><span class="line">    network = builder.create_network(<span class="number">1</span> &lt;&lt; <span class="built_in">int</span>(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))</span><br><span class="line">    profile = builder.create_optimization_profile()</span><br><span class="line">    config = builder.create_builder_config()</span><br><span class="line">    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, <span class="number">7</span> &lt;&lt; <span class="number">30</span>)</span><br><span class="line">    <span class="keyword">if</span> bUseTimeCache:</span><br><span class="line">        cache = config.create_timing_cache(timeCache)</span><br><span class="line">        config.set_timing_cache(cache, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    inputTensor = network.add_input(<span class="string">&quot;inputT0&quot;</span>, trt.float32, [-<span class="number">1</span>, nC, nH, nW])</span><br><span class="line">    profile.set_shape(inputTensor.name, [nB, nC, nH, nW], [nB, nC, nH, nW], [nB * <span class="number">2</span>, nC, nH, nW])</span><br><span class="line">    config.add_optimization_profile(profile)</span><br><span class="line"></span><br><span class="line">    w = np.ascontiguousarray(np.random.rand(<span class="number">32</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>).astype(np.float32))</span><br><span class="line">    b = np.ascontiguousarray(np.random.rand(<span class="number">32</span>).astype(np.float32))</span><br><span class="line">    _0 = network.add_convolution_nd(inputTensor, <span class="number">32</span>, [<span class="number">5</span>, <span class="number">5</span>], w, b)</span><br><span class="line">    _0.padding_nd = [<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">    _1 = network.add_activation(_0.get_output(<span class="number">0</span>), trt.ActivationType.RELU)</span><br><span class="line">    _2 = network.add_pooling_nd(_1.get_output(<span class="number">0</span>), trt.PoolingType.MAX, [<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">    _2.stride_nd = [<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    w = np.ascontiguousarray(np.random.rand(<span class="number">64</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">5</span>).astype(np.float32))</span><br><span class="line">    b = np.ascontiguousarray(np.random.rand(<span class="number">64</span>).astype(np.float32))</span><br><span class="line">    _3 = network.add_convolution_nd(_2.get_output(<span class="number">0</span>), <span class="number">64</span>, [<span class="number">5</span>, <span class="number">5</span>], w, b)</span><br><span class="line">    _3.padding_nd = [<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">    _4 = network.add_activation(_3.get_output(<span class="number">0</span>), trt.ActivationType.RELU)</span><br><span class="line">    _5 = network.add_pooling_nd(_4.get_output(<span class="number">0</span>), trt.PoolingType.MAX, [<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">    _5.stride_nd = [<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    _6 = network.add_shuffle(_5.get_output(<span class="number">0</span>))</span><br><span class="line">    _6.first_transpose = (<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">    _6.reshape_dims = (-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">    w = np.ascontiguousarray(np.random.rand(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">1024</span>).astype(np.float32))</span><br><span class="line">    b = np.ascontiguousarray(np.random.rand(<span class="number">1</span>, <span class="number">1024</span>).astype(np.float32))</span><br><span class="line">    _7 = network.add_constant(w.shape, trt.Weights(w))</span><br><span class="line">    _8 = network.add_matrix_multiply(_6.get_output(<span class="number">0</span>), trt.MatrixOperation.NONE, _7.get_output(<span class="number">0</span>), trt.MatrixOperation.NONE)</span><br><span class="line">    _9 = network.add_constant(b.shape, trt.Weights(b))</span><br><span class="line">    _10 = network.add_elementwise(_8.get_output(<span class="number">0</span>), _9.get_output(<span class="number">0</span>), trt.ElementWiseOperation.SUM)</span><br><span class="line">    _11 = network.add_activation(_10.get_output(<span class="number">0</span>), trt.ActivationType.RELU)</span><br><span class="line"></span><br><span class="line">    w = np.ascontiguousarray(np.random.rand(<span class="number">1024</span>, <span class="number">10</span>).astype(np.float32))</span><br><span class="line">    b = np.ascontiguousarray(np.random.rand(<span class="number">1</span>, <span class="number">10</span>).astype(np.float32))</span><br><span class="line">    _12 = network.add_constant(w.shape, trt.Weights(w))</span><br><span class="line">    _13 = network.add_matrix_multiply(_11.get_output(<span class="number">0</span>), trt.MatrixOperation.NONE, _12.get_output(<span class="number">0</span>), trt.MatrixOperation.NONE)</span><br><span class="line">    _14 = network.add_constant(b.shape, trt.Weights(b))</span><br><span class="line">    _15 = network.add_elementwise(_13.get_output(<span class="number">0</span>), _14.get_output(<span class="number">0</span>), trt.ElementWiseOperation.SUM)</span><br><span class="line"></span><br><span class="line">    _16 = network.add_softmax(_15.get_output(<span class="number">0</span>))</span><br><span class="line">    _16.axes = <span class="number">1</span> &lt;&lt; <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    _17 = network.add_topk(_16.get_output(<span class="number">0</span>), trt.TopKOperation.MAX, <span class="number">1</span>, <span class="number">1</span> &lt;&lt; <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    network.mark_output(_17.get_output(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    t0 = time()</span><br><span class="line">    engineString = builder.build_serialized_network(network, config)</span><br><span class="line">    t1 = time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%s timing cache, %f ms&quot;</span> % (<span class="string">&quot;With&quot;</span> <span class="keyword">if</span> bUseTimeCache <span class="keyword">else</span> <span class="string">&quot;Without&quot;</span>, (t1 - t0) * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> bUseTimeCache <span class="keyword">and</span> <span class="keyword">not</span> os.path.isfile(timeCacheFile):</span><br><span class="line">        timeCache = config.get_timing_cache()</span><br><span class="line">        timeCacheString = timeCache.serialize()</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(timeCacheFile, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(timeCacheString)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Succeeded saving .cache file!&quot;</span>)</span><br><span class="line"></span><br><span class="line">    engine = trt.Runtime(logger).deserialize_cuda_engine(engineString)</span><br><span class="line"></span><br><span class="line">    context = engine.create_execution_context()</span><br><span class="line">    context.set_binding_shape(<span class="number">0</span>, [nB, nC, nH, nW])</span><br><span class="line">    nInput = np.<span class="built_in">sum</span>([engine.binding_is_input(i) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(engine.num_bindings)])</span><br><span class="line">    nOutput = engine.num_bindings - nInput</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Bind[%2d]:i[%2d]-&gt;&quot;</span> % (i, i), engine.get_binding_dtype(i), engine.get_binding_shape(i), context.get_binding_shape(i), engine.get_binding_name(i))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput, nInput + nOutput):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Bind[%2d]:o[%2d]-&gt;&quot;</span> % (i, i - nInput), engine.get_binding_dtype(i), engine.get_binding_shape(i), context.get_binding_shape(i), engine.get_binding_name(i))</span><br><span class="line"></span><br><span class="line">    bufferH = []</span><br><span class="line">    bufferH.append(np.ascontiguousarray(data))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput, nInput + nOutput):</span><br><span class="line">        bufferH.append(np.empty(context.get_binding_shape(i), dtype=trt.nptype(engine.get_binding_dtype(i))))</span><br><span class="line">    bufferD = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput + nOutput):</span><br><span class="line">        bufferD.append(cudart.cudaMalloc(bufferH[i].nbytes)[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput):</span><br><span class="line">        cudart.cudaMemcpy(bufferD[i], bufferH[i].ctypes.data, bufferH[i].nbytes, cudart.cudaMemcpyKind.cudaMemcpyHostToDevice)</span><br><span class="line"></span><br><span class="line">    context.execute_v2(bufferD)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput, nInput + nOutput):</span><br><span class="line">        cudart.cudaMemcpy(bufferH[i].ctypes.data, bufferD[i], bufferH[i].nbytes, cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#for i in range(nInput + nOutput):</span></span><br><span class="line">    <span class="comment">#    print(engine.get_binding_name(i))</span></span><br><span class="line">    <span class="comment">#    print(bufferH[i])</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> bufferD:</span><br><span class="line">        cudart.cudaFree(b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    os.system(<span class="string">&quot;rm -rf ./*.cache&quot;</span>)</span><br><span class="line">    np.set_printoptions(precision=<span class="number">3</span>, linewidth=<span class="number">200</span>, suppress=<span class="literal">True</span>)</span><br><span class="line">    cudart.cudaDeviceSynchronize()</span><br><span class="line"></span><br><span class="line">    run(<span class="number">0</span>)  <span class="comment"># 不使用 Timing Cache</span></span><br><span class="line">    run(<span class="number">0</span>)  <span class="comment"># 不使用 Timing Cache 再次构建</span></span><br><span class="line">    run(<span class="number">1</span>)  <span class="comment"># 创建并保存 Timing Cache</span></span><br><span class="line">    run(<span class="number">1</span>)  <span class="comment"># 读取并使用 Timing Cache</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105170228979.png" class="" title="image-20241105170228979">
<h2 id="2-6-Algorithm-Selector"><a href="#2-6-Algorithm-Selector" class="headerlink" title="2.6 Algorithm Selector"></a>2.6 Algorithm Selector</h2><p>TensorRT的kernel优选过程是一个黑箱，某些layer的选择可能造成较大的误差，我们需要筛选或排除某些算法。需要用到算法选择器。</p>
<ul>
<li>样例代码08-Advance\AlgorithmSelector</li>
<li>要点<ul>
<li>自己实现一个 MyAlgorithmSelector 类</li>
<li>关键是实现两个成员函数<ul>
<li>一个用来挑选特定层的算法</li>
<li>一个用来报告所有层的挑选结果</li>
</ul>
</li>
<li>构建网络时交给 BuilderConfig</li>
</ul>
</li>
<li>实际工作流程：<ul>
<li>先通过 polygraphy 等工具发现某层的个 Tactic 结果不理想</li>
<li>构造 Algorithm Selector 屏蔽掉盖层的该 tactic</li>
<li>构建引擎</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Copyright (c) 2021-2023, NVIDIA CORPORATION. All rights reserved.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment"># you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment"># You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#     http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment"># See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment"># limitations under the License.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorrt <span class="keyword">as</span> trt</span><br><span class="line"><span class="keyword">from</span> cuda <span class="keyword">import</span> cudart</span><br><span class="line"></span><br><span class="line">shape = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>]</span><br><span class="line">np.random.seed(<span class="number">31193</span>)</span><br><span class="line">data = np.random.rand(np.prod(shape)).astype(np.float32).reshape(shape) * <span class="number">2</span> - <span class="number">1</span></span><br><span class="line">np.set_printoptions(precision=<span class="number">3</span>, linewidth=<span class="number">200</span>, suppress=<span class="literal">True</span>)</span><br><span class="line">cudart.cudaDeviceSynchronize()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getSizeString</span>(<span class="params">xByte</span>):</span><br><span class="line">    <span class="keyword">if</span> xByte &lt; (<span class="number">1</span> &lt;&lt; <span class="number">10</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;%5.1f  B&quot;</span> % xByte</span><br><span class="line">    <span class="keyword">if</span> xByte &lt; (<span class="number">1</span> &lt;&lt; <span class="number">20</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;%5.1fKiB&quot;</span> % (xByte / (<span class="number">1</span> &lt;&lt; <span class="number">10</span>))</span><br><span class="line">    <span class="keyword">if</span> xByte &lt; (<span class="number">1</span> &lt;&lt; <span class="number">30</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;%5.1fMiB&quot;</span> % (xByte / (<span class="number">1</span> &lt;&lt; <span class="number">20</span>))</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;%5.1fGiB&quot;</span> % (xByte / (<span class="number">1</span> &lt;&lt; <span class="number">30</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyAlgorithmSelector</span>(trt.IAlgorithmSelector):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, iStrategy=<span class="number">0</span></span>):  <span class="comment"># initialize with a number of our customerized strategies to select algorithm</span></span><br><span class="line">        <span class="built_in">super</span>(MyAlgorithmSelector, self).__init__()</span><br><span class="line">        self.iStrategy = iStrategy</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">select_algorithms</span>(<span class="params">self, layerAlgorithmContext, layerAlgorithmList</span>):</span><br><span class="line">        <span class="comment"># we print the alternative algorithms of each layer here</span></span><br><span class="line">        nInput = layerAlgorithmContext.num_inputs</span><br><span class="line">        nOutput = layerAlgorithmContext.num_outputs</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Layer %s,in=%d,out=%d&quot;</span> % (layerAlgorithmContext.name, nInput, nOutput))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput + nOutput):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;    %s    %2d: shape=%s&quot;</span> % (<span class="string">&quot;Input &quot;</span> <span class="keyword">if</span> i &lt; nInput <span class="keyword">else</span> <span class="string">&quot;Output&quot;</span>, i <span class="keyword">if</span> i &lt; nInput <span class="keyword">else</span> i - nInput, layerAlgorithmContext.get_shape(i)))</span><br><span class="line">        <span class="keyword">for</span> i, algorithm <span class="keyword">in</span> <span class="built_in">enumerate</span>(layerAlgorithmList):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;    algorithm%3d:implementation[%10d], tactic[%20d], timing[%7.3fus], workspace[%s]&quot;</span> % ( \</span><br><span class="line">                i,</span><br><span class="line">                algorithm.algorithm_variant.implementation,</span><br><span class="line">                algorithm.algorithm_variant.tactic,</span><br><span class="line">                algorithm.timing_msec * <span class="number">1000</span>,</span><br><span class="line">                getSizeString(algorithm.workspace_size)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.iStrategy == <span class="number">0</span>:  <span class="comment"># choose the algorithm spending shortest time, the same as TensorRT</span></span><br><span class="line">            timeList = [algorithm.timing_msec <span class="keyword">for</span> algorithm <span class="keyword">in</span> layerAlgorithmList]</span><br><span class="line">            result = [np.argmin(timeList)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self.iStrategy == <span class="number">1</span>:  <span class="comment"># choose the algorithm spending longest time to get a TensorRT engine with worst performance, just for fun :)</span></span><br><span class="line">            timeList = [algorithm.timing_msec <span class="keyword">for</span> algorithm <span class="keyword">in</span> layerAlgorithmList]</span><br><span class="line">            result = [np.argmax(timeList)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self.iStrategy == <span class="number">2</span>:  <span class="comment"># choose the algorithm using smallest workspace</span></span><br><span class="line">            workspaceSizeList = [algorithm.workspace_size <span class="keyword">for</span> algorithm <span class="keyword">in</span> layerAlgorithmList]</span><br><span class="line">            result = [np.argmin(workspaceSizeList)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> self.iStrategy == <span class="number">3</span>:  <span class="comment"># choose one certain algorithm we have known</span></span><br><span class="line">            <span class="comment"># This strategy can be a workaround for building the exactly same engine for many times, but Timing Cache is more recommended to do so.</span></span><br><span class="line">            <span class="comment"># The reason is that function select_algorithms is called after the performance test of all algorithms of a layer is finished (you can find algorithm.timing_msec &gt; 0),</span></span><br><span class="line">            <span class="comment"># so it will not save the time of the test.</span></span><br><span class="line">            <span class="comment"># On the contrary, performance test of the algorithms will be skiped using Timing Cache (though performance test of Reformating can not be skiped),</span></span><br><span class="line">            <span class="comment"># so it surely saves a lot of time comparing with Algorithm Selector.</span></span><br><span class="line">            <span class="keyword">if</span> layerAlgorithmContext.name == <span class="string">&quot;(Unnamed Layer* 0) [Convolution] + (Unnamed Layer* 1) [Activation]&quot;</span>:</span><br><span class="line">                <span class="comment"># the number 2147483648 is from VERBOSE log, marking the certain algorithm</span></span><br><span class="line">                result = [index <span class="keyword">for</span> index, algorithm <span class="keyword">in</span> <span class="built_in">enumerate</span>(layerAlgorithmList) <span class="keyword">if</span> algorithm.algorithm_variant.implementation == <span class="number">2147483648</span>]</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># keep all algorithms for other layers</span></span><br><span class="line">                result = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(layerAlgorithmList)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># default behavior: keep all algorithms</span></span><br><span class="line">            result = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(layerAlgorithmList)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">report_algorithms</span>(<span class="params">self, modelAlgorithmContext, modelAlgorithmList</span>):  <span class="comment"># report the tactic of the whole network</span></span><br><span class="line">        <span class="comment"># some bug in report_algorithms to make the algorithm.timing_msec and algorithm.workspace_size are always 0?</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;[MyAlgorithmSelector::report_algorithms]&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(modelAlgorithmContext)):</span><br><span class="line">            context = modelAlgorithmContext[i]</span><br><span class="line">            algorithm = modelAlgorithmList[i]</span><br><span class="line">            nInput = context.num_inputs</span><br><span class="line">            nOutput = context.num_outputs</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Layer %s,in=%d,out=%d&quot;</span> % (context.name, nInput, nOutput))</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nInput + nOutput):</span><br><span class="line">                ioInfo = algorithm.get_algorithm_io_info(i)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;    %s    %2d: %s stride=%s, vectorized_dim=%d, components_per_element=%d, shape=%s&quot;</span> % ( \</span><br><span class="line">                    <span class="string">&quot;Input &quot;</span> <span class="keyword">if</span> i &lt; nInput <span class="keyword">else</span> <span class="string">&quot;Output&quot;</span>,</span><br><span class="line">                    i <span class="keyword">if</span> i &lt; nInput <span class="keyword">else</span>  i - nInput,</span><br><span class="line">                    ioInfo.dtype,</span><br><span class="line">                    ioInfo.strides,</span><br><span class="line">                    ioInfo.vectorized_dim,</span><br><span class="line">                    ioInfo.components_per_element,</span><br><span class="line">                    context.get_shape(i)))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;    algorithm   :implementation[%10d], tactic[%20d], timing[%7.3fus], workspace[%s]&quot;</span> % ( \</span><br><span class="line">                algorithm.algorithm_variant.implementation,</span><br><span class="line">                algorithm.algorithm_variant.tactic,</span><br><span class="line">                algorithm.timing_msec * <span class="number">1000</span>,</span><br><span class="line">                getSizeString(algorithm.workspace_size)))</span><br><span class="line"></span><br><span class="line">logger = trt.Logger(trt.Logger.INFO)</span><br><span class="line">builder = trt.Builder(logger)</span><br><span class="line">network = builder.create_network(<span class="number">1</span> &lt;&lt; <span class="built_in">int</span>(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))</span><br><span class="line">profile = builder.create_optimization_profile()</span><br><span class="line">config = builder.create_builder_config()</span><br><span class="line">config.algorithm_selector = MyAlgorithmSelector(<span class="number">1</span>)  <span class="comment"># assign Algorithm Selector to BuilderConfig, number here is the index of our customerized strategies to select algorithm</span></span><br><span class="line">config.set_flag(trt.BuilderFlag.FP16)  <span class="comment"># add FP16 to  get more alternative algorithms</span></span><br><span class="line"></span><br><span class="line">inputTensor = network.add_input(<span class="string">&quot;inputT0&quot;</span>, trt.float32, [-<span class="number">1</span>] + shape[<span class="number">1</span>:])</span><br><span class="line">profile.set_shape(inputTensor.name, [<span class="number">1</span>] + shape[<span class="number">1</span>:], [<span class="number">2</span>] + shape[<span class="number">1</span>:], [<span class="number">4</span>] + shape[<span class="number">1</span>:])</span><br><span class="line">config.add_optimization_profile(profile)</span><br><span class="line"></span><br><span class="line">w = np.ascontiguousarray(np.random.rand(<span class="number">32</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>).astype(np.float32))</span><br><span class="line">b = np.ascontiguousarray(np.random.rand(<span class="number">32</span>, <span class="number">1</span>, <span class="number">1</span>).astype(np.float32))</span><br><span class="line">_0 = network.add_convolution_nd(inputTensor, <span class="number">32</span>, [<span class="number">5</span>, <span class="number">5</span>], trt.Weights(w), trt.Weights(b))</span><br><span class="line">_0.padding_nd = [<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">_1 = network.add_activation(_0.get_output(<span class="number">0</span>), trt.ActivationType.RELU)</span><br><span class="line">_2 = network.add_pooling_nd(_1.get_output(<span class="number">0</span>), trt.PoolingType.MAX, [<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">_2.stride_nd = [<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">w = np.ascontiguousarray(np.random.rand(<span class="number">64</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">5</span>).astype(np.float32))</span><br><span class="line">b = np.ascontiguousarray(np.random.rand(<span class="number">64</span>, <span class="number">1</span>, <span class="number">1</span>).astype(np.float32))</span><br><span class="line">_3 = network.add_convolution_nd(_2.get_output(<span class="number">0</span>), <span class="number">64</span>, [<span class="number">5</span>, <span class="number">5</span>], trt.Weights(w), trt.Weights(b))</span><br><span class="line">_3.padding_nd = [<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">_4 = network.add_activation(_3.get_output(<span class="number">0</span>), trt.ActivationType.RELU)</span><br><span class="line">_5 = network.add_pooling_nd(_4.get_output(<span class="number">0</span>), trt.PoolingType.MAX, [<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">_5.stride_nd = [<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">_6 = network.add_shuffle(_5.get_output(<span class="number">0</span>))</span><br><span class="line">_6.reshape_dims = (-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">w = np.ascontiguousarray(np.random.rand(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">1024</span>).astype(np.float32))</span><br><span class="line">b = np.ascontiguousarray(np.random.rand(<span class="number">1</span>, <span class="number">1024</span>).astype(np.float32))</span><br><span class="line">_7 = network.add_constant(w.shape, trt.Weights(w))</span><br><span class="line">_8 = network.add_matrix_multiply(_6.get_output(<span class="number">0</span>), trt.MatrixOperation.NONE, _7.get_output(<span class="number">0</span>), trt.MatrixOperation.NONE)</span><br><span class="line">_9 = network.add_constant(b.shape, trt.Weights(b))</span><br><span class="line">_10 = network.add_elementwise(_8.get_output(<span class="number">0</span>), _9.get_output(<span class="number">0</span>), trt.ElementWiseOperation.SUM)</span><br><span class="line">_11 = network.add_activation(_10.get_output(<span class="number">0</span>), trt.ActivationType.RELU)</span><br><span class="line"></span><br><span class="line">w = np.ascontiguousarray(np.random.rand(<span class="number">1024</span>, <span class="number">10</span>).astype(np.float32))</span><br><span class="line">b = np.ascontiguousarray(np.random.rand(<span class="number">1</span>, <span class="number">10</span>).astype(np.float32))</span><br><span class="line">_12 = network.add_constant(w.shape, trt.Weights(w))</span><br><span class="line">_13 = network.add_matrix_multiply(_11.get_output(<span class="number">0</span>), trt.MatrixOperation.NONE, _12.get_output(<span class="number">0</span>), trt.MatrixOperation.NONE)</span><br><span class="line">_14 = network.add_constant(b.shape, trt.Weights(b))</span><br><span class="line">_15 = network.add_elementwise(_13.get_output(<span class="number">0</span>), _14.get_output(<span class="number">0</span>), trt.ElementWiseOperation.SUM)</span><br><span class="line"></span><br><span class="line">_16 = network.add_softmax(_15.get_output(<span class="number">0</span>))</span><br><span class="line">_16.axes = <span class="number">1</span> &lt;&lt; <span class="number">1</span></span><br><span class="line"></span><br><span class="line">_17 = network.add_topk(_16.get_output(<span class="number">0</span>), trt.TopKOperation.MAX, <span class="number">1</span>, <span class="number">1</span> &lt;&lt; <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">network.mark_output(_17.get_output(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">engineString = builder.build_serialized_network(network, config)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>代码中的<code>algorithm.algorithm_variant.implementation == 2147483648</code>这个数字就是手工挑选的算法，也可以使用不等号屏蔽算法。</p>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105171938919.png" class="" title="image-20241105171938919">
<h2 id="2-7-Refit"><a href="#2-7-Refit" class="headerlink" title="2.7 Refit"></a>2.7 Refit</h2><p>想更新模型的权重，但又不想重新构建 engine</p>
<p>适用与模型权重不停变化，的场景</p>
<ul>
<li><p>优点</p>
<ul>
<li><p>节约反复构建引擎的时间</p>
</li>
<li><p>强化学习必备</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>要点</p>
<ul>
<li>BuilderConfig 中设置相应 Flag</li>
<li>在构建好 engine 的基础上更新权重</li>
<li>更新某层权重后，邻近层可能也需要更新（尽管其值可能不变），如 Convolution 层中的 kernel 和 bias<br>[TensorRT] ERROR: 4: [refit.cpp::refitCudaEngine::1769] Error Code 4: Internal Error (missing 1 needed Weights. Call IRefitter::getMissing to get their layer names and roles or<br>IRefitter::getMissingWeights to get their weights names.)</li>
<li>注意权重的排布方式</li>
<li>Dynamic Shape 模式暂不支持（未来 TensorRT 版本将添加支持）<br>[TRT] [E] 4: [network.cpp::validate::2924] Error Code 4: Internal Error (Refittable networks with dynamic shapes is not supported.)</li>
</ul>
</li>
</ul>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105173548603.png" class="" title="image-20241105173548603">
<h2 id="2-8-Tactic-Source"><a href="#2-8-Tactic-Source" class="headerlink" title="2.8 Tactic Source"></a>2.8 Tactic Source</h2><ul>
<li>要点<ul>
<li>BuilderConfig 中设置相应 Flag</li>
<li>可以开启或关闭 cuBLAS、cuBLASLt、cuDNN（默认开启），tensorrt的tactic优选会从中找最优实现。</li>
</ul>
</li>
<li>优点<ul>
<li>节约部分内存、显存，减少构建时间</li>
<li>缺点</li>
<li>不能使用某些优化，可能导致性能下降</li>
<li>可能导致构建失败</li>
</ul>
</li>
<li>后续版本中，TensorRT 将彻底断开对外部 Library 依赖</li>
</ul>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105173713886.png" class="" title="image-20241105173713886">
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105173918880.png" class="" title="image-20241105173918880">
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/image-20241105174150765.png" class="" title="image-20241105174150765">
<h2 id="2-9-硬件兼容"><a href="#2-9-硬件兼容" class="headerlink" title="2.9 硬件兼容"></a>2.9 硬件兼容</h2><ul>
<li><p>TensorRT 8.6 新功能</p>
<ul>
<li>硬件兼容只支持 Ampere 及更新架构的 GPU（sm ≥ 8.0），版本兼容要求 TensorRT 版本 ≥ 8.6</li>
</ul>
</li>
<li><p>范例代码08-Advance/Hardware compatibility 和 08-Advance/ Version compatibility</p>
</li>
<li><p>硬件兼容</p>
<ul>
<li>config.hardware_compatibility_level = trt.HardwareCompatibilityLevel.AMPERE_PLUS</li>
<li>前向后向均可（A100 构建 A10 运行✔，A10 构建 A100 运行✔）</li>
<li>可能会有少许性能损失</li>
</ul>
</li>
<li><p>版本兼容</p>
<ul>
<li>config.set_flag(trt.BuilderFlag.VERSION_COMPATIBLE)</li>
<li>runtime.engine_host_code_allowed = True</li>
<li>前向后向均可（TRT8.6 构</li>
</ul>
</li>
</ul>
<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-graphs">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-graphs</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>奔跑的IC
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part4-TensorRT%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95/" title="Part4-TensorRT高级用法">http://example.com/TensorRT/TensorRT教程-基于8.6.1/Part4-TensorRT高级用法/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/git/" rel="tag"><i class="fa fa-tag"></i> git</a>
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C</a>
              <a href="/tags/GPU/" rel="tag"><i class="fa fa-tag"></i> GPU</a>
              <a href="/tags/Tensorrt/" rel="tag"><i class="fa fa-tag"></i> Tensorrt</a>
              <a href="/tags/DeepLearning/" rel="tag"><i class="fa fa-tag"></i> DeepLearning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part2-TensorRT%E5%BC%80%E5%8F%91%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/" rel="prev" title="Part2-TensorRT开发辅助工具">
                  <i class="fa fa-chevron-left"></i> Part2-TensorRT开发辅助工具
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-1-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%A6%82%E8%BF%B0/" rel="next" title="Part5-1-TensorRT性能优化概述">
                  Part5-1-TensorRT性能优化概述 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">奔跑的IC</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"zmurder","repo":"zmurder.github.io","client_id":"cf2343f27b6c29efe0bc","client_secret":"3268a1fa92706c7358d5421f88f76a0f7ada3188","admin_user":"zmurder","distraction_free_mode":true,"proxy":"https://strong-caramel-969805.netlify.app/github_access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"f517d9c84439b7448ce6f41b1877f000"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
