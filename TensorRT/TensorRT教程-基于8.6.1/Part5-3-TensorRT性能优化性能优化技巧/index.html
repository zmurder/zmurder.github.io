<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
  <meta name="msvalidate.01" content="7EC20DBC74B004C2782077570E15C280">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
    <meta name="description" content="背景本文档是记录学习Nvidia官方B站的视频，参考对应的PDF文件 TensorRTTraining-TRT8.6.1-Part5-V1.1.pdf 的记录。对应的官方代码[trt-samples-for-hackathon-cn]">
<meta property="og:type" content="article">
<meta property="og:title" content="Part5-3-TensorRT性能优化性能优化技巧">
<meta property="og:url" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/index.html">
<meta property="og:site_name" content="奔跑的IC">
<meta property="og:description" content="背景本文档是记录学习Nvidia官方B站的视频，参考对应的PDF文件 TensorRTTraining-TRT8.6.1-Part5-V1.1.pdf 的记录。对应的官方代码[trt-samples-for-hackathon-cn]">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/image-20241022204349489.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/image-20241107114654759.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/image-20241107145908996.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/tc-layout-kernels.svg">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/specialized-kernels.svg">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/image-20241107161202855.png">
<meta property="article:published_time" content="2024-12-01T10:13:45.536Z">
<meta property="article:modified_time" content="2025-05-15T13:38:24.834Z">
<meta property="article:author" content="奔跑的IC">
<meta property="article:tag" content="git">
<meta property="article:tag" content="C">
<meta property="article:tag" content="GPU">
<meta property="article:tag" content="Tensorrt">
<meta property="article:tag" content="DeepLearning">
<meta property="article:tag" content="Plugin">
<meta property="article:tag" content="卷积神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/image-20241022204349489.png">


<link rel="canonical" href="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/","path":"TensorRT/TensorRT教程-基于8.6.1/Part5-3-TensorRT性能优化性能优化技巧/","title":"Part5-3-TensorRT性能优化性能优化技巧"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Part5-3-TensorRT性能优化性能优化技巧 | 奔跑的IC</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">奔跑的IC</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-reorder fa-fw"></i>文章列表</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-text">背景</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7"><span class="nav-text">性能优化技巧</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#General-Tips"><span class="nav-text">General Tips</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Graph-Fusion-%E5%9B%BE%E8%9E%8D%E5%90%88"><span class="nav-text">Graph Fusion 图融合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimizing-Layer-Performance-Layer%E4%BC%98%E5%8C%96"><span class="nav-text">Optimizing Layer Performance Layer优化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimizing-for-Tensor-Cores"><span class="nav-text">Optimizing for Tensor Cores</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensor-Layouts-In-Memory-NCHW-vs-NHWC"><span class="nav-text">Tensor Layouts In Memory: NCHW vs NHWC</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Channels-In-And-Out"><span class="nav-text">Channels In And Out</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Low-Precision-%E4%BD%8E%E7%B2%BE%E5%BA%A6"><span class="nav-text">Low Precision 低精度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sparsity-%E7%A8%80%E7%96%8F"><span class="nav-text">Sparsity 稀疏</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Misc-%E5%85%B6%E4%BB%96"><span class="nav-text">Misc 其他</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-text">附录</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="奔跑的IC"
      src="/images/zyd.gif">
  <p class="site-author-name" itemprop="name">奔跑的IC</p>
  <div class="site-description" itemprop="description">死磕牛角的IT农民工</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">186</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zyd.gif">
      <meta itemprop="name" content="奔跑的IC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="奔跑的IC">
      <meta itemprop="description" content="死磕牛角的IT农民工">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Part5-3-TensorRT性能优化性能优化技巧 | 奔跑的IC">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Part5-3-TensorRT性能优化性能优化技巧
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-12-01 18:13:45" itemprop="dateCreated datePublished" datetime="2024-12-01T18:13:45+08:00">2024-12-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-05-15 21:38:24" itemprop="dateModified" datetime="2025-05-15T21:38:24+08:00">2025-05-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TensorRT/" itemprop="url" rel="index"><span itemprop="name">TensorRT</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8-6-1/" itemprop="url" rel="index"><span itemprop="name">TensorRT教程-基于8.6.1</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>本文档是记录学习Nvidia官方B站的<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1jj411Z7wG?spm_id_from=333.788.videopod.sections&amp;vd_source=cde2e7b9bca1a7048a13eaf0b48210b6">视频</a>，参考对应的PDF文件 TensorRTTraining-TRT8.6.1-Part5-V1.1.pdf 的记录。对应的官方代码<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/trt-samples-for-hackathon-cn">[trt-samples-for-hackathon-cn]</a></p>
<p>官方的视频教程基于TensorRT8.6.1版本。但是官方代码没有对应的tag。只有8.4、8.5和截至目前最新的8.10（master分支）。因此我这里参考的都是8.4分支的代码。</p>
<ul>
<li>part1 TensorRT简介</li>
<li>part2 开发辅助工具</li>
<li>part3 插件书写</li>
<li>part4 TensorRT高级用法</li>
<li>part5 常见优化策略</li>
</ul>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/image-20241022204349489.png" class="" title="image-20241022204349489">
<p>这一部分为第五部分，对应上面的常见优化策略</p>
<h1 id="性能优化技巧"><a href="#性能优化技巧" class="headerlink" title="性能优化技巧"></a>性能优化技巧</h1><h2 id="General-Tips"><a href="#General-Tips" class="headerlink" title="General Tips"></a>General Tips</h2><p>参考<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#optimize-performance">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#optimize-performance</a></p>
<ul>
<li>Batching<ul>
<li>增大batch size可以更好的发挥GPU算力，从而达到更高吞吐</li>
<li>如果吞吐随着batch size线性增加，一般说明GPU已打满</li>
</ul>
</li>
<li>Stream<ul>
<li>单个execution context内多流并行：TRT 8.6提供了IBuilderConfig::setMaxAuxStreams() API 来设置steam数量</li>
<li>多个execution context间多流并行：设置不同stream</li>
<li>尽量避免使用default stream，否则会引入额外的implicit sync</li>
</ul>
</li>
<li>CUDA Graph<ul>
<li>对于kernel launch bounded情况，可以通过CUDA graph来加速</li>
<li>不支持dynamic shape，需要capture多个CUDA Graph</li>
<li>设置nsys —cuda-graph-trace=node查看kernel</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#cuda-graphs">Overhead of Shape Change and Optimization Profile Switching</a><ul>
<li>当切换execution context或者改变graph input shape后，TRT可能会有额外的开销</li>
<li>可以尝试禁掉 kEDGE_MASK_CONVOLUTIONS tactic source，或者通过多个execution context切换使用来隐藏overhead</li>
</ul>
</li>
</ul>
<h2 id="Graph-Fusion-图融合"><a href="#Graph-Fusion-图融合" class="headerlink" title="Graph Fusion 图融合"></a>Graph Fusion 图融合</h2><ul>
<li>Graph fusion是非常有效的手段，<strong>TRT内部</strong>支持了丰富的<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#fusion-types">fusion pattern</a>，比如经典的conv+bias+relu</li>
<li>如果你发现了通用而且有明显收益的pattern，但是TRT没有fuse，建议发个bug</li>
<li>我们可以通过onnx-graphsurgeon和TRT plugin来<strong>手动</strong>实现graph fusion，比如经典的LayerNorm plugin</li>
<li>我们可以利用TRT外部的资源，比如<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass">Cutlass</a>，<a target="_blank" rel="noopener" href="https://github.com/HazyResearch/flash-attention">FlashAttn</a>， <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/xformers">xformer</a>，将kernel implementation封装到TRT plugin</li>
</ul>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/image-20241107114654759.png" class="" title="image-20241107114654759">
<h2 id="Optimizing-Layer-Performance-Layer优化"><a href="#Optimizing-Layer-Performance-Layer优化" class="headerlink" title="Optimizing Layer Performance Layer优化"></a>Optimizing Layer Performance Layer优化</h2><p>参考链接：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#optimize-layer">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#optimize-layer</a></p>
<ul>
<li><p>Reduce Layer</p>
<ul>
<li><p>尽量保证reduce axis在最后一维，这样能保证数据连续性，从而实现更好的性能。手动加一写Transpose来保证。</p>
</li>
<li><p>如果reduce axis不在最后一维，可以通过Shuffle (transpose) 调整</p>
</li>
<li><p>解释：</p>
<p><strong>Reduce Axis 在最后一维</strong>：如果 <code>reduce axis</code> 设置为最后一维 <code>W</code>，即 <code>dim=3</code>，那么在内存中，数据是连续的。例如，对于一个形状为 <code>[1, 3, 224, 224]</code> 的张量，<code>reduce axis=3</code> 的操作如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 4D 张量</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在最后一维上进行 reduce sum 操作</span></span><br><span class="line">output_tensor = torch.<span class="built_in">sum</span>(input_tensor, dim=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出张量的形状为 [1, 3, 224]</span></span><br><span class="line"><span class="built_in">print</span>(output_tensor.shape)</span><br></pre></td></tr></table></figure>
<p><strong>Reduce Axis 不在最后一维</strong>：如果 <code>reduce axis</code> 设置为其他维度，例如 <code>C</code>（即 <code>dim=1</code>），那么在内存中，数据不是连续的。例如，对于一个形状为 <code>[1, 3, 224, 224]</code> 的张量，<code>reduce axis=1</code> 的操作如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个 4D 张量</span></span><br><span class="line">input_tensor = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在第二维上进行 reduce sum 操作</span></span><br><span class="line">output_tensor = torch.<span class="built_in">sum</span>(input_tensor, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出张量的形状为 [1, 224, 224]</span></span><br><span class="line"><span class="built_in">print</span>(output_tensor.shape)</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>Shuffle Layer</p>
<ul>
<li>如果input和output tensor有相同的memory layout，就不会有实际的kernel</li>
</ul>
</li>
<li><p>MatMul Layer</p>
<ul>
<li><p>如果使用FP16，而且K不是8的倍数，为了使用tensor core ，TRT可能会引入reformat kernel，可以通过手动padding到8的倍数来规避</p>
</li>
<li><p>如果input是4维，可以尝试通过reshape转化为3维或者2维，可能有更好的性能。不会有实际的keenle</p>
</li>
<li><p>解释：</p>
<p>在矩阵乘法（Matrix Multiplication）中，<code>M</code>、<code>N</code> 和 <code>K</code> 是三个重要的参数，它们分别表示输入矩阵的维度。具体来说，矩阵乘法的操作可以表示为：</p>
<p>C=A×B</p>
<p>其中：</p>
<p>A是一个 M×K的矩阵。</p>
<p>B是一个 K×N的矩阵。</p>
<p>C是一个 M×N的矩阵。</p>
</li>
</ul>
</li>
</ul>
<h2 id="Optimizing-for-Tensor-Cores"><a href="#Optimizing-for-Tensor-Cores" class="headerlink" title="Optimizing for Tensor Cores"></a>Optimizing for Tensor Cores</h2><p>参考链接：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#optimize-tensor-cores">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#optimize-tensor-cores</a></p>
<ul>
<li><p>使用Tensor Core才能发挥GPU的最大算力，TF32，FP16，INT8，FP8</p>
</li>
<li><p>TRT的MatrixMultiply, FullyConnected, Convolution, and Deconvolution等计算密集的算子会使用Tensor Core</p>
</li>
<li><p>Tensor Core对data alignment有要求：</p>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/image-20241107145908996.png" class="" title="image-20241107145908996">
<p>官网原文</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Tensor Core layers tend to achieve better performance if the I/O tensor dimensions are aligned to a certain minimum granularity: </span><br><span class="line">    The alignment requirement is on the I/O channel dimension in the Convolution and Deconvolution layers.</span><br><span class="line">    In MatrixMultiply and FullyConnected layers, the alignment requirement is on matrix dimensions K and N in a MatrixMultiply that is M x K times K x N.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>例如上面讲到的Matrix Multiplication中，<code>M</code>、<code>N</code> 和 <code>K</code> 。如果是TF32时K和N最好是4的倍数。</p>
<p>对于卷积操作，假设输入特征图的形状为 (N,C,H,W)，卷积核的形状为 (O,C,kH,kW)，输出特征图的形状为 (N,O,oH,oW)。TF32：输入特征图的通道数 C必须是 4 的倍数。这里我不知道对于NHWC和NCHW都是对C的要求。</p>
</li>
<li><p>当tensor不满足alignment条件时，TRT会隐式地做padding，比如插入reformat layer</p>
</li>
<li><p>可以设置nsys —gpu-metrics-device all来查看Tensor Core使用情况</p>
</li>
</ul>
<h2 id="Tensor-Layouts-In-Memory-NCHW-vs-NHWC"><a href="#Tensor-Layouts-In-Memory-NCHW-vs-NHWC" class="headerlink" title="Tensor Layouts In Memory: NCHW vs NHWC"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#tensor-layout">Tensor Layouts In Memory: NCHW vs NHWC</a></h2><p>参考<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#tensor-layout">Tensor Layouts In Memory: NCHW vs NHWC</a></p>
<p>卷积通常对四维张量进行作： a batch composed of N “images” of C channels of H x W feature maps. </p>
<p>深度学习框架通常在内存中使用 NCHW 和 NHWC 布局（首字母缩略词列出了内存中从最慢到最快的维度）。布局选择会影响性能，因为为 <strong>Tensor Core 实现的卷积需要 NHWC 布局，并且在 NHWC 中布局输入张量时速度最快</strong>。</p>
<p>NCHW 布局仍然可以由 Tensor Core 进行作，但由于自动转置作而包含一些开销，如图 2 所示。当输入和输出张量较大或所需的计算量较低时（例如，当滤波器大小较小时），转置开销往往更显著。为了最大限度地提高性能，我们建议使用 NHWC 张量布局。</p>
<p><em>Figure 2. Kernels that do not  require a transpose (NHWC) perform better than kernels that require one  or more (NCHW). NVIDIA A100-SXM4-80GB, CUDA 11.2, cuDNN 8.1.</em></p>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/tc-layout-kernels.svg" class="" title="tc-layout-kernels">
<h2 id="Channels-In-And-Out"><a href="#Channels-In-And-Out" class="headerlink" title="Channels In And Out"></a><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#channels">Channels In And Out</a></h2><p>参考<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html#channels">Channels In And Out</a></p>
<p>启用 Tensor Core 的要求取决于所使用的 cuDNN 版本。使用 cuDNN v7.6.3  及更高版本，卷积维度将在必要时自动填充以利用 Tensor Core。早期版本的 cuDNN 更严格：将 Tensor Core 与 NHWC  打包数据一起使用需要将 C 和 K 与 TF32 对齐为 4 的倍数，将 FP16 对齐为 8 的倍数，将 16 与 INT8 对齐为 16  的倍数。对于 NCHW 打包的 FP16 数据，通道将自动填充为 8 的倍数，以便启用 Tensor Core。但是，将 NCHW 数据与启用  Tensor Core 的内核一起使用会涉及一些额外的转置成本，这在内存中的 Tensor 布局：NCHW 与 NHWC 中进行了讨论。</p>
<p>另一方面，对于这些早期版本的 cuDNN，自动填充不会对 NHWC 打包的数据启动，因此会选择效率较低的回退内核，该内核不使用 Tensor  Core 。鉴于 C 和 K 能被 8 整除，使用 NHWC 数据的卷积确实比使用 NCHW 数据的卷积表现更好。换句话说，如果某个图层已经与  NCHW 数据一起使用，则会发生自动填充;但是，如果正在使用 NHWC 数据，则选择或填充 C 和 K 为 8 的倍数可以提高性能。</p>
<p>对于 cuDNN v7.6.3 及更高版本，无论数据格式如何，填充都是自动的。填充会增加一些时间，尽管与启用 Tensor Core  的性能增益相比，这种成本通常可以忽略不计。值得注意的是，对于 FP16 选择 C 和 K 是 8  的倍数，或者对于其他数据类型选择等效值，效率最高：对于这些情况，不需要填充。</p>
<p>在某些情况下，通道数很小且不可协商。对于网络中的第一层，通常具有非常小的 C 值（灰度和 RGB 或 YCrCb 图像分别为 1 或  3）。特殊情况的卷积实现可以满足这一需求，特别是对于 C = 4 和 stride 为 2（图 8）。此处显示的数据是使用 cuDNN 8.1  收集的，因此填充是自动的。与 7.6.3 之前的版本相比，从 C = 3 到 C = 4 的性能改进没有那么剧烈，但选择 C = 4  仍然更快，因为不会发生填充。</p>
<p> 图8.C = 4 的专用内核加快了卷积神经网络中的常见第一层（使用 NHWC 数据）的速度。<strong>选择 C = 4 或 8 的倍数可提供最佳性能。</strong>NVIDIA A100-SXM4-80GB、CUDA 11.2、cuDNN 8.1。</p>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/specialized-kernels.svg" class="" title="specialized-kernels">
<h2 id="Low-Precision-低精度"><a href="#Low-Precision-低精度" class="headerlink" title="Low Precision 低精度"></a>Low Precision 低精度</h2><p>参考：<a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31876/">https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31876/</a></p>
<ul>
<li>使用低精度可以降低memory开销，其中FP16比较成熟，INT8需要PTQ或者QAT来保证精度，TRT接下来的版本会支持FP8</li>
<li>INT8 QAT<ul>
<li>量化感知训练（QAT）相比于训练后量化（PTQ）有更高的精度</li>
<li>训练阶段，TensorRT提供了基于PyTorch的INT8量化工具包，帮助生成QAT模型，并维持原有精</li>
<li>模型转换阶段， TensorRT ONNX parser新增了对ONNX QuantizeLinear和DequantizeLinear算子的支持推理阶段，TensorRT 8.0新增了Quantize</li>
</ul>
</li>
</ul>
<h2 id="Sparsity-稀疏"><a href="#Sparsity-稀疏" class="headerlink" title="Sparsity 稀疏"></a>Sparsity 稀疏</h2><p>参考</p>
<p><a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31876/">https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31876/</a><br><a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31552/">https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31552/</a><br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/">https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/</a></p>
<ul>
<li>TRT8.0支持了结构化稀疏，它是安培架构引入的新特性</li>
<li>利用结构化稀疏(structured sparsity)加速推理<ul>
<li>通过2:4细粒度结构化稀疏减少一半参数，并利用SparseTensor Core进行加速，在Bert模型上能实现1.3倍的加速（下图针对w，4中个选最大的2个保留，剩下的作为一个mask，减少一半的参数量和一个mask）</li>
<li>训练阶段，通过稀疏化工具ASP (Automatic SParsity)修剪参数，然后微调模型以维持原有精度</li>
<li>推理阶段，通过设置build_config的SPARSE_WEIGHTS flag来挑选sparse kernel</li>
</ul>
</li>
<li>可以通过trtexec —sparsity=enable/force来快速测试加速效果</li>
</ul>
<img src="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/image-20241107161202855.png" class="" title="image-20241107161202855">
<h2 id="Misc-其他"><a href="#Misc-其他" class="headerlink" title="Misc 其他"></a>Misc 其他</h2><p>参考：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#performance">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#performance</a></p>
<ul>
<li>Optimizing Plugins 手动写plugin<ul>
<li>The performance of plugins depends on the CUDA code performing the plugin operation.</li>
<li>Support as many formats as possible in the plugin. This removes the need for internal reformat operations during the execution of the network。自己写的plugin的不同的精度支持</li>
</ul>
</li>
<li>Optimizing Builder Performance 优化build engine的时间<ul>
<li>Timing cache</li>
<li>Tactic Selection Heuristic</li>
</ul>
</li>
<li>Builder Optimization Level trt8.6引入的，设置不同的等级，例如更快设置为0 更好设置为5，正常是3<ul>
<li>Set the optimization</li>
</ul>
</li>
</ul>
<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><ul>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#optimize-performance">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#optimize-performance</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#optimize-layer">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#optimize-layer</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#optimize-tensor-cores">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#optimize-tensor-cores</a></li>
<li><a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31876/">https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31876/</a></li>
<li><a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31876/">https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31876/</a><br><a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31552/">https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31552/</a><br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/">https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#performance">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#performance</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#fusion-types">fusion pattern</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/cutlass">Cutlass</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/HazyResearch/flash-attention">FlashAttn</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/xformers">xformer</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>奔跑的IC
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://example.com/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-3-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%8A%80%E5%B7%A7/" title="Part5-3-TensorRT性能优化性能优化技巧">http://example.com/TensorRT/TensorRT教程-基于8.6.1/Part5-3-TensorRT性能优化性能优化技巧/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/git/" rel="tag"><i class="fa fa-tag"></i> git</a>
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C</a>
              <a href="/tags/GPU/" rel="tag"><i class="fa fa-tag"></i> GPU</a>
              <a href="/tags/Tensorrt/" rel="tag"><i class="fa fa-tag"></i> Tensorrt</a>
              <a href="/tags/DeepLearning/" rel="tag"><i class="fa fa-tag"></i> DeepLearning</a>
              <a href="/tags/Plugin/" rel="tag"><i class="fa fa-tag"></i> Plugin</a>
              <a href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="fa fa-tag"></i> 卷积神经网络</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT%E6%95%99%E7%A8%8B-%E5%9F%BA%E4%BA%8E8.6.1/Part5-2-TensorRT%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7/" rel="prev" title="Part5-2-TensorRT性能优化性能分析工具">
                  <i class="fa fa-chevron-left"></i> Part5-2-TensorRT性能优化性能分析工具
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%B8%80)/" rel="next" title="TensorRT量化实战课YOLOv7量化：YOLOv7-PTQ量化(一)">
                  TensorRT量化实战课YOLOv7量化：YOLOv7-PTQ量化(一) <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">奔跑的IC</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"zmurder","repo":"zmurder.github.io","client_id":"cf2343f27b6c29efe0bc","client_secret":"3268a1fa92706c7358d5421f88f76a0f7ada3188","admin_user":"zmurder","distraction_free_mode":true,"proxy":"https://strong-caramel-969805.netlify.app/github_access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"ce247c9084b425178e6330573e5ad540"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
