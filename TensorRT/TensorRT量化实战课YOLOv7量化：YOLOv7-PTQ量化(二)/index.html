<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
  <meta name="msvalidate.01" content="7EC20DBC74B004C2782077570E15C280">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"æœç´¢...","empty":"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœï¼š${query}","hits_time":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰","hits":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœ"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
    <meta name="description" content="ç›®å½•  æ³¨æ„äº‹é¡¹ ä¸€ã€2023&#x2F;11&#x2F;19æ›´æ–° äºŒã€2023&#x2F;12&#x2F;27æ›´æ–° å‰è¨€ 1. YOLOv7-PTQé‡åŒ–æµç¨‹ 2. æ¨¡å‹æ ‡å®š 3. æ•æ„Ÿå±‚åˆ†æ 4. PTQé‡åŒ– æ€»ç»“">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorRTé‡åŒ–å®æˆ˜è¯¾YOLOv7é‡åŒ–ï¼šYOLOv7-PTQé‡åŒ–(äºŒ)">
<meta property="og:url" content="http://example.com/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/index.html">
<meta property="og:site_name" content="å¥”è·‘çš„IC">
<meta property="og:description" content="ç›®å½•  æ³¨æ„äº‹é¡¹ ä¸€ã€2023&#x2F;11&#x2F;19æ›´æ–° äºŒã€2023&#x2F;12&#x2F;27æ›´æ–° å‰è¨€ 1. YOLOv7-PTQé‡åŒ–æµç¨‹ 2. æ¨¡å‹æ ‡å®š 3. æ•æ„Ÿå±‚åˆ†æ 4. PTQé‡åŒ– æ€»ç»“">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/b03a269e88c866011bdced6d9b002fee.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/0175313e5448829028e4b202628242e5.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/0a1db5742c80d3fbfc571ceaa95b816b.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/9fded8c52fb14152c5445db3ce933bb2.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/23a9074455dd319a5c73c8bc4ce233d6.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/9b42b85a0aa80b64c6f0071e7277a486.png">
<meta property="og:image" content="http://example.com/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/ba68500dd729d1e17a0d5f6f5e5cd043.png">
<meta property="article:published_time" content="2024-12-01T10:13:45.551Z">
<meta property="article:modified_time" content="2024-12-01T10:13:45.551Z">
<meta property="article:author" content="å¥”è·‘çš„IC">
<meta property="article:tag" content="C">
<meta property="article:tag" content="Tensorrt">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/b03a269e88c866011bdced6d9b002fee.png">


<link rel="canonical" href="http://example.com/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://example.com/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/","path":"TensorRT/TensorRTé‡åŒ–å®æˆ˜è¯¾YOLOv7é‡åŒ–ï¼šYOLOv7-PTQé‡åŒ–(äºŒ)/","title":"TensorRTé‡åŒ–å®æˆ˜è¯¾YOLOv7é‡åŒ–ï¼šYOLOv7-PTQé‡åŒ–(äºŒ)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>TensorRTé‡åŒ–å®æˆ˜è¯¾YOLOv7é‡åŒ–ï¼šYOLOv7-PTQé‡åŒ–(äºŒ) | å¥”è·‘çš„IC</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ " role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">å¥”è·‘çš„IC</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="æœç´¢" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>åˆ†ç±»</a></li><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-reorder fa-fw"></i>æ–‡ç« åˆ—è¡¨</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>å…³äº</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>æ ‡ç­¾</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>æœç´¢
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="æœç´¢..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9B%AE%E5%BD%95"><span class="nav-text">ç›®å½•</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="nav-text">æ³¨æ„äº‹é¡¹</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E3%80%812023-11-19%E6%9B%B4%E6%96%B0"><span class="nav-text">ä¸€ã€2023&#x2F;11&#x2F;19æ›´æ–°</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E3%80%812023-12-27%E6%9B%B4%E6%96%B0"><span class="nav-text">äºŒã€2023&#x2F;12&#x2F;27æ›´æ–°</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-text">å‰è¨€</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-YOLOv7-PTQ%E9%87%8F%E5%8C%96%E6%B5%81%E7%A8%8B"><span class="nav-text">1. YOLOv7-PTQé‡åŒ–æµç¨‹</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="nav-text">1. å‡†å¤‡å·¥ä½œ</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%8F%92%E5%85%A5-QDQ-%E8%8A%82%E7%82%B9"><span class="nav-text">2. æ’å…¥ QDQ èŠ‚ç‚¹</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E6%A0%87%E5%AE%9A"><span class="nav-text">3. æ ‡å®š</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E6%95%8F%E6%84%9F%E5%B1%82%E5%88%86%E6%9E%90"><span class="nav-text">4. æ•æ„Ÿå±‚åˆ†æ</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E5%AF%BC%E5%87%BA-PTQ-%E6%A8%A1%E5%9E%8B"><span class="nav-text">5. å¯¼å‡º PTQ æ¨¡å‹</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94"><span class="nav-text">6. æ€§èƒ½å¯¹æ¯”</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E6%A8%A1%E5%9E%8B%E6%A0%87%E5%AE%9A"><span class="nav-text">2. æ¨¡å‹æ ‡å®š</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-calibrate-model"><span class="nav-text">1. calibrate_model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-collect-stats"><span class="nav-text">2. collect_stats</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-compute-amax"><span class="nav-text">3. compute_amax</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E6%95%8F%E6%84%9F%E5%B1%82%E5%88%86%E6%9E%90"><span class="nav-text">3. æ•æ„Ÿå±‚åˆ†æ</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-PTQ%E9%87%8F%E5%8C%96"><span class="nav-text">4. PTQé‡åŒ–</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-text">æ€»ç»“</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="å¥”è·‘çš„IC"
      src="/images/zyd.gif">
  <p class="site-author-name" itemprop="name">å¥”è·‘çš„IC</p>
  <div class="site-description" itemprop="description">æ­»ç£•ç‰›è§’çš„ITå†œæ°‘å·¥</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">166</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zyd.gif">
      <meta itemprop="name" content="å¥”è·‘çš„IC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="å¥”è·‘çš„IC">
      <meta itemprop="description" content="æ­»ç£•ç‰›è§’çš„ITå†œæ°‘å·¥">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="TensorRTé‡åŒ–å®æˆ˜è¯¾YOLOv7é‡åŒ–ï¼šYOLOv7-PTQé‡åŒ–(äºŒ) | å¥”è·‘çš„IC">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          TensorRTé‡åŒ–å®æˆ˜è¯¾YOLOv7é‡åŒ–ï¼šYOLOv7-PTQé‡åŒ–(äºŒ)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">å‘è¡¨äº</span>

      <time title="åˆ›å»ºæ—¶é—´ï¼š2024-12-01 18:13:45" itemprop="dateCreated datePublished" datetime="2024-12-01T18:13:45+08:00">2024-12-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">åˆ†ç±»äº</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TensorRT/" itemprop="url" rel="index"><span itemprop="name">TensorRT</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="é˜…è¯»æ¬¡æ•°" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">é˜…è¯»æ¬¡æ•°ï¼š</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="ç›®å½•"><a href="#ç›®å½•" class="headerlink" title="ç›®å½•"></a>ç›®å½•</h1><ul>
<li><ul>
<li><a href="#_2">æ³¨æ„äº‹é¡¹</a></li>
<li><a href="#20231119_4">ä¸€ã€2023/11/19æ›´æ–°</a></li>
<li><a href="#20231227_8">äºŒã€2023/12/27æ›´æ–°</a></li>
<li><a href="#_11">å‰è¨€</a></li>
<li><a href="#1_YOLOv7PTQ_24">1. YOLOv7-PTQé‡åŒ–æµç¨‹</a></li>
<li><a href="#2__74">2. æ¨¡å‹æ ‡å®š</a></li>
<li><a href="#3__505">3. æ•æ„Ÿå±‚åˆ†æ</a></li>
<li><a href="#4_PTQ_1078">4. PTQé‡åŒ–</a></li>
<li><a href="#_1596">æ€»ç»“</a></li>
</ul>
</li>
</ul>
<h1 id="æ³¨æ„äº‹é¡¹"><a href="#æ³¨æ„äº‹é¡¹" class="headerlink" title="æ³¨æ„äº‹é¡¹"></a>æ³¨æ„äº‹é¡¹</h1><h3 id="ä¸€ã€2023-11-19æ›´æ–°"><a href="#ä¸€ã€2023-11-19æ›´æ–°" class="headerlink" title="ä¸€ã€2023/11/19æ›´æ–°"></a>ä¸€ã€2023/11/19æ›´æ–°</h3><p><strong>æ–°å¢æ•æ„Ÿå±‚åˆ†æå’Œ PTQ é‡åŒ–ä»£ç å·¥ç¨‹åŒ–</strong></p>
<h3 id="äºŒã€2023-12-27æ›´æ–°"><a href="#äºŒã€2023-12-27æ›´æ–°" class="headerlink" title="äºŒã€2023/12/27æ›´æ–°"></a>äºŒã€2023/12/27æ›´æ–°</h3><p><strong>å’Œ <code>è´è’‚å°ç†Š</code> çœ‹å®˜äº¤æµçš„è¿‡ç¨‹ä¸­å‘ç°æ¨¡å‹æ ‡å®šå°èŠ‚ä¸­çš„ä¸€äº›æè¿°å­˜åœ¨é—®é¢˜ï¼Œä¿®æ”¹æ¨¡å‹æ ‡å®šå°èŠ‚ä¸€äº›æè¿°è¯è¯­ï¼Œé‡æ–°æ¢³ç†ä¸‹ PTQ é‡åŒ–å’Œ QAT é‡åŒ–çš„åŒºåˆ«ï¼Œå…·ä½“å¯å‚è€ƒç¬¬ 2 å°èŠ‚ä¿®æ”¹çš„å†…å®¹</strong></p>
<h1 id="å‰è¨€"><a href="#å‰è¨€" class="headerlink" title="å‰è¨€"></a>å‰è¨€</h1><blockquote>
<p>æ‰‹å†™ AI æ¨å‡ºçš„å…¨æ–° TensorRT æ¨¡å‹é‡åŒ–å®æˆ˜è¯¾ç¨‹ï¼Œ<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1NN411b7HZ/?spm_id_from=333.999.0.0">é“¾æ¥</a>ã€‚è®°å½•ä¸‹ä¸ªäººå­¦ä¹ ç¬”è®°ï¼Œä»…ä¾›è‡ªå·±å‚è€ƒã€‚</p>
<p>è¯¥å®æˆ˜è¯¾ç¨‹ä¸»è¦åŸºäºæ‰‹å†™ AI çš„ Latte è€å¸ˆæ‰€å‡ºçš„ <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV18L41197Uz/">TensorRTä¸‹çš„æ¨¡å‹é‡åŒ–</a>ï¼Œåœ¨å…¶è¯¾ç¨‹çš„åŸºç¡€ä¸Šï¼Œæ‰€æ•´ç†å‡ºçš„ä¸€äº›å®æˆ˜åº”ç”¨ã€‚</p>
<p>æœ¬æ¬¡è¯¾ç¨‹ä¸º YOLOv7 é‡åŒ–å®æˆ˜ç¬¬ä¸‰è¯¾ï¼Œä¸»è¦ä»‹ç» YOLOv7-PTQ é‡åŒ–</p>
<p>è¯¾ç¨‹å¤§çº²å¯çœ‹ä¸‹é¢çš„æ€ç»´å¯¼å›¾</p>
</blockquote>
<img src="/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/b03a269e88c866011bdced6d9b002fee.png" class="" title="b03a269e88c866011bdced6d9b002fee">
<h1 id="1-YOLOv7-PTQé‡åŒ–æµç¨‹"><a href="#1-YOLOv7-PTQé‡åŒ–æµç¨‹" class="headerlink" title="1. YOLOv7-PTQé‡åŒ–æµç¨‹"></a>1. YOLOv7-PTQé‡åŒ–æµç¨‹</h1><blockquote>
<p>åœ¨ä¸ŠèŠ‚è¯¾ç¨‹ä¸­æˆ‘ä»¬ä»‹ç»äº† YOLOv7-PTQ é‡åŒ–ä¸­ QDQ èŠ‚ç‚¹çš„æ’å…¥ï¼Œè¿™èŠ‚è¯¾æˆ‘ä»¬å°†ä¼šå®Œæˆ PTQ æ¨¡å‹çš„é‡åŒ–å’Œå¯¼å‡ºã€‚</p>
<p>ä»ä¸Šé¢çš„æ€ç»´å¯¼å›¾æˆ‘ä»¬å¯ä»¥çœ‹åˆ° YOLOv7-PTQ é‡åŒ–çš„æ­¥éª¤ï¼Œæˆ‘ä»¬ä»£ç çš„è®²è§£å’Œç¼–å†™éƒ½æ˜¯æŒ‰ç…§è¿™ä¸ªæµç¨‹æ¥çš„ã€‚</p>
</blockquote>
<p>åœ¨ç¼–å†™ä»£ç å¼€å§‹ä¹‹å‰æˆ‘ä»¬è¿˜æ˜¯å†æ¥æ¢³ç†ä¸‹æ•´ä¸ª YOLOv7-PTQ é‡åŒ–çš„è¿‡ç¨‹ï¼Œå¦‚ä¸‹ï¼š</p>
<h2 id="1-å‡†å¤‡å·¥ä½œ"><a href="#1-å‡†å¤‡å·¥ä½œ" class="headerlink" title="1. å‡†å¤‡å·¥ä½œ"></a><strong>1.</strong> <strong>å‡†å¤‡å·¥ä½œ</strong></h2><p>é¦–å…ˆæ˜¯æˆ‘ä»¬çš„å‡†å¤‡å·¥ä½œï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½ YOLOv7 å®˜æ–¹ä»£ç å’Œé¢„è®­ç»ƒæ¨¡å‹ä»¥åŠ COCO æ•°æ®é›†ï¼Œå¹¶ç¼–å†™ä»£ç å®Œæˆæ¨¡å‹å’Œæ•°æ®çš„åŠ è½½å·¥ä½œã€‚</p>
<h2 id="2-æ’å…¥-QDQ-èŠ‚ç‚¹"><a href="#2-æ’å…¥-QDQ-èŠ‚ç‚¹" class="headerlink" title="2. æ’å…¥ QDQ èŠ‚ç‚¹"></a><strong>2.</strong> <strong>æ’å…¥ QDQ èŠ‚ç‚¹</strong></h2><p>ç¬¬äºŒä¸ªå°±æ˜¯æˆ‘ä»¬éœ€è¦å¯¹æ¨¡å‹æ’å…¥ QDQ èŠ‚ç‚¹ï¼Œå®ƒæœ‰ä»¥ä¸‹ä¸¤ç§æ–¹å¼ï¼š</p>
<ul>
<li><strong>è‡ªåŠ¨æ’å…¥</strong><ul>
<li>ä½¿ç”¨ quant_modules.initialize() è‡ªåŠ¨æ’å…¥é‡åŒ–èŠ‚ç‚¹</li>
</ul>
</li>
<li><strong>æ‰‹åŠ¨æ’å…¥</strong><ul>
<li>ä½¿ç”¨ quant_modules.initialize() åˆå§‹åŒ–é‡åŒ–æ“ä½œæˆ–ä½¿ç”¨ QuantDescriptor() è‡ªå®šä¹‰åˆå§‹åŒ–é‡åŒ–æ“ä½œ</li>
<li>ç¼–å†™ä»£ç ä¸ºæ¨¡å‹æ’å…¥é‡åŒ–èŠ‚ç‚¹</li>
</ul>
</li>
</ul>
<h2 id="3-æ ‡å®š"><a href="#3-æ ‡å®š" class="headerlink" title="3. æ ‡å®š"></a><strong>3.</strong> <strong>æ ‡å®š</strong></h2><p>ç¬¬ä¸‰éƒ¨åˆ†å°±æ˜¯æˆ‘ä»¬çš„æ ‡å®šï¼Œå…¶æµç¨‹å¦‚ä¸‹ï¼š</p>
<ul>
<li><strong>1.</strong> é€šè¿‡å°†æ ‡å®šæ•°æ®é€åˆ°ç½‘ç»œå¹¶æ”¶é›†ç½‘ç»œæ¯ä¸ªå±‚çš„è¾“å…¥è¾“å‡ºä¿¡æ¯</li>
<li><strong>2.</strong> æ ¹æ®ç»Ÿè®¡å‡ºçš„ä¿¡æ¯ï¼Œè®¡ç®—åŠ¨æ€èŒƒå›´ range å’Œ scaleï¼Œå¹¶ä¿å­˜åœ¨ QDQ èŠ‚ç‚¹ä¸­</li>
</ul>
<h2 id="4-æ•æ„Ÿå±‚åˆ†æ"><a href="#4-æ•æ„Ÿå±‚åˆ†æ" class="headerlink" title="4. æ•æ„Ÿå±‚åˆ†æ"></a><strong>4.</strong> <strong>æ•æ„Ÿå±‚åˆ†æ</strong></h2><p>ç¬¬å››éƒ¨åˆ†æ˜¯æ•æ„Ÿå±‚åˆ†æï¼Œå¤§è‡´æµç¨‹å¦‚ä¸‹ï¼š</p>
<ul>
<li><strong>1.</strong> è¿›è¡Œå•ä¸€é€å±‚é‡åŒ–ï¼Œåªå¼€å¯æŸä¸€å±‚çš„é‡åŒ–å…¶ä»–å±‚éƒ½ä¸å¼€å¯</li>
<li><strong>2.</strong> åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œæ¨¡å‹ç²¾åº¦æµ‹è¯•</li>
<li><strong>3.</strong> é€‰å‡ºå‰ 10 ä¸ªå¯¹æ¨¡å‹ç²¾åº¦å½±å“æ¯”è¾ƒå¤§çš„å±‚ï¼Œå…³é—­è¿™ 10 ä¸ªå±‚çš„é‡åŒ–ï¼Œåœ¨å‰å‘è®¡ç®—æ—¶ä½¿ç”¨ float16 è€Œä¸å»ä½¿ç”¨ int8</li>
</ul>
<h2 id="5-å¯¼å‡º-PTQ-æ¨¡å‹"><a href="#5-å¯¼å‡º-PTQ-æ¨¡å‹" class="headerlink" title="5. å¯¼å‡º PTQ æ¨¡å‹"></a><strong>5.</strong> <strong>å¯¼å‡º PTQ æ¨¡å‹</strong></h2><p>ç¬¬äº”ä¸ªå°±æ˜¯æˆ‘ä»¬åœ¨æ ‡å®šä¹‹åéœ€è¦å¯¼å‡º PTQ æ¨¡å‹ï¼Œå¯¼å‡ºæµç¨‹å¦‚ä¸‹ï¼š</p>
<ul>
<li><strong>1.</strong> éœ€è¦å°†æˆ‘ä»¬ä¸ŠèŠ‚è¯¾æ‰€è¯´çš„ quant_nn.TensorQuantizer.use_fb_fake_quant å±æ€§è®¾ç½®ä¸º true</li>
<li><strong>2.</strong> torch.onnx.export() å¯¼å‡º ONNX æ¨¡å‹</li>
</ul>
<h2 id="6-æ€§èƒ½å¯¹æ¯”"><a href="#6-æ€§èƒ½å¯¹æ¯”" class="headerlink" title="6. æ€§èƒ½å¯¹æ¯”"></a><strong>6.</strong> <strong>æ€§èƒ½å¯¹æ¯”</strong></h2><p>ç¬¬å…­ä¸ªå°±æ˜¯æ€§èƒ½çš„å¯¹æ¯”ï¼ŒåŒ…æ‹¬ç²¾åº¦å’Œé€Ÿåº¦çš„å¯¹æ¯”ã€‚</p>
<p>ä¸ŠèŠ‚è¯¾æˆ‘ä»¬å®Œæˆäº† YOLOv7-PTQ é‡åŒ–æµç¨‹ä¸­çš„å‡†å¤‡å·¥ä½œå’Œæ’å…¥ QDQ èŠ‚ç‚¹ï¼Œè¿™èŠ‚æˆ‘ä»¬ç»§ç»­æŒ‰ç…§æµç¨‹èµ°ï¼Œå…ˆæ¥å®ç°æ¨¡å‹çš„æ ‡å®šå·¥ä½œï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼ï¼ï¼ğŸš€ğŸš€ğŸš€</p>
<h1 id="2-æ¨¡å‹æ ‡å®š"><a href="#2-æ¨¡å‹æ ‡å®š" class="headerlink" title="2. æ¨¡å‹æ ‡å®š"></a>2. æ¨¡å‹æ ‡å®š</h1><p>æ¨¡å‹é‡åŒ–æ ¡å‡†ä¸»è¦æ˜¯ç”±ä»¥ä¸‹ä¸‰ä¸ª<a target="_blank" rel="noopener" href="https://marketing.csdn.net/p/3127db09a98e0723b83b2914d9256174?pId=2782?utm_source=glcblog&amp;spm=1001.2101.3001.7020">å‡½æ•°</a>å®Œæˆçš„ï¼š</p>
<h2 id="1-calibrate-model"><a href="#1-calibrate-model" class="headerlink" title="1. calibrate_model"></a><strong>1.</strong> <strong>calibrate_model</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calibrate_model</span>(<span class="params">model, dataloader, device</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># æ”¶é›†å‰å‘ä¿¡æ¯</span></span><br><span class="line">    collect_stats(model, dataloader, device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># è·å–åŠ¨æ€èŒƒå›´ï¼Œè®¡ç®— amax å€¼ï¼Œscale å€¼</span></span><br><span class="line">    compute_amax(model, method = <span class="string">&#x27;mse&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>è¯¥å‡½æ•°ä¸»è¦æ˜¯è®²ä¸¤ä¸ªæ ¡å‡†æ­¥éª¤ç»„åˆèµ·æ¥ï¼Œç”¨äºæ¨¡å‹çš„æ•´ä½“æ ¡å‡†ï¼Œæ•´ä½“æ­¥éª¤å¦‚ä¸‹ï¼š</p>
<ul>
<li>ä½¿ç”¨ collect_stats å‡½æ•°æ”¶é›†å‰å‘ä¼ æ’­çš„ç»Ÿè®¡ä¿¡æ¯</li>
<li>è°ƒç”¨ compute_amax å‡½æ•°è®¡ç®—é‡åŒ–çš„å°ºåº¦å› å­ amax</li>
</ul>
<h2 id="2-collect-stats"><a href="#2-collect-stats" class="headerlink" title="2. collect_stats"></a><strong>2.</strong> <strong>collect_stats</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">collect_stats</span>(<span class="params">model, data_loader, device, num_batch = <span class="number">200</span></span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># å¼€å¯æ ¡å‡†å™¨</span></span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                module.disable_quant()</span><br><span class="line">                module.enable_calib()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                module.disable()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># test</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, datas <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">            imgs = datas[<span class="number">0</span>].to(device, non_blocking=<span class="literal">True</span>).<span class="built_in">float</span>() / <span class="number">255.0</span></span><br><span class="line">            model(imgs)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i &gt;= num_batch:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># å…³é—­æ ¡å‡†å™¨</span></span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                module.enable_quant()</span><br><span class="line">                module.disable_calib()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                module.enable()</span><br></pre></td></tr></table></figure>
<p>è¯¥å‡½æ•°çš„ç›®çš„æ˜¯æ”¶é›†æ¨¡å‹åœ¨ç»™å®šæ•°æ®é›†ä¸Šçš„æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯ï¼Œè¿™é€šå¸¸æ˜¯æ¨¡å‹é‡åŒ–æ ¡å‡†è¿‡ç¨‹ä¸­çš„ç¬¬ä¸€æ­¥ï¼Œå…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š</p>
<ul>
<li>è®¾ç½®æ¨¡å‹ä¸º eval æ¨¡å‹ï¼Œç¡®ä¿ä¸å¯ç”¨å¦‚ dropout è¿™æ ·çš„è®­ç»ƒç‰¹æœ‰çš„è¡Œä¸º</li>
<li>éå†æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—ï¼Œå¯¹äºæ¯ä¸€ä¸ª TensorQuantizer å®ä¾‹<ul>
<li>å¦‚æœæœ‰æ ¡å‡†å™¨å­˜åœ¨ï¼Œåˆ™ç¦ç”¨é‡åŒ–ï¼ˆä¸å¯¹è¾“å…¥è¿›è¡Œé‡åŒ–ï¼‰å¹¶å¯åŠ¨æ ¡å‡†æ¨¡å¼ï¼ˆæ”¶é›†ç»Ÿè®¡ä¿¡æ¯ï¼‰</li>
<li>å¦‚æœæ²¡æœ‰æ ¡å‡†å™¨ï¼Œåˆ™å®Œå…¨ç¦ç”¨è¯¥é‡åŒ–å™¨ï¼ˆä¸æ‰§è¡Œä»»ä½•æ“ä½œï¼‰</li>
</ul>
</li>
<li>ä½¿ç”¨ data_loader æ¥æä¾›æ•°æ®ï¼Œå¹¶é€šè¿‡æ¨¡å‹æ‰§è¡Œå‰å‘ä¼ æ’­<ul>
<li>è®²æ•°æ®è½¬ç§»åˆ° device ä¸Šï¼Œå¹¶è¿›è¡Œé€‚å½“çš„å½’ä¸€åŒ–</li>
<li>å¯¹æ¯ä¸ªæ‰¹æ¬¡æ•°æ®ï¼Œæ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œä½†ä¸è¿›è¡Œæ¢¯åº¦è®¡ç®—</li>
<li>æ”¶é›†æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯ç›´åˆ°å¤„ç†æŒ‡å®šæ•°é‡çš„æ‰¹æ¬¡</li>
</ul>
</li>
<li>æœ€åï¼Œéå†æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—ï¼Œå¯¹äºæ¯ä¸€ä¸ª TensorQuantizer å®ä¾‹<ul>
<li>å¦‚æœæœ‰æ ¡å‡†å™¨å­˜åœ¨ï¼Œåˆ™å¯ç”¨é‡åŒ–å¹¶ç¦ç”¨æ ¡å‡†æ¨¡å¼</li>
<li>å¦‚æœæ²¡æœ‰æ ¡å‡†å™¨ï¼Œåˆ™é‡æ–°å¯ç”¨è¯¥é‡åŒ–å™¨</li>
</ul>
</li>
</ul>
<h2 id="3-compute-amax"><a href="#3-compute-amax" class="headerlink" title="3. compute_amax"></a><strong>3.</strong> <strong>compute_amax</strong></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_amax</span>(<span class="params">model, **kwargs</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(module._calibrator, calib.MaxCalibrator):</span><br><span class="line">                    module.load_calib_amax()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    module.load_calib_amax(**kwargs)</span><br><span class="line">                module._amax = module._amax.to(device)</span><br></pre></td></tr></table></figure>
<p>ä¸€æ—¦æ”¶é›†äº†æ¿€æ´»çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œè¯¥å‡½æ•°å°±ä¼šè®¡ç®—é‡åŒ–çš„å°ºåº¦å› å­ amaxï¼ˆåŠ¨æ€èŒƒå›´çš„æœ€å¤§å€¼ï¼‰ï¼Œè¿™é€šå¸¸æ˜¯æ¨¡å‹é‡åŒ–æ ¡å‡†è¿‡ç¨‹ä¸­çš„ç¬¬äºŒæ­¥ï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š</p>
<ul>
<li>éå†æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—ï¼Œå¯¹äºæ¯ä¸€ä¸ª TensorQuantizer å®ä¾‹<ul>
<li>å¦‚æœæœ‰æ ¡å‡†å™¨å­˜åœ¨ï¼Œåˆ™æ ¹æ®æ”¶é›†çš„ç»Ÿè®¡ä¿¡æ¯è®¡ç®— amax å€¼ï¼Œè¿™ä¸ªå€¼ä»£è¡¨äº†æ¿€æ´»çš„æœ€å¤§å¹…å€¼ï¼Œç”¨äºç¡®å®šé‡åŒ–çš„å°ºåº¦</li>
<li>å°† amax å€¼è½¬ç§»åˆ° device ä¸Šï¼Œä»¥ä¾¿åœ¨åç»­ä¸­ä½¿ç”¨</li>
</ul>
</li>
</ul>
<p>ä¸‹é¢æˆ‘ä»¬ç®€å•æ€»ç»“ä¸‹æ¨¡å‹é‡åŒ–æ ¡å‡†çš„æµç¨‹ï¼š</p>
<ul>
<li><p><strong>1.æ•°æ®å‡†å¤‡</strong>: å‡†å¤‡ç”¨äºæ ‡å®šçš„æ•°æ®é›†ï¼Œé€šå¸¸æ˜¯<a target="_blank" rel="noopener" href="https://ml-summit.org/cloud-member?uid=c1041&amp;spm=1001.2101.3001.7020">æ¨¡å‹è®­ç»ƒ</a>æˆ–éªŒè¯æ•°æ®é›†çš„ä¸€ä¸ªå­é›†ã€‚</p>
</li>
<li><p><strong>2.æ”¶é›†ç»Ÿè®¡ä¿¡æ¯</strong>: é€šè¿‡ collect_stats å‡½æ•°è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œä»¥æ”¶é›†æ¨¡å‹å„å±‚çš„æ¿€æ´»åˆ†å¸ƒç»Ÿè®¡ä¿¡æ¯ã€‚</p>
</li>
<li><p><strong>3.è®¡ç®— amax</strong>: ä½¿ç”¨ compute_amax å‡½æ•°åŸºäºæ”¶é›†çš„ç»Ÿè®¡ä¿¡æ¯è®¡ç®—é‡åŒ–å‚æ•°ï¼ˆå¦‚æœ€å¤§æ¿€æ´»å€¼ amaxï¼‰ã€‚</p>
</li>
</ul>
<p>é€šè¿‡ä¸Šè¿°æ­¥éª¤ï¼Œæ¨¡å‹å°±å¯ä»¥å¾—åˆ°åˆé€‚çš„é‡åŒ–å‚æ•°ï¼Œä»è€Œåœ¨é‡åŒ–åä¿æŒæ€§èƒ½å¹¶å‡å°ç²¾åº¦æŸå¤±ã€‚</p>
<p>å®Œæ•´çš„ç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> yaml</span><br><span class="line"><span class="keyword">import</span> test</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> models.yolo <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization <span class="keyword">import</span> calib</span><br><span class="line"><span class="keyword">from</span> absl <span class="keyword">import</span> logging <span class="keyword">as</span> quant_logging</span><br><span class="line"><span class="keyword">from</span> utils.datasets <span class="keyword">import</span> create_dataloader</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization <span class="keyword">import</span> quant_modules</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization <span class="keyword">import</span> nn <span class="keyword">as</span> quant_nn</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization.tensor_quant <span class="keyword">import</span> QuantDescriptor</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization.nn.modules <span class="keyword">import</span> _utils <span class="keyword">as</span> quant_nn_utils</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_yolov7_model</span>(<span class="params">weight, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line">    ckpt  = torch.load(weight, map_location=device)</span><br><span class="line">    model = Model(<span class="string">&quot;cfg/training/yolov7.yaml&quot;</span>, ch=<span class="number">3</span>, nc=<span class="number">80</span>).to(device)</span><br><span class="line">    state_dict = ckpt[<span class="string">&#x27;model&#x27;</span>].<span class="built_in">float</span>().state_dict()</span><br><span class="line">    model.load_state_dict(state_dict, strict=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_val_dataset</span>(<span class="params">cocodir, batch_size=<span class="number">32</span></span>):</span><br><span class="line">    dataloader = create_dataloader(</span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;cocodir&#125;</span>/val2017.txt&quot;</span>,</span><br><span class="line">        imgsz=<span class="number">640</span>,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        opt=collections.namedtuple(<span class="string">&quot;Opt&quot;</span>, <span class="string">&quot;single_cls&quot;</span>)(<span class="literal">False</span>),</span><br><span class="line">        augment=<span class="literal">False</span>, hyp=<span class="literal">None</span>, rect=<span class="literal">True</span>, cache=<span class="literal">False</span>, stride=<span class="number">32</span>, pad=<span class="number">0.5</span>, image_weights=<span class="literal">False</span></span><br><span class="line">    )[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_train_dataset</span>(<span class="params">cocodir, batch_size=<span class="number">32</span></span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;data/hyp.scratch.p5.yaml&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        hyp = yaml.load(f, Loader=yaml.SafeLoader)</span><br><span class="line"></span><br><span class="line">    dataloader = create_dataloader(</span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;cocodir&#125;</span>/train2017.txt&quot;</span>,</span><br><span class="line">        imgsz=<span class="number">640</span>,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        opt=collections.namedtuple(<span class="string">&quot;Opt&quot;</span>, <span class="string">&quot;single_cls&quot;</span>)(<span class="literal">False</span>),</span><br><span class="line">        augment=<span class="literal">True</span>, hyp=hyp, rect=<span class="literal">True</span>, cache=<span class="literal">False</span>, stride=<span class="number">32</span>, pad=<span class="number">0</span>, image_weights=<span class="literal">False</span></span><br><span class="line">    )[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br><span class="line"></span><br><span class="line"><span class="comment"># input: Max ==&gt; Histogram</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize</span>():</span><br><span class="line">    quant_desc_input = QuantDescriptor(calib_method=<span class="string">&#x27;histogram&#x27;</span>)</span><br><span class="line">    quant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)</span><br><span class="line">    quant_nn.QuantMaxPool2d.set_default_quant_desc_input(quant_desc_input)</span><br><span class="line">    quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)</span><br><span class="line"></span><br><span class="line">    quant_logging.set_verbosity(quant_logging.ERROR)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_model</span>(<span class="params">weight, device</span>):</span><br><span class="line">    <span class="comment"># quant_modules.initialize()</span></span><br><span class="line">    initialize()</span><br><span class="line">    model = load_yolov7_model(weight, device)</span><br><span class="line">    model.<span class="built_in">float</span>()</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        model.fuse()    <span class="comment"># conv bn è¿›è¡Œå±‚çš„åˆå¹¶, åŠ é€Ÿ</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tranfer_torch_to_quantization</span>(<span class="params">nn_instance, quant_module</span>):</span><br><span class="line">    </span><br><span class="line">    quant_instances = quant_module.__new__(quant_module)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># å±æ€§èµ‹å€¼</span></span><br><span class="line">    <span class="keyword">for</span> k, val <span class="keyword">in</span> <span class="built_in">vars</span>(nn_instance).items():</span><br><span class="line">        <span class="built_in">setattr</span>(quant_instances, k, val)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åˆå§‹åŒ–</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># è¿”å›ä¸¤ä¸ª QuantDescriptor çš„å®ä¾‹ self.__class__ æ˜¯ quant_instance çš„ç±», QuantConv2d</span></span><br><span class="line">        quant_desc_input, quant_desc_weight = quant_nn_utils.pop_quant_desc_in_kwargs(self.__class__)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(self, quant_nn_utils.QuantInputMixin):</span><br><span class="line">            self.init_quantizer(quant_desc_input)</span><br><span class="line">            <span class="comment"># åŠ å¿«é‡åŒ–é€Ÿåº¦</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(self._input_quantizer._calibrator, calib.HistogramCalibrator):</span><br><span class="line">                self._input_quantizer._calibrator._torch_hist = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.init_quantizer(quant_desc_input, quant_desc_weight)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(self._input_quantizer._calibrator, calib.HistogramCalibrator):</span><br><span class="line">                self._input_quantizer._calibrator._torch_hist = <span class="literal">True</span></span><br><span class="line">                self._weight_quantizer._calibrator._torch_hist = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    __init__(quant_instances)</span><br><span class="line">    <span class="keyword">return</span> quant_instances</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">torch_module_find_quant_module</span>(<span class="params">model, module_list, prefix=<span class="string">&#x27;&#x27;</span></span>):</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> model._modules:</span><br><span class="line">        submodule = model._modules[name]</span><br><span class="line">        path = name <span class="keyword">if</span> prefix == <span class="string">&#x27;&#x27;</span> <span class="keyword">else</span> prefix + <span class="string">&#x27;.&#x27;</span> + name</span><br><span class="line">        torch_module_find_quant_module(submodule, module_list, prefix=path) <span class="comment"># é€’å½’</span></span><br><span class="line"></span><br><span class="line">        submodule_id = <span class="built_in">id</span>(<span class="built_in">type</span>(submodule))</span><br><span class="line">        <span class="keyword">if</span> submodule_id <span class="keyword">in</span> module_list:</span><br><span class="line">            <span class="comment"># è½¬æ¢</span></span><br><span class="line">            model._modules[name] = tranfer_torch_to_quantization(submodule, module_list[submodule_id])</span><br><span class="line">        </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">replace_to_quantization_model</span>(<span class="params">model</span>):</span><br><span class="line">    </span><br><span class="line">    module_list = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> entry <span class="keyword">in</span> quant_modules._DEFAULT_QUANT_MAP:</span><br><span class="line">        module = <span class="built_in">getattr</span>(entry.orig_mod, entry.mod_name)  <span class="comment"># module -&gt; torch.nn.modules.conv.Conv1d</span></span><br><span class="line">        module_list[<span class="built_in">id</span>(module)] = entry.replace_mod</span><br><span class="line">    </span><br><span class="line">    torch_module_find_quant_module(model, module_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_coco</span>(<span class="params">model, loader, save_dir=<span class="string">&#x27;&#x27;</span>, conf_thres=<span class="number">0.001</span>, iou_thres=<span class="number">0.65</span></span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> save_dir <span class="keyword">and</span> os.path.dirname(save_dir) != <span class="string">&quot;&quot;</span>:</span><br><span class="line">        os.makedirs(os.path.dirname(save_dir), exist_ok=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> test.test(</span><br><span class="line">        <span class="string">&quot;data/coco.yaml&quot;</span>,</span><br><span class="line">        save_dir=Path(save_dir),</span><br><span class="line">        conf_thres=conf_thres,</span><br><span class="line">        iou_thres=iou_thres,</span><br><span class="line">        model=model,</span><br><span class="line">        dataloader=loader,</span><br><span class="line">        is_coco=<span class="literal">True</span>,</span><br><span class="line">        plots=<span class="literal">False</span>,</span><br><span class="line">        half_precision=<span class="literal">True</span>,</span><br><span class="line">        save_json=<span class="literal">False</span></span><br><span class="line">    )[<span class="number">0</span>][<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collect_stats</span>(<span class="params">model, data_loader, device, num_batch = <span class="number">200</span></span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># å¼€å¯æ ¡å‡†å™¨</span></span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                module.disable_quant()</span><br><span class="line">                module.enable_calib()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                module.disable()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># test</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, datas <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">            imgs = datas[<span class="number">0</span>].to(device, non_blocking=<span class="literal">True</span>).<span class="built_in">float</span>() / <span class="number">255.0</span></span><br><span class="line">            model(imgs)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i &gt;= num_batch:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># å…³é—­æ ¡å‡†å™¨</span></span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                module.enable_quant()</span><br><span class="line">                module.disable_calib()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                module.enable()</span><br><span class="line">            </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_amax</span>(<span class="params">model, **kwargs</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(module._calibrator, calib.MaxCalibrator):</span><br><span class="line">                    module.load_calib_amax()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    module.load_calib_amax(**kwargs)</span><br><span class="line">                module._amax = module._amax.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calibrate_model</span>(<span class="params">model, dataloader, device</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># æ”¶é›†å‰å‘ä¿¡æ¯</span></span><br><span class="line">    collect_stats(model, dataloader, device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># è·å–åŠ¨æ€èŒƒå›´ï¼Œè®¡ç®— amax å€¼ï¼Œscale å€¼</span></span><br><span class="line">    compute_amax(model, method = <span class="string">&#x27;mse&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    weight = <span class="string">&quot;yolov7.pt&quot;</span></span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åŠ è½½æ•°æ®</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Evalute Dataset...&quot;</span>)</span><br><span class="line">    cocodir = <span class="string">&quot;dataset/coco2017&quot;</span></span><br><span class="line">    val_dataloader   = prepare_val_dataset(cocodir)</span><br><span class="line">    train_dataloader = prepare_train_dataset(cocodir)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åŠ è½½ pth æ¨¡å‹</span></span><br><span class="line">    pth_model = load_yolov7_model(weight, device)</span><br><span class="line">    <span class="comment"># pth æ¨¡å‹éªŒè¯</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Evalute Origin...&quot;</span>)</span><br><span class="line">    ap = evaluate_coco(pth_model, val_dataloader)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># è·å–ä¼ªé‡åŒ–æ¨¡å‹(æ‰‹åŠ¨ initial(), æ‰‹åŠ¨æ’å…¥ QDQ)</span></span><br><span class="line">    model = prepare_model(weight, device)</span><br><span class="line">    replace_to_quantization_model(model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># æ¨¡å‹æ ‡å®š</span></span><br><span class="line">    calibrate_model(model, train_dataloader, device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># # PTQ æ¨¡å‹éªŒè¯</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Evaluate PTQ...&quot;</span>)</span><br><span class="line">    ptq_ap = evaluate_coco(model, val_dataloader)</span><br></pre></td></tr></table></figure>
<p>å€¼å¾—æ³¨æ„çš„æ˜¯æˆ‘ä»¬æ ¡å‡†æ—¶æ˜¯åœ¨è®­ç»ƒé›†ä¸Šå®Œæˆçš„ï¼Œæµ‹è¯•æ—¶æ˜¯åœ¨éªŒè¯é›†ä¸Šå®Œæˆçš„ï¼Œè¿è¡Œæ•ˆæœå¦‚ä¸‹ï¼š</p>
<img src="/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/0175313e5448829028e4b202628242e5.png" class="" title="0175313e5448829028e4b202628242e5">
<p>å¯ä»¥çœ‹åˆ°é‡åŒ–æ ¡å‡†åçš„æ¨¡å‹çš„ mAP ä»…ä»…ä¸‹é™äº† 0.003 ä¸ªç‚¹ã€‚</p>
<p>åšä¸»å­¦å¾—æœ‰ç‚¹æ··æ·†äº†ï¼Œå…ˆæ¢³ç†ä¸‹ä¸€äº›æ¦‚å¿µï¼Œæˆ‘ä»¬æ”¶é›†ç»Ÿè®¡ä¿¡æ¯çš„ç›®çš„æ˜¯ä¸ºäº†ç¡®å®šå½“å‰ tensor çš„ amax å³å¹…åº¦çš„æœ€å¤§å€¼ï¼Œç„¶åæ ¹æ®ä¸åŒçš„æ ¡å‡†æ–¹æ³•å’Œè·å–çš„ç»Ÿè®¡ä¿¡æ¯å»æ ¡å‡†è®¡ç®— amaxï¼Œå…¶ä¸­åŒ…æ‹¬ Max å’Œç›´æ–¹å›¾ä¸¤ç§æ ¡å‡†æ–¹æ³•ï¼ŒMax æ ¡å‡†æ–¹æ³•ç›´æ¥é€‰æ‹© tensor ç»Ÿè®¡ä¿¡æ¯çš„æœ€å¤§å€¼æ¥ä½œä¸º amaxï¼Œè€Œç›´æ–¹å›¾æ ¡å‡†ä¸­åˆåŒ…å« entropyã€mseã€percentile ä¸‰ç§æ–¹æ³•æ¥è®¡ç®— amaxï¼Œ~ä¸Šè¿°è¿‡ç¨‹ä»…ä»…æ˜¯è¿›è¡Œäº†æ ¡å‡†ç¡®å®šäº† amax å€¼ï¼Œå¾—åˆ°äº†é‡åŒ–æ—¶æ‰€éœ€è¦çš„ scaleï¼Œä½†æ˜¯è¿˜æ²¡æœ‰åˆ©ç”¨ scale è¿›è¡Œå…·ä½“çš„é‡åŒ–æ“ä½œï¼Œæ¨¡å‹çš„æƒé‡æˆ–æ¿€æ´»å€¼è¿˜æ²¡æœ‰æ”¹å˜ï¼Œåº”è¯¥æ˜¯è¿™ä¹ˆç†è§£çš„å§ğŸ˜‚~</p>
<p><strong>ä¸Šè¿°è¿‡ç¨‹ä¸­è¿›è¡Œäº†æ ¡å‡†ç¡®å®šäº† amax å€¼ï¼Œå¾—åˆ°äº†é‡åŒ–æ—¶æ‰€éœ€è¦çš„ scaleï¼Œå¹¶åœ¨æ¨¡å‹ forward çš„è¿‡ç¨‹ä¸­å†…éƒ¨æ‰§è¡Œäº†é‡åŒ–æ“ä½œï¼Œå› æ­¤ä¸Šè¿°æµç¨‹æ˜¯è¿›è¡Œäº† PTQ é‡åŒ–çš„</strong></p>
<hr>
<hr>
<p><strong>2023/12/27 æ–°å¢å†…å®¹</strong></p>
<p>åšä¸»ä¹‹å‰ä¸€ç›´ä»¥ä¸º Q/DQ èŠ‚ç‚¹æ˜¯ QAT é‡åŒ–ä¸“å±çš„ï¼Œè¿™è¿˜æ˜¯å±äºé‡åŒ–çš„ä¸€äº›åŸºç¡€æ¦‚å¿µéƒ½æ²¡æœ‰ç†æ¸…æ¥šğŸ˜‚</p>
<p>å®é™…ä¸Š Q/DQ èŠ‚ç‚¹æ—¢ç”¨äº QAT é‡åŒ–ä¹Ÿç”¨äº PTQ é‡åŒ–ï¼Œè¿™ä¸¤ç§é‡åŒ–ç­–ç•¥çš„ä¸»è¦åŒºåˆ«åœ¨äºå®ƒä»¬ä½¿ç”¨ Q/DQ èŠ‚ç‚¹çš„æ–¹å¼å’Œé‡åŒ–çš„æ—¶é—´ç‚¹ï¼Œå…·ä½“å¦‚ä¸‹ï¼š(<strong>from ChatGPT</strong>)</p>
<p><strong>PTQ ä¸­çš„ Q/DQ èŠ‚ç‚¹</strong></p>
<ul>
<li>åœ¨ PTQ é‡åŒ–è¿‡ç¨‹ä¸­ï¼ŒQ/DQ èŠ‚ç‚¹è¢«æ’å…¥åˆ°å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹ä¸­ã€‚è¿™æ˜¯ä¸ºäº†æ¨¡æ‹Ÿé‡åŒ–è¿‡ç¨‹ä¸­å¯¹æ¨¡å‹æ¨ç†çš„å½±å“ï¼Œå¹¶é€šè¿‡æ ¡å‡†æ•°æ®æ¥ç¡®å®šæœ€ä½³çš„é‡åŒ–å‚æ•°ï¼ˆå¦‚ scale å’Œ zero-pointï¼‰</li>
<li>åœ¨ PTQ é‡åŒ–è¿‡ç¨‹ä¸­ï¼ŒQ/DQ èŠ‚ç‚¹<strong>ä¸»è¦ç”¨äºé‡åŒ–è½¬æ¢è¿‡ç¨‹ä¸­çš„æ•°æ®æ”¶é›†å’Œé‡åŒ–å‚æ•°çš„ç¡®å®šï¼Œå®ƒä»¬ä¸å‚ä¸æ¨¡å‹è®­ç»ƒçš„åå‘ä¼ æ’­è¿‡ç¨‹</strong></li>
</ul>
<p><strong>QAT ä¸­çš„ Q/DQ èŠ‚ç‚¹</strong></p>
<ul>
<li>åœ¨ QAT é‡åŒ–è¿‡ç¨‹ä¸­ï¼ŒQ/DQ èŠ‚ç‚¹æ˜¯æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚å®ƒä»¬è¢«ç”¨æ¥æ¨¡æ‹Ÿé‡åŒ–çš„å½±å“ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´æ¨¡å‹çš„æƒé‡ï¼Œä»¥æœ€å°åŒ–é‡åŒ–å¸¦æ¥çš„æ€§èƒ½æŸå¤±</li>
<li>åœ¨ QAT é‡åŒ–è¿‡ç¨‹ä¸­ï¼ŒQ/DQ èŠ‚ç‚¹<strong>å¯¹æ¨¡å‹æƒé‡çš„æ›´æ–°æœ‰ç›´æ¥å½±å“</strong>ã€‚è¿™æ˜¯å› ä¸ºå®ƒä»¬å‚ä¸äº†æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ã€‚</li>
</ul>
<p>æ‰€ä»¥è¯´ Q/DQ åœ¨ PTQ å’Œ QAT ä¸­æ‰®æ¼”ç€ä¸åŒçš„è§’è‰²ï¼Œåœ¨ PTQ ä¸­æ˜¯æ¨¡æ‹Ÿé‡åŒ–è¿‡ç¨‹ç¡®å®š scaleï¼Œè€Œåœ¨ QAT ä¸­ä¸ä»…ä»…ä¼šæ¨¡æ‹Ÿé‡åŒ–ç¡®å®š scale è¿˜ä¼šåœ¨å¾®è°ƒè®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´æ¨¡å‹çš„æƒé‡ä»¥é€‚åº”é‡åŒ–å¸¦æ¥çš„å½±å“</p>
<p>ä»¥ä¸‹æ˜¯ QAT ä¸­ Q/DQ èŠ‚ç‚¹ä½œç”¨çš„è¯¦ç»†è§£é‡Šï¼š(<strong>from ChatGPT</strong>)</p>
<ul>
<li><strong>æ¨¡æ‹Ÿè®­ç»ƒç¯å¢ƒ</strong>ï¼šQ/DQ èŠ‚ç‚¹è¢«å¼•å…¥åˆ°å·¡ç¤¼è¿‡ç¨‹ä¸­ï¼Œæ¨¡æ‹Ÿé‡åŒ–åæ¨¡å‹çš„è¿è¡Œç¯å¢ƒã€‚è¿™æ„å‘³ç€åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæƒé‡å’Œæ¿€æ´»æ•°æ®ä¼šç»å†å®é™…çš„é‡åŒ–å’Œåé‡åŒ–è¿‡ç¨‹ã€‚</li>
<li><strong>æƒé‡è°ƒæ•´</strong>ï¼šç”±äºé‡åŒ–è¿‡ç¨‹å¯èƒ½å¼•å…¥ä¸€å®šçš„è¯¯å·®ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šé€šè¿‡æ ‡å‡†çš„æ¢¯åº¦ä¸‹é™å’Œåå‘ä¼ æ’­è¿‡ç¨‹ï¼Œ<strong>ä¸æ–­è°ƒæ•´æƒé‡</strong>ã€‚è¿™ä¸ªè¿‡ç¨‹æ—¨åœ¨<strong>ä½¿æ¨¡å‹é€‚åº”é‡åŒ–å¸¦æ¥çš„å½±å“ï¼Œä»è€Œå‡å°‘é‡åŒ–è¯¯å·®å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“</strong></li>
<li><strong>å­¦ä¹ é‡åŒ–å‚æ•°</strong>ï¼šåŒæ—¶ï¼ŒQAT è¿‡ç¨‹ä¸­è¿˜ä¼šå­¦ä¹ ç¡®å®šé‡åŒ–è¿‡ç¨‹ä¸­çš„å…³é”®å‚æ•°ï¼Œå¦‚ scale å’Œ zero-pointã€‚è¿™äº›å‚æ•°æ˜¯é‡åŒ–è¿‡ç¨‹ä¸­éå¸¸å…³é”®çš„ï¼Œå®ƒä»¬å†³å®šäº†å¦‚ä½•è®²æµ®ç‚¹æ•°å€¼æ˜ å°„åˆ°æ•´æ•°è¡¨ç¤º</li>
<li><strong>æœ€ç»ˆç»“æœ</strong>ï¼šé€šè¿‡è¿™ç§æ–¹å¼ï¼ŒQAT é‡åŒ–åçš„æ¨¡å‹ä¸ä»…ä»…æ˜¯è·å¾—äº†é€‚åˆé‡åŒ–çš„ scale å€¼ï¼Œè€Œä¸”å…¶æƒé‡ä¹Ÿè¢«è°ƒæ•´ä¸ºæ›´é€‚åˆé‡åŒ–åçš„è¿è¡Œç¯å¢ƒï¼Œè¿™æœ‰åŠ©äºä¿æŒæˆ–æ¥è¿‘åŸå§‹æµ®ç‚¹æ¨¡å‹çš„æ€§èƒ½</li>
</ul>
<p>QAT å’Œ PTQ é‡åŒ–æœ€æ˜¾è‘—çš„åŒºåˆ«åœ¨äº QAT é‡åŒ–ä¸­æ¨¡å‹çš„æƒé‡ä¼šå‘ç”Ÿå˜åŒ–ä»¥é€‚åº”é‡åŒ–å¸¦æ¥çš„å½±å“ã€‚</p>
<p>ç®€å•æ€»ç»“ä¸‹ï¼ŒPTQ å’Œ QAT æ¨¡å‹éƒ½ä¼šæºå¸¦ Q/DQ èŠ‚ç‚¹ï¼ŒQAT é‡åŒ–ä¼šé€šè¿‡è®­ç»ƒçš„æ–¹å¼è·å– scale ç­‰é‡åŒ–ä¿¡æ¯å¹¶è°ƒæ•´æ¨¡å‹æƒé‡ä»¥é€‚åº”é‡åŒ–å¸¦æ¥çš„å½±å“ï¼ŒPTQ é‡åŒ–åˆ™æ˜¯é€šè¿‡æ ¡å‡†å›¾ç‰‡æ¥è·å– scale ç­‰é‡åŒ–ä¿¡æ¯æ— éœ€è®­ç»ƒ</p>
<p>æœ€åå†æ¥æ¢³ç†ä¸‹äºŒè€…çš„åŒºåˆ«ï¼š(<strong>from ChatGPT</strong>)</p>
<p><strong>PTQ</strong></p>
<ul>
<li><strong>æ“ä½œæ—¶é—´</strong>ï¼šPTQ æ˜¯åœ¨æ¨¡å‹è®­ç»ƒå®Œæˆåè¿›è¡Œçš„ã€‚è¿™ç§æ–¹æ³•ä¸æ¶‰åŠé‡æ–°è®­ç»ƒæ¨¡å‹</li>
<li><strong>ä¸»è¦æ­¥éª¤</strong>ï¼š<ul>
<li><strong>æ’å…¥ Q/DQ èŠ‚ç‚¹</strong>ï¼šé¦–å…ˆåœ¨æ¨¡å‹çš„é€‚å½“ä½ç½®æ’å…¥é‡åŒ–ï¼ˆQuantizeï¼‰å’Œåé‡åŒ–ï¼ˆDequantizeï¼‰èŠ‚ç‚¹</li>
<li><strong>æ ¡å‡†</strong>ï¼šé€šè¿‡ä½¿ç”¨ä¸€ç»„ä»£è¡¨æ€§æ•°æ®ï¼ˆé€šå¸¸å«æ ¡å‡†æ•°æ®é›†ï¼‰æ¥è¿è¡Œæ¨¡å‹ï¼Œä»¥æ­¤æ¥æ”¶é›†æ¿€æ´»ï¼ˆActivationï¼‰çš„ç»Ÿè®¡æ•°æ®ã€‚è¿™äº›æ•°æ®ç”¨äºç¡®å®šé‡åŒ–å‚æ•°ï¼ˆå¦‚ scale å’Œ zero-pointï¼‰</li>
<li><strong>é‡åŒ–è½¬æ¢</strong>ï¼šåˆ©ç”¨æ”¶é›†åˆ°çš„ç»Ÿè®¡æ•°æ®ï¼Œå°†æµ®ç‚¹æƒé‡å’Œæ¿€æ´»è½¬æ¢ä¸ºæ•´æ•°æ ¼å¼</li>
</ul>
</li>
<li><strong>ä¼˜åŠ¿</strong>ï¼šæ“ä½œç®€å•ï¼Œä¸éœ€è¦é¢å¤–è®­ç»ƒï¼Œé€‚ç”¨äºèµ„æºæœ‰é™çš„æƒ…å†µ</li>
<li><strong>åŠ£åŠ¿</strong>ï¼šå¯èƒ½ä¼šæœ‰è¾ƒå¤§çš„ç²¾åº¦æŸå¤±ï¼Œå°¤å…¶æ˜¯å¯¹äºé‚£äº›å¯¹é‡åŒ–æ•æ„Ÿçš„æ¨¡å‹ï¼ˆéœ€è¦è¿›è¡Œæ•æ„Ÿå±‚åˆ†æï¼‰</li>
</ul>
<p><strong>QAT</strong></p>
<ul>
<li><strong>æ“ä½œæ—¶é—´</strong>ï¼šQAT æ˜¯åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œçš„ã€‚å®ƒå®é™…ä¸Šæ˜¯æ¨¡å‹è®­ç»ƒçš„ä¸€ä¸ªéƒ¨åˆ†ã€‚</li>
<li><strong>ä¸»è¦æ­¥éª¤</strong>ï¼š<ul>
<li><strong>æ¨¡æ‹Ÿé‡åŒ–</strong>ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¼•å…¥ Q/DQ èŠ‚ç‚¹ï¼Œæ¨¡æ‹Ÿé‡åŒ–è¿‡ç¨‹ä¸­çš„å½±å“ã€‚è¿™æ„å‘³ç€åœ¨å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­æ—¶ï¼Œæƒé‡å’Œæ¿€æ´»éƒ½ä¼šç»å†é‡åŒ–å’Œåé‡åŒ–çš„è¿‡ç¨‹</li>
<li><strong>è®­ç»ƒå¾®è°ƒ</strong>ï¼šé€šè¿‡å¯¹æ¨¡å‹çš„æ­£å¸¸è®­ç»ƒæµç¨‹è¿›è¡Œå¾®è°ƒï¼Œè°ƒæ•´æƒé‡ï¼Œä»¥è¡¥å¿é‡åŒ–è¿‡ç¨‹å¯èƒ½å¼•å…¥çš„è¯¯å·®</li>
<li><strong>å­¦ä¹ é‡åŒ–å‚æ•°</strong>ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ ç¡®å®šæœ€ä½³çš„é‡åŒ–å‚æ•°ï¼ˆå¦‚ scaleï¼‰</li>
</ul>
</li>
<li><strong>ä¼˜åŠ¿</strong>ï¼šç”±äºæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å·²ç»é€‚åº”äº†é‡åŒ–çš„å½±å“ï¼Œå› æ­¤é‡åŒ–åçš„æ¨¡å‹é€šå¸¸æœ‰æ›´å¥½çš„æ€§èƒ½å’Œè¾ƒå°çš„ç²¾åº¦æŸå¤±</li>
<li><strong>åŠ£åŠ¿</strong>ï¼šéœ€è¦é¢å¤–çš„è®­ç»ƒèµ„æºå’Œæ—¶é—´ï¼Œç›¸å¯¹äº PTQ æ¥è¯´æ›´åŠ å¤æ‚</li>
</ul>
<p>OKï¼Œä»¥ä¸Šå°±æ˜¯æœ¬æ¬¡æ›´æ–°æ–°å¢çš„å†…å®¹ï¼Œå¦‚æœ‰ä¸å¯¹çš„åœ°æ–¹ï¼Œæ¬¢è¿å„ä½çœ‹å®˜æ‰¹è¯„æŒ‡æ­£ğŸ˜„</p>
<hr>
<hr>
<p>ä¸‹é¢æˆ‘ä»¬æ¥å¯¹æ¯”ä¸‹ Max å’Œç›´æ–¹å›¾æ ¡å‡†æ–¹æ³•çš„ PTQ æ¨¡å‹çš„å¯¹æ¯”ï¼Œæ¥çœ‹çœ‹ä¸åŒçš„æ ¡å‡†æ–¹æ³•å¯¹æ¨¡å‹çš„å½±å“</p>
<p>ä¸Šé¢æˆ‘ä»¬æµ‹è¯•äº†ç›´æ–¹å›¾æ ¡å‡†åçš„ PTQ <a target="_blank" rel="noopener" href="https://edu.csdn.net/cloud/pm_summit?utm_source=blogglc&amp;spm=1001.2101.3001.7020">æ¨¡å‹æ€§èƒ½</a>ï¼Œä¸‹é¢æˆ‘ä»¬æ¥çœ‹ Max æ ¡å‡†æ–¹æ³•ï¼Œæˆ‘ä»¬å°† prepare_model å‡½æ•°ä¸­çš„æ‰‹åŠ¨ initialize å‡½æ•°æ³¨é‡Šï¼Œæ‰“å¼€è‡ªåŠ¨åˆå§‹åŒ– quant_module.initialize</p>
<p>å†æ¬¡æ‰§è¡Œä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š</p>
<img src="/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/0a1db5742c80d3fbfc571ceaa95b816b.png" class="" title="0a1db5742c80d3fbfc571ceaa95b816b">
<p>å¯ä»¥çœ‹åˆ°æˆ‘ä»¬ä½¿ç”¨é»˜è®¤çš„ Max æ ¡å‡†æ–¹æ³•å¾—åˆ°çš„ mAP å€¼æ˜¯ 0.444ï¼Œç›¸æ¯”äºä¹‹å‰ç›´æ–¹å›¾æ ¡å‡†çš„æ•ˆæœè¦å·®ä¸€äº›ï¼Œå› æ­¤åç»­æˆ‘ä»¬å¯èƒ½å°±ä½¿ç”¨ç›´æ–¹å›¾æ ¡å‡†çš„æ–¹å¼æ¥è¿›è¡Œé‡åŒ–ã€‚</p>
<p>ä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹ PTQ æ¨¡å‹çš„å¯¼å‡ºï¼Œå¯¼å‡ºå‡½æ•°å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">export_ptq</span>(<span class="params">model, save_file, device, dynamic_batch = <span class="literal">True</span></span>):</span><br><span class="line">    </span><br><span class="line">    input_dummy = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">640</span>, <span class="number">640</span>, device=device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># æ‰“å¼€ fake ç®—å­</span></span><br><span class="line">    quant_nn.TensorQuantizer.use_fb_fake_quant = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        torch.onnx.export(model, input_dummy, save_file, opset_version=<span class="number">13</span>,</span><br><span class="line">                          input_names=[<span class="string">&#x27;input&#x27;</span>], output_names=[<span class="string">&#x27;output&#x27;</span>],</span><br><span class="line">                          dynamic_axes=&#123;<span class="string">&#x27;input&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>&#125;, <span class="string">&#x27;output&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>&#125;&#125; <span class="keyword">if</span> dynamic_batch <span class="keyword">else</span> <span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>æ‰§è¡Œåæ•ˆæœå¦‚ä¸‹ï¼š</p>
<img src="/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/9fded8c52fb14152c5445db3ce933bb2.png" class="" title="9fded8c52fb14152c5445db3ce933bb2">
<p>æˆ‘ä»¬å°†å¯¼å‡ºçš„ PTQ æ¨¡å‹å’ŒåŸå§‹çš„ YOLOv7 æ¨¡å‹å¯¹æ¯”ï¼Œ</p>
<img src="/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/23a9074455dd319a5c73c8bc4ce233d6.png" class="" title="23a9074455dd319a5c73c8bc4ce233d6">
<p>å·¦è¾¹æ˜¯æˆ‘ä»¬åŸå§‹çš„ ONNXï¼Œå³è¾¹æ˜¯æˆ‘ä»¬ PTQ æ¨¡å‹çš„ ONNXï¼Œå¯ä»¥çœ‹åˆ°å¯¼å‡ºçš„ PTQ æ¨¡å‹ä¸­å¤šäº† QDQ èŠ‚ç‚¹çš„æ’å…¥ï¼Œå…¶ä¸­åŒ…å«äº†æ ¡å‡†é‡åŒ–ä¿¡æ¯ scaleã€‚</p>
<p>ä»¥ä¸Šå°±æ˜¯ torch å’Œ PTQ æ¨¡å‹çš„å¯¹æ¯”ï¼Œä¸‹é¢æˆ‘ä»¬æ¥è¿›è¡Œæ•æ„Ÿå±‚çš„åˆ†æã€‚</p>
<h1 id="3-æ•æ„Ÿå±‚åˆ†æ"><a href="#3-æ•æ„Ÿå±‚åˆ†æ" class="headerlink" title="3. æ•æ„Ÿå±‚åˆ†æ"></a>3. æ•æ„Ÿå±‚åˆ†æ</h1><p>æˆ‘ä»¬å…ˆæ¢³ç†ä¸‹æ•æ„Ÿå±‚åˆ†æçš„æµç¨‹ï¼š</p>
<ul>
<li><strong>1.</strong> for å¾ªç¯ model çš„æ¯ä¸€ä¸ª quantizer å±‚</li>
<li><strong>2.</strong> åªå…³é—­è¯¥å±‚çš„é‡åŒ–ï¼Œå…¶ä½™å±‚çš„é‡åŒ–ä¿ç•™</li>
<li><strong>3.</strong> éªŒè¯æ¨¡å‹çš„ç²¾åº¦ï¼Œevaluate_coco(), å¹¶ä¿å­˜ç²¾åº¦å€¼</li>
<li><strong>4.</strong> éªŒè¯ç»“æŸï¼Œé‡å¯è¯¥å±‚çš„é‡åŒ–æ“ä½œ</li>
<li><strong>5.</strong> for å¾ªç¯ç»“æŸï¼Œå¾—åˆ°æ‰€æœ‰å±‚çš„ç²¾åº¦å€¼</li>
<li><strong>6.</strong> æ’åºï¼Œå¾—åˆ°å‰ 10 ä¸ªå¯¹ç²¾åº¦å½±å“æ¯”è¾ƒå¤§çš„å±‚ï¼Œå°†è¿™äº›å±‚è¿›è¡Œæ‰“å°è¾“å‡º</li>
</ul>
<p>ç±»ä¼¼äºæ§åˆ¶å˜é‡æ³•ï¼Œå…³é—­æŸä¸€å±‚çš„é‡åŒ–çœ‹ç²¾åº¦ä¸‹é™å¹…åº¦ï¼Œé€‰å‡ºå¯¹ç²¾åº¦å½±å“æœ€å¤§çš„å‡ ä¸ªå±‚ä½œä¸ºæ•æ„Ÿå±‚ã€‚</p>
<p>æˆ‘ä»¬æ¥æŒ‰ç…§ä¸Šè¿°æµç¨‹ç¼–å†™ä»£ç å³å¯ï¼Œé¦–å…ˆæ˜¯ <strong>sensitive_analysis</strong> å‡½æ•°çš„å®ç°ï¼Œä»£ç å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sensitive_analysis</span>(<span class="params">model, loader</span>):</span><br><span class="line">    </span><br><span class="line">    save_file = <span class="string">&quot;senstive_analysis.json&quot;</span></span><br><span class="line"></span><br><span class="line">    summary =  SummaryTools(save_file)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># for å¾ªç¯æ¯ä¸€ä¸ªå±‚</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sensitive analysis by each layer...&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(model.model)):</span><br><span class="line">        layer = model.model[i]</span><br><span class="line">        <span class="comment"># åˆ¤æ–­ layer æ˜¯å¦æ˜¯é‡åŒ–å±‚</span></span><br><span class="line">        <span class="keyword">if</span> have_quantizer(layer):   <span class="comment"># å¦‚æœæ˜¯é‡åŒ–å±‚</span></span><br><span class="line">            <span class="comment"># ä½¿è¯¥å±‚çš„é‡åŒ–å¤±æ•ˆï¼Œä¸è¿›è¡Œ int8 çš„é‡åŒ–ï¼Œä½¿ç”¨ fp16 ç²¾åº¦è¿ç®—</span></span><br><span class="line">            disable_quantization(layer).apply()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># è®¡ç®— map å€¼</span></span><br><span class="line">            ap = evaluate_coco(model, loader )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ä¿å­˜ç²¾åº¦å€¼ï¼Œjson æ–‡ä»¶</span></span><br><span class="line">            summary.append([ap, <span class="string">f&quot;model.<span class="subst">&#123;i&#125;</span>&quot;</span>])</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;layer <span class="subst">&#123;i&#125;</span> ap: <span class="subst">&#123;ap&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># é‡å¯å±‚çš„é‡åŒ–ï¼Œè¿˜åŸ</span></span><br><span class="line">            enable_quantization(layer).apply()</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;ignore model.<span class="subst">&#123;i&#125;</span> because it is <span class="subst">&#123;<span class="built_in">type</span>(layer)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># å¾ªç¯ç»“æŸï¼Œæ‰“å°å‰ 10 ä¸ªå½±å“æ¯”è¾ƒå¤§çš„å±‚</span></span><br><span class="line">    summary = <span class="built_in">sorted</span>(summary.data, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Sensitive Summary&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> n, (ap, name) <span class="keyword">in</span> <span class="built_in">enumerate</span>(summary[:<span class="number">10</span>]):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Top<span class="subst">&#123;n&#125;</span>: Using fp16 <span class="subst">&#123;name&#125;</span>, ap = <span class="subst">&#123;ap:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>è¯¥å‡½æ•°æ˜¯æ•æ„Ÿå±‚åˆ†æçš„ä¸»è¦å‡½æ•°ï¼Œå…¶å…·ä½“å®ç°æµç¨‹å¦‚ä¸‹ï¼š</p>
<ul>
<li>å¾ªç¯éå†æ¨¡å‹çš„æ¯ä¸€å±‚ï¼Œé€šè¿‡ä½¿ç”¨ <strong>have_quantizer</strong> å‡½æ•°æ¥æ£€æŸ¥å±‚æ˜¯å¦ä¸ºé‡åŒ–å±‚</li>
<li>ä½¿ç”¨ <strong>disable_quantization</strong> å’Œ <strong>enable_quantization</strong> ç±»æ¥å…³é—­å’Œé‡å¯é‡åŒ–</li>
<li>ä½¿ç”¨ä¹‹å‰çš„ <strong>evaluate_coco</strong> å‡½æ•°æ¥è®¡ç®— mAP å€¼</li>
<li>ä½¿ç”¨ <strong>SummaryTools</strong> ç±»æ¥ä¿å­˜æ¯å±‚çš„è¯„ä¼°ç»“æœ</li>
<li>æœ€åæ‰“å°å‰ 10 ä¸ªå¯¹ç²¾åº¦å½±å“æœ€å¤§çš„å±‚</li>
</ul>
<p>ä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹å…¶ä¸­è°ƒç”¨çš„å‡½æ•°å’Œç±»çš„å…·ä½“å®ç°</p>
<p>é¦–å…ˆæ˜¯ <strong>have_quantizer</strong> å‡½æ•°ï¼Œå…¶å…·ä½“å®ç°å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åˆ¤æ–­å±‚æ˜¯å¦æ˜¯é‡åŒ–å±‚</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">have_quantizer</span>(<span class="params">layer</span>):</span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> layer.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>è¯¥å‡½æ•°çš„åŠŸèƒ½æ˜¯æ£€æŸ¥ä¼ å…¥çš„å±‚æ˜¯å¦ä¸ºé‡åŒ–å±‚ï¼Œé€šè¿‡éå†è¯¥å±‚çš„æ‰€æœ‰æ¨¡å—ï¼Œæ£€æµ‹æ˜¯å¦æœ‰ <strong>quant_nn.TensorQuantizer</strong> çš„æ¨¡å—ï¼Œå¦‚æœæœ‰åˆ™è¿”å› Trueï¼Œä»£è¡¨è¯¥å±‚ä¸ºé‡åŒ–å±‚ï¼Œå¦åˆ™è¿”å› Falseã€‚</p>
<p>ç„¶åæ˜¯ <strong>disable_quantization</strong> å’Œ <strong>enable_quantization</strong> ç±»ï¼Œå…¶å…·ä½“å®ç°å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">disable_quantization</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åˆå§‹åŒ–</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åº”ç”¨ å…³é—­é‡åŒ–</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">apply</span>(<span class="params">self, disabled=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.model.named_modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">                module._disabled = disabled</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__enter__</span>(<span class="params">self</span>):</span><br><span class="line">        self.apply(disabled=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__exit__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        self.apply(disabled=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># é‡å¯é‡åŒ–</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">enable_quantization</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">apply</span>(<span class="params">self, enabled=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.model.named_modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">                module._disabled = <span class="keyword">not</span> enabled</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__enter__</span>(<span class="params">self</span>):</span><br><span class="line">        self.apply(enabled=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__exit__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        self.apply(enabled=<span class="literal">False</span>) </span><br></pre></td></tr></table></figure>
<p>å®ƒä»¬çš„åŠŸèƒ½æ˜¯åˆ†åˆ«ç”¨äºä¸´æ—¶å…³é—­å’Œé‡å¯æ¨¡å‹ä¸­çš„é‡åŒ–æ“ä½œã€‚è¿™ä¸¤ä¸ªç±»åœ¨æ„é€ æ—¶ä¼šæ¥æ”¶æ¨¡å‹å¯¹è±¡ï¼Œå¹¶åœ¨ <strong>apply</strong> æ–¹æ³•ä¸­éå†æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—ï¼Œæ ¹æ®é‡åŒ–çŠ¶æ€ï¼ˆå¯ç”¨/ç¦ç”¨ï¼‰è®¾ç½® <strong>module._disabled</strong> å±æ€§ã€‚</p>
<p>æœ€åæ˜¯ <strong>SummaryTools</strong> ç±»ï¼Œå…¶å®ç°å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SummaryTools</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file</span>):</span><br><span class="line">        self.file = file</span><br><span class="line">        self.data = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">append</span>(<span class="params">self, item</span>):</span><br><span class="line">        self.data.append(item)</span><br><span class="line">        json.dump(self.data, <span class="built_in">open</span>(self.file, <span class="string">&quot;w&quot;</span>), indent=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>è¯¥ç±»çš„åŠŸèƒ½æ˜¯ç”¨äºä¿å­˜æ¯å±‚çš„ mAP ç»“æœã€‚åœ¨å…¶ <strong>append</strong> æ–¹æ³•ä¸­ä¼šæ·»åŠ  mAP ç»“æœåˆ°å†…éƒ¨æ•°æ®åˆ—è¡¨ï¼Œå¹¶å°†è¿™äº›æ•°æ®ä¿å­˜åˆ° JSON æ–‡ä»¶ä¸­ã€‚</p>
<p>å®Œæ•´çš„æ•æ„Ÿå±‚åˆ†æä»£ç å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> yaml</span><br><span class="line"><span class="keyword">import</span> test</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> models.yolo <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization <span class="keyword">import</span> calib</span><br><span class="line"><span class="keyword">from</span> absl <span class="keyword">import</span> logging <span class="keyword">as</span> quant_logging</span><br><span class="line"><span class="keyword">from</span> utils.datasets <span class="keyword">import</span> create_dataloader</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization <span class="keyword">import</span> quant_modules</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization <span class="keyword">import</span> nn <span class="keyword">as</span> quant_nn</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization.tensor_quant <span class="keyword">import</span> QuantDescriptor</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization.nn.modules <span class="keyword">import</span> _utils <span class="keyword">as</span> quant_nn_utils</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_yolov7_model</span>(<span class="params">weight, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line">    ckpt  = torch.load(weight, map_location=device)</span><br><span class="line">    model = Model(<span class="string">&quot;cfg/training/yolov7.yaml&quot;</span>, ch=<span class="number">3</span>, nc=<span class="number">80</span>).to(device)</span><br><span class="line">    state_dict = ckpt[<span class="string">&#x27;model&#x27;</span>].<span class="built_in">float</span>().state_dict()</span><br><span class="line">    model.load_state_dict(state_dict, strict=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_val_dataset</span>(<span class="params">cocodir, batch_size=<span class="number">32</span></span>):</span><br><span class="line">    dataloader = create_dataloader(</span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;cocodir&#125;</span>/val2017.txt&quot;</span>,</span><br><span class="line">        imgsz=<span class="number">640</span>,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        opt=collections.namedtuple(<span class="string">&quot;Opt&quot;</span>, <span class="string">&quot;single_cls&quot;</span>)(<span class="literal">False</span>),</span><br><span class="line">        augment=<span class="literal">False</span>, hyp=<span class="literal">None</span>, rect=<span class="literal">True</span>, cache=<span class="literal">False</span>, stride=<span class="number">32</span>, pad=<span class="number">0.5</span>, image_weights=<span class="literal">False</span></span><br><span class="line">    )[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_train_dataset</span>(<span class="params">cocodir, batch_size=<span class="number">32</span></span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;data/hyp.scratch.p5.yaml&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        hyp = yaml.load(f, Loader=yaml.SafeLoader)</span><br><span class="line"></span><br><span class="line">    dataloader = create_dataloader(</span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;cocodir&#125;</span>/train2017.txt&quot;</span>,</span><br><span class="line">        imgsz=<span class="number">640</span>,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        opt=collections.namedtuple(<span class="string">&quot;Opt&quot;</span>, <span class="string">&quot;single_cls&quot;</span>)(<span class="literal">False</span>),</span><br><span class="line">        augment=<span class="literal">True</span>, hyp=hyp, rect=<span class="literal">True</span>, cache=<span class="literal">False</span>, stride=<span class="number">32</span>, pad=<span class="number">0</span>, image_weights=<span class="literal">False</span></span><br><span class="line">    )[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br><span class="line"></span><br><span class="line"><span class="comment"># input: Max ==&gt; Histogram</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize</span>():</span><br><span class="line">    quant_desc_input = QuantDescriptor(calib_method=<span class="string">&#x27;histogram&#x27;</span>)</span><br><span class="line">    quant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)</span><br><span class="line">    quant_nn.QuantMaxPool2d.set_default_quant_desc_input(quant_desc_input)</span><br><span class="line">    quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)</span><br><span class="line"></span><br><span class="line">    quant_logging.set_verbosity(quant_logging.ERROR)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_model</span>(<span class="params">weight, device</span>):</span><br><span class="line">    <span class="comment"># quant_modules.initialize()</span></span><br><span class="line">    initialize()</span><br><span class="line">    model = load_yolov7_model(weight, device)</span><br><span class="line">    model.<span class="built_in">float</span>()</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        model.fuse()    <span class="comment"># conv bn è¿›è¡Œå±‚çš„åˆå¹¶, åŠ é€Ÿ</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tranfer_torch_to_quantization</span>(<span class="params">nn_instance, quant_module</span>):</span><br><span class="line">    </span><br><span class="line">    quant_instances = quant_module.__new__(quant_module)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># å±æ€§èµ‹å€¼</span></span><br><span class="line">    <span class="keyword">for</span> k, val <span class="keyword">in</span> <span class="built_in">vars</span>(nn_instance).items():</span><br><span class="line">        <span class="built_in">setattr</span>(quant_instances, k, val)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åˆå§‹åŒ–</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># è¿”å›ä¸¤ä¸ª QuantDescriptor çš„å®ä¾‹ self.__class__ æ˜¯ quant_instance çš„ç±», QuantConv2d</span></span><br><span class="line">        quant_desc_input, quant_desc_weight = quant_nn_utils.pop_quant_desc_in_kwargs(self.__class__)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(self, quant_nn_utils.QuantInputMixin):</span><br><span class="line">            self.init_quantizer(quant_desc_input)</span><br><span class="line">            <span class="comment"># åŠ å¿«é‡åŒ–é€Ÿåº¦</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(self._input_quantizer._calibrator, calib.HistogramCalibrator):</span><br><span class="line">                self._input_quantizer._calibrator._torch_hist = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.init_quantizer(quant_desc_input, quant_desc_weight)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(self._input_quantizer._calibrator, calib.HistogramCalibrator):</span><br><span class="line">                self._input_quantizer._calibrator._torch_hist = <span class="literal">True</span></span><br><span class="line">                self._weight_quantizer._calibrator._torch_hist = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    __init__(quant_instances)</span><br><span class="line">    <span class="keyword">return</span> quant_instances</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">quantization_ignore_match</span>(<span class="params">ignore_layer, path</span>):</span><br><span class="line">    <span class="keyword">if</span> ignore_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(ignore_layer, <span class="built_in">str</span>) <span class="keyword">or</span> <span class="built_in">isinstance</span>(ignore_layer, <span class="built_in">list</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(ignore_layer, <span class="built_in">str</span>):</span><br><span class="line">            ignore_layer = [ignore_layer]</span><br><span class="line">        <span class="keyword">if</span> path <span class="keyword">in</span> ignore_layer:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> ignore_layer:</span><br><span class="line">            <span class="keyword">if</span> re.<span class="keyword">match</span>(item, path):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">torch_module_find_quant_module</span>(<span class="params">model, module_list, ignore_layer, prefix=<span class="string">&#x27;&#x27;</span></span>):</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> model._modules:</span><br><span class="line">        submodule = model._modules[name]</span><br><span class="line">        path = name <span class="keyword">if</span> prefix == <span class="string">&#x27;&#x27;</span> <span class="keyword">else</span> prefix + <span class="string">&#x27;.&#x27;</span> + name</span><br><span class="line">        torch_module_find_quant_module(submodule, module_list, ignore_layer, prefix=path) <span class="comment"># é€’å½’</span></span><br><span class="line"></span><br><span class="line">        submodule_id = <span class="built_in">id</span>(<span class="built_in">type</span>(submodule))</span><br><span class="line">        <span class="keyword">if</span> submodule_id <span class="keyword">in</span> module_list:</span><br><span class="line">            ignored = quantization_ignore_match(ignore_layer, path)</span><br><span class="line">            <span class="keyword">if</span> ignored:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Quantization : <span class="subst">&#123;path&#125;</span> has ignored.&quot;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># è½¬æ¢</span></span><br><span class="line">            model._modules[name] = tranfer_torch_to_quantization(submodule, module_list[submodule_id])</span><br><span class="line">        </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">replace_to_quantization_model</span>(<span class="params">model, ignore_layer=<span class="literal">None</span></span>):</span><br><span class="line">    </span><br><span class="line">    module_list = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> entry <span class="keyword">in</span> quant_modules._DEFAULT_QUANT_MAP:</span><br><span class="line">        module = <span class="built_in">getattr</span>(entry.orig_mod, entry.mod_name)  <span class="comment"># module -&gt; torch.nn.modules.conv.Conv1d</span></span><br><span class="line">        module_list[<span class="built_in">id</span>(module)] = entry.replace_mod</span><br><span class="line">    </span><br><span class="line">    torch_module_find_quant_module(model, module_list, ignore_layer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_coco</span>(<span class="params">model, loader, save_dir=<span class="string">&#x27;&#x27;</span>, conf_thres=<span class="number">0.001</span>, iou_thres=<span class="number">0.65</span></span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> save_dir <span class="keyword">and</span> os.path.dirname(save_dir) != <span class="string">&quot;&quot;</span>:</span><br><span class="line">        os.makedirs(os.path.dirname(save_dir), exist_ok=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> test.test(</span><br><span class="line">        <span class="string">&quot;data/coco.yaml&quot;</span>,</span><br><span class="line">        save_dir=Path(save_dir),</span><br><span class="line">        conf_thres=conf_thres,</span><br><span class="line">        iou_thres=iou_thres,</span><br><span class="line">        model=model,</span><br><span class="line">        dataloader=loader,</span><br><span class="line">        is_coco=<span class="literal">True</span>,</span><br><span class="line">        plots=<span class="literal">False</span>,</span><br><span class="line">        half_precision=<span class="literal">True</span>,</span><br><span class="line">        save_json=<span class="literal">False</span></span><br><span class="line">    )[<span class="number">0</span>][<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collect_stats</span>(<span class="params">model, data_loader, device, num_batch = <span class="number">200</span></span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># å¼€å¯æ ¡å‡†å™¨</span></span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                module.disable_quant()</span><br><span class="line">                module.enable_calib()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                module.disable()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># test</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, datas <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">            imgs = datas[<span class="number">0</span>].to(device, non_blocking=<span class="literal">True</span>).<span class="built_in">float</span>() / <span class="number">255.0</span></span><br><span class="line">            model(imgs)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i &gt;= num_batch:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># å…³é—­æ ¡å‡†å™¨</span></span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                module.enable_quant()</span><br><span class="line">                module.disable_calib()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                module.enable()</span><br><span class="line">            </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_amax</span>(<span class="params">model, **kwargs</span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(module._calibrator, calib.MaxCalibrator):</span><br><span class="line">                    module.load_calib_amax()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    module.load_calib_amax(**kwargs)</span><br><span class="line">                module._amax = module._amax.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calibrate_model</span>(<span class="params">model, dataloader, device</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># æ”¶é›†å‰å‘ä¿¡æ¯</span></span><br><span class="line">    collect_stats(model, dataloader, device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># è·å–åŠ¨æ€èŒƒå›´ï¼Œè®¡ç®— amax å€¼ï¼Œscale å€¼</span></span><br><span class="line">    compute_amax(model, method = <span class="string">&#x27;mse&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">export_ptq</span>(<span class="params">model, save_file, device, dynamic_batch = <span class="literal">True</span></span>):</span><br><span class="line">    </span><br><span class="line">    input_dummy = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">640</span>, <span class="number">640</span>, device=device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># æ‰“å¼€ fake ç®—å­</span></span><br><span class="line">    quant_nn.TensorQuantizer.use_fb_fake_quant = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        torch.onnx.export(model, input_dummy, save_file, opset_version=<span class="number">13</span>,</span><br><span class="line">                          input_names=[<span class="string">&#x27;input&#x27;</span>], output_names=[<span class="string">&#x27;output&#x27;</span>],</span><br><span class="line">                          dynamic_axes=&#123;<span class="string">&#x27;input&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>&#125;, <span class="string">&#x27;output&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>&#125;&#125; <span class="keyword">if</span> dynamic_batch <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    quant_nn.TensorQuantizer.use_fb_fake_quant = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ¤æ–­å±‚æ˜¯å¦æ˜¯é‡åŒ–å±‚</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">have_quantizer</span>(<span class="params">layer</span>):</span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> layer.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">disable_quantization</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åˆå§‹åŒ–</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åº”ç”¨ å…³é—­é‡åŒ–</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">apply</span>(<span class="params">self, disabled=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.model.named_modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">                module._disabled = disabled</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__enter__</span>(<span class="params">self</span>):</span><br><span class="line">        self.apply(disabled=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__exit__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        self.apply(disabled=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># é‡å¯é‡åŒ–</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">enable_quantization</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">apply</span>(<span class="params">self, enabled=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.model.named_modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">                module._disabled = <span class="keyword">not</span> enabled</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__enter__</span>(<span class="params">self</span>):</span><br><span class="line">        self.apply(enabled=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__exit__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        self.apply(enabled=<span class="literal">False</span>)    </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SummaryTools</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file</span>):</span><br><span class="line">        self.file = file</span><br><span class="line">        self.data = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">append</span>(<span class="params">self, item</span>):</span><br><span class="line">        self.data.append(item)</span><br><span class="line">        json.dump(self.data, <span class="built_in">open</span>(self.file, <span class="string">&quot;w&quot;</span>), indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sensitive_analysis</span>(<span class="params">model, loader</span>):</span><br><span class="line">    </span><br><span class="line">    save_file = <span class="string">&quot;senstive_analysis.json&quot;</span></span><br><span class="line"></span><br><span class="line">    summary =  SummaryTools(save_file)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># for å¾ªç¯æ¯ä¸€ä¸ªå±‚</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sensitive analysis by each layer...&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(model.model)):</span><br><span class="line">        layer = model.model[i]</span><br><span class="line">        <span class="comment"># åˆ¤æ–­ layer æ˜¯å¦æ˜¯é‡åŒ–å±‚</span></span><br><span class="line">        <span class="keyword">if</span> have_quantizer(layer):   <span class="comment"># å¦‚æœæ˜¯é‡åŒ–å±‚</span></span><br><span class="line">            <span class="comment"># ä½¿è¯¥å±‚çš„é‡åŒ–å¤±æ•ˆï¼Œä¸è¿›è¡Œ int8 çš„é‡åŒ–ï¼Œä½¿ç”¨ fp16 ç²¾åº¦è¿ç®—</span></span><br><span class="line">            disable_quantization(layer).apply()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># è®¡ç®— map å€¼</span></span><br><span class="line">            ap = evaluate_coco(model, loader )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ä¿å­˜ç²¾åº¦å€¼ï¼Œjson æ–‡ä»¶</span></span><br><span class="line">            summary.append([ap, <span class="string">f&quot;model.<span class="subst">&#123;i&#125;</span>&quot;</span>])</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;layer <span class="subst">&#123;i&#125;</span> ap: <span class="subst">&#123;ap&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># é‡å¯å±‚çš„é‡åŒ–ï¼Œè¿˜åŸ</span></span><br><span class="line">            enable_quantization(layer).apply()</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;ignore model.<span class="subst">&#123;i&#125;</span> because it is <span class="subst">&#123;<span class="built_in">type</span>(layer)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># å¾ªç¯ç»“æŸï¼Œæ‰“å°å‰ 10 ä¸ªå½±å“æ¯”è¾ƒå¤§çš„å±‚</span></span><br><span class="line">    summary = <span class="built_in">sorted</span>(summary.data, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Sensitive Summary&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> n, (ap, name) <span class="keyword">in</span> <span class="built_in">enumerate</span>(summary[:<span class="number">10</span>]):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Top<span class="subst">&#123;n&#125;</span>: Using fp16 <span class="subst">&#123;name&#125;</span>, ap = <span class="subst">&#123;ap:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    weight = <span class="string">&quot;yolov7.pt&quot;</span></span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åŠ è½½æ•°æ®</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Evalute Dataset...&quot;</span>)</span><br><span class="line">    cocodir = <span class="string">&quot;dataset/coco2017&quot;</span></span><br><span class="line">    val_dataloader   = prepare_val_dataset(cocodir)</span><br><span class="line">    train_dataloader = prepare_train_dataset(cocodir)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åŠ è½½ pth æ¨¡å‹</span></span><br><span class="line">    <span class="comment"># pth_model = load_yolov7_model(weight, device)</span></span><br><span class="line">    <span class="comment"># pth æ¨¡å‹éªŒè¯</span></span><br><span class="line">    <span class="comment"># print(&quot;Evalute Origin...&quot;)</span></span><br><span class="line">    <span class="comment"># ap = evaluate_coco(pth_model, val_dataloader)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># è·å–ä¼ªé‡åŒ–æ¨¡å‹(æ‰‹åŠ¨ initial(), æ‰‹åŠ¨æ’å…¥ QDQ)</span></span><br><span class="line">    model = prepare_model(weight, device)</span><br><span class="line">    replace_to_quantization_model(model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># æ¨¡å‹æ ‡å®š</span></span><br><span class="line">    calibrate_model(model, train_dataloader, device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># æ•æ„Ÿå±‚åˆ†æ</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    æµç¨‹:</span></span><br><span class="line"><span class="string">    1. for å¾ªç¯ model çš„æ¯ä¸€ä¸ª quantizer å±‚</span></span><br><span class="line"><span class="string">    2. åªå…³é—­è¯¥å±‚çš„é‡åŒ–ï¼Œå…¶ä½™å±‚çš„é‡åŒ–ä¿ç•™</span></span><br><span class="line"><span class="string">    3. éªŒè¯æ¨¡å‹çš„ç²¾åº¦, evaluate_coco(), å¹¶ä¿å­˜ç²¾åº¦å€¼</span></span><br><span class="line"><span class="string">    4. éªŒè¯ç»“æŸï¼Œé‡å¯è¯¥å±‚çš„é‡åŒ–æ“ä½œ</span></span><br><span class="line"><span class="string">    5. for å¾ªç¯ç»“æŸï¼Œå¾—åˆ°æ‰€æœ‰å±‚çš„ç²¾åº¦å€¼</span></span><br><span class="line"><span class="string">    6. æ’åºï¼Œå¾—åˆ°å‰ 10 ä¸ªå¯¹ç²¾åº¦å½±å“æ¯”è¾ƒå¤§çš„å±‚ï¼Œå°†è¿™äº›å±‚è¿›è¡Œæ‰“å°è¾“å‡º</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    sensitive_analysis(model, val_dataloader)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># PTQ æ¨¡å‹éªŒè¯</span></span><br><span class="line">    <span class="comment"># print(&quot;Evaluate PTQ...&quot;)</span></span><br><span class="line">    <span class="comment"># ptq_ap = evaluate_coco(model, val_dataloader)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># PTQ æ¨¡å‹å¯¼å‡º</span></span><br><span class="line">    <span class="comment"># print(&quot;Export PTQ...&quot;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># export_ptq(model, &quot;ptq_yolov7.onnx&quot;, device)</span></span><br></pre></td></tr></table></figure>
<p>åœ¨ä»£ç ä¸­æˆ‘ä»¬å…³é—­äº†æŸäº›ä¸å¿…è¦çš„æ“ä½œï¼Œæ‰§è¡Œåè¿è¡Œæ•ˆæœå¦‚ä¸‹ï¼š</p>
<img src="/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/9b42b85a0aa80b64c6f0071e7277a486.png" class="" title="9b42b85a0aa80b64c6f0071e7277a486">
<p>ä»ä¸Šå›¾ä¸­å¯ä»¥çœ‹å‡ºå®ƒä¼šè®¡ç®—æ¯å±‚å…³é—­é‡åŒ–åçš„ mAP å€¼ï¼Œæ¯å±‚çš„ mAP å€¼éƒ½ä¸ä¸€æ ·ï¼Œè¿™è¯´æ˜ä¸åŒå±‚é‡åŒ–å¯¹æœ€ç»ˆç²¾åº¦å½±å“çš„æ•ˆæœä¸åŒï¼Œæœ€åæˆ‘ä»¬ä¼šå°†æ¯å±‚çš„ mAP å€¼éƒ½ä¿å­˜å¹¶ç»Ÿè®¡å‰ 10 ä¸ªå¯¹ç²¾åº¦å½±å“æœ€å¤§çš„å±‚ã€‚</p>
<p>æ•æ„Ÿå±‚çš„åˆ†æç­‰å¾…æ—¶é—´ä¼šæ¯”è¾ƒä¹…ï¼Œå› ä¸ºæ¯å±‚éƒ½è¦å»è®¡ç®— mAP å€¼ã€‚ç”±äºåšä¸»<a target="_blank" rel="noopener" href="https://marketing.csdn.net/p/3127db09a98e0723b83b2914d9256174?pId=2782?utm_source=glcblog&amp;spm=1001.2101.3001.7020">ç¡¬ä»¶</a>çš„åŸå› ï¼Œæ²¡æœ‰è·‘å®Œæ‰€æœ‰å±‚çš„åˆ†æï¼Œåç»­æ˜¯ç›´æ¥é€‰ç”¨è§†é¢‘ä¸­çš„ 10 ä¸ªå±‚ä½œä¸ºæ•æ„Ÿå±‚ã€‚</p>
<p>è§†é¢‘ä¸­åˆ†æå‡ºæ¥çš„å‰ 10 ä¸ªæ•æ„Ÿå±‚å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ignore_layer = [<span class="string">&quot;model\.104\.(.*)&quot;</span>, <span class="string">&quot;model\.37\.(.*)&quot;</span>, <span class="string">&quot;model\.2\.(.*)&quot;</span>, <span class="string">&quot;model\.1\.(.*)&quot;</span>, <span class="string">&quot;model\.77\.(.*)&quot;</span>,</span><br><span class="line">                <span class="string">&quot;model\.99\.(.*)&quot;</span>, <span class="string">&quot;model\.70\.(.*)&quot;</span>, <span class="string">&quot;model\.95\.(.*)&quot;</span>, <span class="string">&quot;model\.92\.(.*)&quot;</span>, <span class="string">&quot;model\.81\.(.*)&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>OKï¼ä¸Šé¢æˆ‘ä»¬å¯¹æ•æ„Ÿå±‚è¿›è¡Œäº†ä¸€ä¸ªåˆ†æï¼Œå¹¶ä¸”å°†å‰ 10 ä¸ªå¯¹ç²¾åº¦å½±å“æœ€å¤§çš„å±‚è¿›è¡Œäº†æ‰“å°ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†å¤„ç†æ•æ„Ÿå±‚åˆ†æå‡ºæ¥çš„ç»“æœï¼Œå¯¹ç²¾åº¦å½±å“è¾ƒå¤§çš„å±‚å…³é—­å®ƒçš„é‡åŒ–ï¼Œä½¿ç”¨ FP16 è¿›è¡Œè®¡ç®—</p>
<p>æˆ‘ä»¬åœ¨è¿›è¡Œ PTQ é‡åŒ–å‰å°±è¦è¿›è¡Œæ•æ„Ÿå±‚çš„åˆ†æï¼Œå¾—åˆ°å½±å“æ¯”è¾ƒå¤§çš„å±‚ï¼Œç„¶ååœ¨ä½¿ç”¨æ‰‹åŠ¨æ’å…¥ QDQ é‡åŒ–èŠ‚ç‚¹çš„æ—¶å€™å°†è¿™äº›æ•æ„Ÿå±‚ä¼ é€’è¿›æ¥ï¼Œå°†å…¶é‡åŒ–è¿›è¡Œå…³é—­ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¯¹æ•æ„Ÿå±‚çš„å¤„ç†ã€‚</p>
<p>å› æ­¤æˆ‘ä»¬åœ¨ä¹‹å‰çš„ <strong>replace_to_quantization_model</strong> å‡½æ•°ä¸­éœ€è¦å¤šä¼ å…¥ä¸€ä¸ªå‚æ•°ï¼Œå³ä¸Šé¢çš„æ•æ„Ÿå±‚åˆ—è¡¨ï¼Œä¿®æ”¹åçš„å‡½æ•°å…·ä½“å®ç°å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">replace_to_quantization_model</span>(<span class="params">model, ignore_layer=<span class="literal">None</span></span>):</span><br><span class="line">    </span><br><span class="line">    module_list = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> entry <span class="keyword">in</span> quant_modules._DEFAULT_QUANT_MAP:</span><br><span class="line">        module = <span class="built_in">getattr</span>(entry.orig_mod, entry.mod_name)  <span class="comment"># module -&gt; torch.nn.modules.conv.Conv1d</span></span><br><span class="line">        module_list[<span class="built_in">id</span>(module)] = entry.replace_mod</span><br><span class="line">    </span><br><span class="line">    torch_module_find_quant_module(model, module_list, ignore_layer)</span><br></pre></td></tr></table></figure>
<p>æ¥ç€æˆ‘ä»¬ä¼šå°† <strong>ignore_layer</strong> åˆ—è¡¨ä¼ å…¥åˆ° <strong>torch_module_find_quant_module</strong> å‡½æ•°ä¸­ï¼Œåœ¨é‡åŒ–è½¬æ¢æ—¶å¿½ç•¥è¿™äº›å±‚ï¼Œä¿®æ”¹åçš„å‡½æ•°å…·ä½“å®ç°å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">torch_module_find_quant_module</span>(<span class="params">model, module_list, ignore_layer, prefix=<span class="string">&#x27;&#x27;</span></span>):</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> model._modules:</span><br><span class="line">        submodule = model._modules[name]</span><br><span class="line">        path = name <span class="keyword">if</span> prefix == <span class="string">&#x27;&#x27;</span> <span class="keyword">else</span> prefix + <span class="string">&#x27;.&#x27;</span> + name</span><br><span class="line">        torch_module_find_quant_module(submodule, module_list, ignore_layer, prefix=path) <span class="comment"># é€’å½’</span></span><br><span class="line"></span><br><span class="line">        submodule_id = <span class="built_in">id</span>(<span class="built_in">type</span>(submodule))</span><br><span class="line">        <span class="keyword">if</span> submodule_id <span class="keyword">in</span> module_list:</span><br><span class="line">            ignored = quantization_ignore_match(ignore_layer, path)</span><br><span class="line">            <span class="keyword">if</span> ignored:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Quantization : <span class="subst">&#123;path&#125;</span> has ignored.&quot;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># è½¬æ¢</span></span><br><span class="line">            model._modules[name] = tranfer_torch_to_quantization(submodule, module_list[submodule_id])</span><br></pre></td></tr></table></figure>
<p>è¯¥å‡½æ•°åŠŸèƒ½è¿˜æ˜¯éå†æ¨¡å‹çš„æ¯ä¸ªå­æ¨¡å—ï¼Œæ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›è¡Œé‡åŒ–è½¬æ¢ã€‚ä½†ä¸ä¹‹å‰ä¸åŒçš„æ˜¯æˆ‘ä»¬æ–°å¢äº†ä¸€ä¸ªåˆ¤æ–­ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨ <strong>quantization_ignore_match</strong> å‡½æ•°æ¥åˆ¤æ–­å½“å‰å­æ¨¡å—æ˜¯å¦åœ¨ <strong>ignore_layer</strong> åˆ—è¡¨ä¸­ï¼Œå¦‚æœåœ¨åˆ™è·³è¿‡é‡åŒ–è½¬æ¢å¼€å§‹ä¸‹ä¸€ä¸ªæ¨¡å—ï¼Œå¦‚æœä¸åœ¨åˆ™æ‰§è¡Œé‡åŒ–è½¬æ¢ã€‚</p>
<p><strong>quantization_ignore_match</strong> çš„å…·ä½“å®ç°å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">quantization_ignore_match</span>(<span class="params">ignore_layer, path</span>):</span><br><span class="line">    <span class="keyword">if</span> ignore_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(ignore_layer, <span class="built_in">str</span>) <span class="keyword">or</span> <span class="built_in">isinstance</span>(ignore_layer, <span class="built_in">list</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(ignore_layer, <span class="built_in">str</span>):</span><br><span class="line">            ignore_layer = [ignore_layer]</span><br><span class="line">        <span class="keyword">if</span> path <span class="keyword">in</span> ignore_layer:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> ignore_layer:</span><br><span class="line">            <span class="keyword">if</span> re.<span class="keyword">match</span>(item, path):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p>è¯¥å‡½æ•°çš„åŠŸèƒ½æ˜¯åˆ¤æ–­æ¨¡å‹ä¸­çš„æŸä¸€ä¸ªå±‚æ˜¯å¦åœ¨ <strong>ignore_layer</strong> åˆ—è¡¨ä¸­ï¼Œå³æ˜¯å¦åº”è¯¥å¿½ç•¥è¯¥å±‚çš„é‡åŒ–ï¼Œè¿”å›å€¼æ˜¯ä¸€ä¸ªå¸ƒå°”å€¼ã€‚<strong>ignore_layer</strong> å¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ <strong>re.match</strong> æ¥æ£€æŸ¥ <strong>path</strong> æ˜¯å¦èƒ½å’Œ <strong>ignore_layer</strong> åˆ—è¡¨ä¸­çš„å…ƒç´ åŒ¹é…ä¸Šã€‚</p>
<p>æˆ‘ä»¬å°†ä¸Šè¿°ä»£ç ä¿®æ”¹å¥½åï¼Œå†æ¥æµ‹è¯•ä¸‹ï¼Œçœ‹å¿½ç•¥è¿™äº›å±‚åé‡åŒ–èŠ‚ç‚¹çš„æ’å…¥æ˜¯å¦å‘ç”Ÿå˜åŒ–ï¼Œæµ‹è¯•çš„è¿è¡Œæ•ˆæœå¦‚ä¸‹ï¼š</p>
<img src="/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/ba68500dd729d1e17a0d5f6f5e5cd043.png" class="" title="ba68500dd729d1e17a0d5f6f5e5cd043">
<p>å¯ä»¥çœ‹åˆ°æˆ‘ä»¬æ‰“å°äº†å¿½ç•¥æŸäº›å±‚çš„é‡åŒ–åæ’å…¥ QDQ èŠ‚ç‚¹çš„æ¨¡å‹ç»“æ„ï¼Œæˆ‘ä»¬ä»å›¾ä¸­å¯ä»¥çœ‹åˆ° 99 å±‚æ˜¯æˆ‘ä»¬å¿½ç•¥çš„å±‚ï¼Œå®ƒå¹¶æ²¡æœ‰ _input_quantizer å’Œ _weight_quantizerï¼Œè¯´æ˜å®ƒå¹¶æ²¡æœ‰è¢«æ’å…¥é‡åŒ–èŠ‚ç‚¹ï¼Œä½¿ç”¨çš„æ˜¯ FP16 çš„è®¡ç®—ï¼ŒåŒç† 104 å±‚ä¹Ÿæ˜¯å¦‚æ­¤ã€‚</p>
<p>é‚£ä»¥ä¸Šå°±æ˜¯æ•æ„Ÿå±‚çš„åˆ†æï¼Œä»¥åŠæˆ‘ä»¬æ ¹æ®æ•æ„Ÿå±‚çš„ç»“æœå¯¹æ•æ„Ÿå±‚çš„é‡åŒ–è¿›è¡Œå…³é—­çš„å†…å®¹äº†ã€‚</p>
<p>ä¸‹é¢æˆ‘ä»¬å†æ¥æ¢³ç†ä¸‹ PTQ é‡åŒ–</p>
<h2 id="4-PTQé‡åŒ–"><a href="#4-PTQé‡åŒ–" class="headerlink" title="4. PTQé‡åŒ–"></a>4. PTQé‡åŒ–</h2><p>è¿™èŠ‚æˆ‘ä»¬å°† PTQ çš„ä»£ç è¿›è¡Œå·¥ç¨‹åŒ–</p>
<p>é¦–å…ˆç¼–å†™ä¸€ä¸ª <strong>quantize.py</strong> å°†æˆ‘ä»¬ä¹‹å‰çš„ç¼–å†™çš„å‡½æ•°å’Œç±»æ”¾å…¥å…¶ä¸­ï¼Œå…¶å…·ä½“å†…å®¹å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> yaml</span><br><span class="line"><span class="keyword">import</span> test</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> models.yolo <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization <span class="keyword">import</span> calib</span><br><span class="line"><span class="keyword">from</span> absl <span class="keyword">import</span> logging <span class="keyword">as</span> quant_logging</span><br><span class="line"><span class="keyword">from</span> utils.datasets <span class="keyword">import</span> create_dataloader</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization <span class="keyword">import</span> quant_modules</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization <span class="keyword">import</span> nn <span class="keyword">as</span> quant_nn</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization.tensor_quant <span class="keyword">import</span> QuantDescriptor</span><br><span class="line"><span class="keyword">from</span> pytorch_quantization.nn.modules <span class="keyword">import</span> _utils <span class="keyword">as</span> quant_nn_utils</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_yolov7_model</span>(<span class="params">weight, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line">    ckpt  = torch.load(weight, map_location=device)</span><br><span class="line">    model = Model(<span class="string">&quot;cfg/training/yolov7.yaml&quot;</span>, ch=<span class="number">3</span>, nc=<span class="number">80</span>).to(device)</span><br><span class="line">    state_dict = ckpt[<span class="string">&#x27;model&#x27;</span>].<span class="built_in">float</span>().state_dict()</span><br><span class="line">    model.load_state_dict(state_dict, strict=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_val_dataset</span>(<span class="params">cocodir, batch_size=<span class="number">32</span></span>):</span><br><span class="line">    dataloader = create_dataloader(</span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;cocodir&#125;</span>/val2017.txt&quot;</span>,</span><br><span class="line">        imgsz=<span class="number">640</span>,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        opt=collections.namedtuple(<span class="string">&quot;Opt&quot;</span>, <span class="string">&quot;single_cls&quot;</span>)(<span class="literal">False</span>),</span><br><span class="line">        augment=<span class="literal">False</span>, hyp=<span class="literal">None</span>, rect=<span class="literal">True</span>, cache=<span class="literal">False</span>, stride=<span class="number">32</span>, pad=<span class="number">0.5</span>, image_weights=<span class="literal">False</span></span><br><span class="line">    )[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_train_dataset</span>(<span class="params">cocodir, batch_size=<span class="number">32</span></span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;data/hyp.scratch.p5.yaml&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        hyp = yaml.load(f, Loader=yaml.SafeLoader)</span><br><span class="line"></span><br><span class="line">    dataloader = create_dataloader(</span><br><span class="line">        <span class="string">f&quot;<span class="subst">&#123;cocodir&#125;</span>/train2017.txt&quot;</span>,</span><br><span class="line">        imgsz=<span class="number">640</span>,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        opt=collections.namedtuple(<span class="string">&quot;Opt&quot;</span>, <span class="string">&quot;single_cls&quot;</span>)(<span class="literal">False</span>),</span><br><span class="line">        augment=<span class="literal">True</span>, hyp=hyp, rect=<span class="literal">True</span>, cache=<span class="literal">False</span>, stride=<span class="number">32</span>, pad=<span class="number">0</span>, image_weights=<span class="literal">False</span></span><br><span class="line">    )[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br><span class="line"></span><br><span class="line"><span class="comment"># input: Max ==&gt; Histogram</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize</span>():</span><br><span class="line">    quant_desc_input = QuantDescriptor(calib_method=<span class="string">&#x27;histogram&#x27;</span>)</span><br><span class="line">    quant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)</span><br><span class="line">    quant_nn.QuantMaxPool2d.set_default_quant_desc_input(quant_desc_input)</span><br><span class="line">    quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)</span><br><span class="line"></span><br><span class="line">    quant_logging.set_verbosity(quant_logging.ERROR)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_model</span>(<span class="params">weight, device</span>):</span><br><span class="line">    <span class="comment"># quant_modules.initialize()</span></span><br><span class="line">    initialize()</span><br><span class="line">    model = load_yolov7_model(weight, device)</span><br><span class="line">    model.<span class="built_in">float</span>()</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        model.fuse()    <span class="comment"># conv bn è¿›è¡Œå±‚çš„åˆå¹¶, åŠ é€Ÿ</span></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tranfer_torch_to_quantization</span>(<span class="params">nn_instance, quant_module</span>):</span><br><span class="line">    </span><br><span class="line">    quant_instances = quant_module.__new__(quant_module)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># å±æ€§èµ‹å€¼</span></span><br><span class="line">    <span class="keyword">for</span> k, val <span class="keyword">in</span> <span class="built_in">vars</span>(nn_instance).items():</span><br><span class="line">        <span class="built_in">setattr</span>(quant_instances, k, val)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åˆå§‹åŒ–</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># è¿”å›ä¸¤ä¸ª QuantDescriptor çš„å®ä¾‹ self.__class__ æ˜¯ quant_instance çš„ç±», QuantConv2d</span></span><br><span class="line">        quant_desc_input, quant_desc_weight = quant_nn_utils.pop_quant_desc_in_kwargs(self.__class__)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(self, quant_nn_utils.QuantInputMixin):</span><br><span class="line">            self.init_quantizer(quant_desc_input)</span><br><span class="line">            <span class="comment"># åŠ å¿«é‡åŒ–é€Ÿåº¦</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(self._input_quantizer._calibrator, calib.HistogramCalibrator):</span><br><span class="line">                self._input_quantizer._calibrator._torch_hist = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.init_quantizer(quant_desc_input, quant_desc_weight)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(self._input_quantizer._calibrator, calib.HistogramCalibrator):</span><br><span class="line">                self._input_quantizer._calibrator._torch_hist = <span class="literal">True</span></span><br><span class="line">                self._weight_quantizer._calibrator._torch_hist = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    __init__(quant_instances)</span><br><span class="line">    <span class="keyword">return</span> quant_instances</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">quantization_ignore_match</span>(<span class="params">ignore_layer, path</span>):</span><br><span class="line">    <span class="keyword">if</span> ignore_layer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(ignore_layer, <span class="built_in">str</span>) <span class="keyword">or</span> <span class="built_in">isinstance</span>(ignore_layer, <span class="built_in">list</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(ignore_layer, <span class="built_in">str</span>):</span><br><span class="line">            ignore_layer = [ignore_layer]</span><br><span class="line">        <span class="keyword">if</span> path <span class="keyword">in</span> ignore_layer:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> ignore_layer:</span><br><span class="line">            <span class="keyword">if</span> re.<span class="keyword">match</span>(item, path):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">torch_module_find_quant_module</span>(<span class="params">model, module_list, ignore_layer, prefix=<span class="string">&#x27;&#x27;</span></span>):</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> model._modules:</span><br><span class="line">        submodule = model._modules[name]</span><br><span class="line">        path = name <span class="keyword">if</span> prefix == <span class="string">&#x27;&#x27;</span> <span class="keyword">else</span> prefix + <span class="string">&#x27;.&#x27;</span> + name</span><br><span class="line">        torch_module_find_quant_module(submodule, module_list, ignore_layer, prefix=path) <span class="comment"># é€’å½’</span></span><br><span class="line"></span><br><span class="line">        submodule_id = <span class="built_in">id</span>(<span class="built_in">type</span>(submodule))</span><br><span class="line">        <span class="keyword">if</span> submodule_id <span class="keyword">in</span> module_list:</span><br><span class="line">            ignored = quantization_ignore_match(ignore_layer, path)</span><br><span class="line">            <span class="keyword">if</span> ignored:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Quantization : <span class="subst">&#123;path&#125;</span> has ignored.&quot;</span>)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># è½¬æ¢</span></span><br><span class="line">            model._modules[name] = tranfer_torch_to_quantization(submodule, module_list[submodule_id])</span><br><span class="line">        </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">replace_to_quantization_model</span>(<span class="params">model, ignore_layer=<span class="literal">None</span></span>):</span><br><span class="line">    </span><br><span class="line">    module_list = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> entry <span class="keyword">in</span> quant_modules._DEFAULT_QUANT_MAP:</span><br><span class="line">        module = <span class="built_in">getattr</span>(entry.orig_mod, entry.mod_name)  <span class="comment"># module -&gt; torch.nn.modules.conv.Conv1d</span></span><br><span class="line">        module_list[<span class="built_in">id</span>(module)] = entry.replace_mod</span><br><span class="line">    </span><br><span class="line">    torch_module_find_quant_module(model, module_list, ignore_layer)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_coco</span>(<span class="params">model, loader, save_dir=<span class="string">&#x27;&#x27;</span>, conf_thres=<span class="number">0.001</span>, iou_thres=<span class="number">0.65</span></span>):</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> save_dir <span class="keyword">and</span> os.path.dirname(save_dir) != <span class="string">&quot;&quot;</span>:</span><br><span class="line">        os.makedirs(os.path.dirname(save_dir), exist_ok=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> test.test(</span><br><span class="line">        <span class="string">&quot;data/coco.yaml&quot;</span>,</span><br><span class="line">        save_dir=Path(save_dir),</span><br><span class="line">        conf_thres=conf_thres,</span><br><span class="line">        iou_thres=iou_thres,</span><br><span class="line">        model=model,</span><br><span class="line">        dataloader=loader,</span><br><span class="line">        is_coco=<span class="literal">True</span>,</span><br><span class="line">        plots=<span class="literal">False</span>,</span><br><span class="line">        half_precision=<span class="literal">True</span>,</span><br><span class="line">        save_json=<span class="literal">False</span></span><br><span class="line">    )[<span class="number">0</span>][<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collect_stats</span>(<span class="params">model, data_loader, device, num_batch = <span class="number">200</span></span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># å¼€å¯æ ¡å‡†å™¨</span></span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                module.disable_quant()</span><br><span class="line">                module.enable_calib()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                module.disable()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># test</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, datas <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_loader):</span><br><span class="line">            imgs = datas[<span class="number">0</span>].to(device, non_blocking=<span class="literal">True</span>).<span class="built_in">float</span>() / <span class="number">255.0</span></span><br><span class="line">            model(imgs)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i &gt;= num_batch:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># å…³é—­æ ¡å‡†å™¨</span></span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                module.enable_quant()</span><br><span class="line">                module.disable_calib()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                module.enable()</span><br><span class="line">            </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_amax</span>(<span class="params">model, device, **kwargs</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">if</span> module._calibrator <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">isinstance</span>(module._calibrator, calib.MaxCalibrator):</span><br><span class="line">                    module.load_calib_amax()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    module.load_calib_amax(**kwargs)</span><br><span class="line">                module._amax = module._amax.to(device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calibrate_model</span>(<span class="params">model, dataloader, device</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># æ”¶é›†å‰å‘ä¿¡æ¯</span></span><br><span class="line">    collect_stats(model, dataloader, device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># è·å–åŠ¨æ€èŒƒå›´ï¼Œè®¡ç®— amax å€¼ï¼Œscale å€¼</span></span><br><span class="line">    compute_amax(model, device, method = <span class="string">&#x27;mse&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">export_ptq</span>(<span class="params">model, save_file, device, dynamic_batch = <span class="literal">True</span></span>):</span><br><span class="line">    </span><br><span class="line">    input_dummy = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">640</span>, <span class="number">640</span>, device=device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># æ‰“å¼€ fake ç®—å­</span></span><br><span class="line">    quant_nn.TensorQuantizer.use_fb_fake_quant = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        torch.onnx.export(model, input_dummy, save_file, opset_version=<span class="number">13</span>,</span><br><span class="line">                          input_names=[<span class="string">&#x27;input&#x27;</span>], output_names=[<span class="string">&#x27;output&#x27;</span>],</span><br><span class="line">                          dynamic_axes=&#123;<span class="string">&#x27;input&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>&#125;, <span class="string">&#x27;output&#x27;</span>: &#123;<span class="number">0</span>: <span class="string">&#x27;batch&#x27;</span>&#125;&#125; <span class="keyword">if</span> dynamic_batch <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    quant_nn.TensorQuantizer.use_fb_fake_quant = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ¤æ–­å±‚æ˜¯å¦æ˜¯é‡åŒ–å±‚</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">have_quantizer</span>(<span class="params">layer</span>):</span><br><span class="line">    <span class="keyword">for</span> name, module <span class="keyword">in</span> layer.named_modules():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">disable_quantization</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åˆå§‹åŒ–</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åº”ç”¨ å…³é—­é‡åŒ–</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">apply</span>(<span class="params">self, disabled=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.model.named_modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">                module._disabled = disabled</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__enter__</span>(<span class="params">self</span>):</span><br><span class="line">        self.apply(disabled=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__exit__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        self.apply(disabled=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># é‡å¯é‡åŒ–</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">enable_quantization</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">apply</span>(<span class="params">self, enabled=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">for</span> name, module <span class="keyword">in</span> self.model.named_modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(module, quant_nn.TensorQuantizer):</span><br><span class="line">                module._disabled = <span class="keyword">not</span> enabled</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__enter__</span>(<span class="params">self</span>):</span><br><span class="line">        self.apply(enabled=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__exit__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        self.apply(enabled=<span class="literal">False</span>)    </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SummaryTools</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file</span>):</span><br><span class="line">        self.file = file</span><br><span class="line">        self.data = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">append</span>(<span class="params">self, item</span>):</span><br><span class="line">        self.data.append(item)</span><br><span class="line">        json.dump(self.data, <span class="built_in">open</span>(self.file, <span class="string">&quot;w&quot;</span>), indent=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sensitive_analysis</span>(<span class="params">model, loader</span>):</span><br><span class="line">    </span><br><span class="line">    save_file = <span class="string">&quot;senstive_analysis.json&quot;</span></span><br><span class="line"></span><br><span class="line">    summary =  SummaryTools(save_file)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># for å¾ªç¯æ¯ä¸€ä¸ªå±‚</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Sensitive analysis by each layer...&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(model.model)):</span><br><span class="line">        layer = model.model[i]</span><br><span class="line">        <span class="comment"># åˆ¤æ–­ layer æ˜¯å¦æ˜¯é‡åŒ–å±‚</span></span><br><span class="line">        <span class="keyword">if</span> have_quantizer(layer):   <span class="comment"># å¦‚æœæ˜¯é‡åŒ–å±‚</span></span><br><span class="line">            <span class="comment"># ä½¿è¯¥å±‚çš„é‡åŒ–å¤±æ•ˆï¼Œä¸è¿›è¡Œ int8 çš„é‡åŒ–ï¼Œä½¿ç”¨ fp16 ç²¾åº¦è¿ç®—</span></span><br><span class="line">            disable_quantization(layer).apply()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># è®¡ç®— map å€¼</span></span><br><span class="line">            ap = evaluate_coco(model, loader )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ä¿å­˜ç²¾åº¦å€¼ï¼Œjson æ–‡ä»¶</span></span><br><span class="line">            summary.append([ap, <span class="string">f&quot;model.<span class="subst">&#123;i&#125;</span>&quot;</span>])</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;layer <span class="subst">&#123;i&#125;</span> ap: <span class="subst">&#123;ap&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># é‡å¯å±‚çš„é‡åŒ–ï¼Œè¿˜åŸ</span></span><br><span class="line">            enable_quantization(layer).apply()</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;ignore model.<span class="subst">&#123;i&#125;</span> because it is <span class="subst">&#123;<span class="built_in">type</span>(layer)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># å¾ªç¯ç»“æŸï¼Œæ‰“å°å‰ 10 ä¸ªå½±å“æ¯”è¾ƒå¤§çš„å±‚</span></span><br><span class="line">    summary = <span class="built_in">sorted</span>(summary.data, key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Sensitive Summary&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> n, (ap, name) <span class="keyword">in</span> <span class="built_in">enumerate</span>(summary[:<span class="number">10</span>]):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Top<span class="subst">&#123;n&#125;</span>: Using fp16 <span class="subst">&#123;name&#125;</span>, ap = <span class="subst">&#123;ap:<span class="number">.5</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>è¿™å°±æ˜¯æˆ‘ä»¬ä¹‹å‰ç”¨äº YOLOv7-PTQ é‡åŒ–çš„å„ç§å‡½æ•°å’Œç±»çš„å®ç°ï¼Œè¿™é‡Œä¸å†èµ˜è¿°</p>
<p>å¦å¤–æˆ‘ä»¬æ–°å»ºä¸€ä¸ª <strong>ptq.py</strong> æ–‡ä»¶ï¼Œç”¨äºå®ç° YOLOv7 çš„ PTQ é‡åŒ–ï¼Œæˆ‘ä»¬é€šè¿‡ <strong>argparse</strong> æ¨¡å—æ¥ä¼ å…¥ PTQ é‡åŒ–æ‰€éœ€è¦çš„å‚æ•°ï¼Œä»£ç å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--weights&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;yolov7.pt&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;initial weights path&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--cocodir&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,  default=<span class="string">&quot;dataset/coco2017&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;coco directory&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,  default=<span class="number">8</span>, <span class="built_in">help</span>=<span class="string">&quot;batch size for data loader&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--device&#x27;</span>, default=<span class="string">&#x27;0&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;cuda device, i.e. 0 or 0,1,2,3 or cpu&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--sensitive&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">bool</span>, default=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">&quot;use sensitive analysis or not befor ptq&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--sensitive_summary&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;sensitive-summary.json&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;summary save file&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--ignore_layers&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;model\.105\.m\.(.*)&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;regx&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    parser.add_argument(<span class="string">&quot;--save_ptq&quot;</span>, <span class="built_in">type</span>=<span class="built_in">bool</span>, default=<span class="literal">False</span>, <span class="built_in">help</span>=<span class="string">&quot;file&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--ptq&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;ptq_yolov7.onnx&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;file&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    parser.add_argument(<span class="string">&quot;--confidence&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.001</span>, <span class="built_in">help</span>=<span class="string">&quot;confidence threshold&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--nmsthres&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.65</span>, <span class="built_in">help</span>=<span class="string">&quot;nms threshold&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    parser.add_argument(<span class="string">&quot;--eval_origin&quot;</span>, action=<span class="string">&quot;store_true&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;do eval for origin model&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--eval_ptq&quot;</span>, action=<span class="string">&quot;store_true&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;do eval for ptq model&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    parser.add_argument(<span class="string">&quot;--ptq_summary&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;ptq_summary.json&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;summary save file&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    args = parser.parse_args()</span><br></pre></td></tr></table></figure>
<p>ä¼ å…¥çš„å‚æ•°æœ‰æƒé‡ã€æ•°æ®é›†è·¯å¾„çš„æŒ‡å®šï¼Œæ•æ„Ÿå±‚åˆ†æçš„æŒ‡å®šï¼Œç½®ä¿¡åº¦é˜ˆå€¼çš„æŒ‡å®šç­‰ç­‰</p>
<p>æˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨ <strong>quantize.py</strong> æ¨¡å—çš„å„ç§å‡½æ•°å’Œç±»æ¥å®ç°çœŸæ­£çš„é‡åŒ–ï¼Œé‡åŒ–ä¸»è¦åˆ†ä¸ºæ•æ„Ÿå±‚åˆ†æå’Œ PTQ é‡åŒ–ä¸¤ä¸ªéƒ¨åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†åˆ«ç¼–å†™ä¸¤ä¸ªå‡½æ•°æ¥è°ƒç”¨å®ç°ï¼Œé¦–å…ˆæ˜¯æ•æ„Ÿå±‚åˆ†æå‡½æ•°ï¼Œå…¶å®ç°å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run_SensitiveAnalysis</span>(<span class="params">weight, cocodir, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prepare model</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Prepare Model ....&quot;</span>)</span><br><span class="line">    model = quantize.prepare_model(weight, device)</span><br><span class="line">    quantize.replace_to_quantization_model(model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prepare dataset</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Prepare Dataset ....&quot;</span>)</span><br><span class="line">    train_dataloader = quantize.prepare_train_dataset(cocodir)</span><br><span class="line">    val_dataloader = quantize.prepare_val_dataset(cocodir)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calibration model</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Begining Calibration ....&quot;</span>)</span><br><span class="line">    quantize.calibrate_model(model, train_dataloader, device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sensitive analysis</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Begining Sensitive Analysis ....&quot;</span>)</span><br><span class="line">    quantize.sensitive_analysis(model, val_dataloader, args.sensitive_summary)</span><br></pre></td></tr></table></figure>
<p>æˆ‘ä»¬åœ¨å‰é¢å°±è®²è¿‡æ•æ„Ÿå±‚åˆ†æçš„æµç¨‹ï¼ŒåŒ…æ‹¬æ¨¡å‹ã€æ•°æ®é›†çš„å‡†å¤‡ã€æ¨¡å‹çš„æ ‡å®šï¼Œæ•æ„Ÿå±‚çš„åˆ†æï¼Œéƒ½æ˜¯é€šè¿‡ <strong>quantize.py</strong> æ¨¡å—çš„å„ç§å‡½æ•°å’Œç±»æ¥å®ç°çš„</p>
<p>æˆ‘ä»¬å†æ¥ç¼–å†™ä¸‹è¿è¡Œ PTQ é‡åŒ–çš„å‡½æ•°ï¼Œå…¶å®ç°å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run_PTQ</span>(<span class="params">args, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prepare model</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Prepare Model ....&quot;</span>)</span><br><span class="line">    model = quantize.prepare_model(args.weights, device)</span><br><span class="line">    quantize.replace_to_quantization_model(model, args.ignore_layers)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prepare dataset</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Prepare Dataset ....&quot;</span>)</span><br><span class="line">    val_dataloader = quantize.prepare_val_dataset(args.cocodir, batch_size=args.batch_size)</span><br><span class="line">    train_dataloader = quantize.prepare_train_dataset(args.cocodir, batch_size=args.batch_size)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calibration model</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Begining Calibration ....&quot;</span>)</span><br><span class="line">    quantize.calibrate_model(model, train_dataloader, device)</span><br><span class="line">    </span><br><span class="line">    summary = quantize.SummaryTool(args.ptq_summary)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> args.eval_origin:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Evaluate Origin...&quot;</span>)</span><br><span class="line">        <span class="keyword">with</span> quantize.disable_quantization(model):</span><br><span class="line">            ap = quantize.evaluate_coco(model, val_dataloader, conf_thres=args.conf_thres, iou_thres=args.iou_thres)</span><br><span class="line">            summary.append([<span class="string">&quot;Origin&quot;</span>, ap])</span><br><span class="line">    <span class="keyword">if</span> args.eval_ptq:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Evaluate PTQ...&quot;</span>)</span><br><span class="line">        ap = quantize.evaluate_coco(model, val_dataloader, conf_thres=args.conf_thres, iou_thres=args.iou_thres)</span><br><span class="line">        summary.append([<span class="string">&quot;PTQ&quot;</span>, ap])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.save_ptq:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Export PTQ...&quot;</span>)</span><br><span class="line">        quantize.export_ptq(model, args.ptq, device)</span><br></pre></td></tr></table></figure>
<p>å®é™…çš„ PTQ é‡åŒ–è¿‡ç¨‹åŒ…æ‹¬æƒé‡ã€æ•°æ®é›†çš„å‡†å¤‡ï¼Œæ ‡å®šï¼Œåç»­ PTQ æ¨¡å‹æ€§èƒ½çš„éªŒè¯å’Œå¯¼å‡º</p>
<p>é‚£ä»¥ä¸Šå°±æ˜¯ <strong>ptq.py</strong> æ–‡ä»¶ä¸­çš„å…¨éƒ¨å†…å®¹ï¼Œå®Œæ•´çš„å†…å®¹å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> quantize</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_SensitiveAnalysis</span>(<span class="params">weight, cocodir, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prepare model</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Prepare Model ....&quot;</span>)</span><br><span class="line">    model = quantize.prepare_model(weight, device)</span><br><span class="line">    quantize.replace_to_quantization_model(model)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prepare dataset</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Prepare Dataset ....&quot;</span>)</span><br><span class="line">    train_dataloader = quantize.prepare_train_dataset(cocodir)</span><br><span class="line">    val_dataloader = quantize.prepare_val_dataset(cocodir)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># calibration model</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Begining Calibration ....&quot;</span>)</span><br><span class="line">    quantize.calibrate_model(model, train_dataloader, device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sensitive analysis</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Begining Sensitive Analysis ....&quot;</span>)</span><br><span class="line">    quantize.sensitive_analysis(model, val_dataloader, args.sensitive_summary)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_PTQ</span>(<span class="params">args, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prepare model</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Prepare Model ....&quot;</span>)</span><br><span class="line">    model = quantize.prepare_model(args.weights, device)</span><br><span class="line">    quantize.replace_to_quantization_model(model, args.ignore_layers)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prepare dataset</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Prepare Dataset ....&quot;</span>)</span><br><span class="line">    val_dataloader = quantize.prepare_val_dataset(args.cocodir, batch_size=args.batch_size)</span><br><span class="line">    train_dataloader = quantize.prepare_train_dataset(args.cocodir, batch_size=args.batch_size)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># calibration model</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Begining Calibration ....&quot;</span>)</span><br><span class="line">    quantize.calibrate_model(model, train_dataloader, device)</span><br><span class="line">    </span><br><span class="line">    summary = quantize.SummaryTool(args.ptq_summary)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> args.eval_origin:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Evaluate Origin...&quot;</span>)</span><br><span class="line">        <span class="keyword">with</span> quantize.disable_quantization(model):</span><br><span class="line">            ap = quantize.evaluate_coco(model, val_dataloader, conf_thres=args.conf_thres, iou_thres=args.iou_thres)</span><br><span class="line">            summary.append([<span class="string">&quot;Origin&quot;</span>, ap])</span><br><span class="line">    <span class="keyword">if</span> args.eval_ptq:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Evaluate PTQ...&quot;</span>)</span><br><span class="line">        ap = quantize.evaluate_coco(model, val_dataloader, conf_thres=args.conf_thres, iou_thres=args.iou_thres)</span><br><span class="line">        summary.append([<span class="string">&quot;PTQ&quot;</span>, ap])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.save_ptq:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Export PTQ...&quot;</span>)</span><br><span class="line">        quantize.export_ptq(model, args.ptq, device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--weights&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&#x27;yolov7.pt&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;initial weights path&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--cocodir&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,  default=<span class="string">&quot;dataset/coco2017&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;coco directory&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch_size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>,  default=<span class="number">8</span>, <span class="built_in">help</span>=<span class="string">&quot;batch size for data loader&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--device&#x27;</span>, default=<span class="string">&#x27;0&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;cuda device, i.e. 0 or 0,1,2,3 or cpu&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--sensitive&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">bool</span>, default=<span class="literal">True</span>, <span class="built_in">help</span>=<span class="string">&quot;use sensitive analysis or not befor ptq&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--sensitive_summary&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;sensitive-summary.json&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;summary save file&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--ignore_layers&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;model\.105\.m\.(.*)&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;regx&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    parser.add_argument(<span class="string">&quot;--save_ptq&quot;</span>, <span class="built_in">type</span>=<span class="built_in">bool</span>, default=<span class="literal">False</span>, <span class="built_in">help</span>=<span class="string">&quot;file&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--ptq&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;ptq_yolov7.onnx&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;file&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    parser.add_argument(<span class="string">&quot;--confidence&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.001</span>, <span class="built_in">help</span>=<span class="string">&quot;confidence threshold&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--nmsthres&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.65</span>, <span class="built_in">help</span>=<span class="string">&quot;nms threshold&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    parser.add_argument(<span class="string">&quot;--eval_origin&quot;</span>, action=<span class="string">&quot;store_true&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;do eval for origin model&quot;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&quot;--eval_ptq&quot;</span>, action=<span class="string">&quot;store_true&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;do eval for ptq model&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    parser.add_argument(<span class="string">&quot;--ptq_summary&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;ptq_summary.json&quot;</span>, <span class="built_in">help</span>=<span class="string">&quot;summary save file&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">    is_cuda = (args.device != <span class="string">&quot;cpu&quot;</span>) <span class="keyword">and</span> torch.cuda.is_available()</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> is_cuda <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># æ•æ„Ÿå±‚åˆ†æ</span></span><br><span class="line">    <span class="keyword">if</span> args.sensitive:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Sensitive Analysis ...&quot;</span>)</span><br><span class="line">        run_SensitiveAnalysis(args.weights, args.cocodir, device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># PTQ</span></span><br><span class="line">    <span class="comment"># ignore_layers= [&quot;model\.105\.m\.(.*)&quot;, model\.99\.m\.(.*)]</span></span><br><span class="line">    <span class="comment"># args.ignore_layer = ignore_layers</span></span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Begining PTQ ....&quot;</span>)</span><br><span class="line">    run_PTQ(args, device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;PTQ Quantization Has Finished ....&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>é‚£å…¶å®è¿™éƒ½æ˜¯æˆ‘ä»¬ä¹‹å‰è®²è¿‡çš„å†…å®¹ï¼Œåªæ˜¯è¿™è¾¹å†é‡æ–°æ•´ç†å¹¶å·¥ç¨‹åŒ–ä¸‹ï¼Œæ–¹ä¾¿æˆ‘ä»¬åç»­çš„ä½¿ç”¨ã€‚</p>
<p>OKï¼YOLOv7-PTQ é‡åŒ–çš„å†…å®¹åˆ°è¿™é‡Œå°±ç»“æŸäº†ï¼Œä¸‹èŠ‚å¼€å§‹æˆ‘ä»¬å°†è®²è§£ QAT é‡åŒ–ç›¸å…³çš„çŸ¥è¯†</p>
<h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h1><blockquote>
<p>æœ¬æ¬¡è¯¾ç¨‹ä»‹ç»äº† YOLOv7-PTQ é‡åŒ–æµç¨‹ä¸­çš„æ ‡å®šã€æ•æ„Ÿå±‚åˆ†æï¼Œæ ‡å®šä¸»è¦æ˜¯åˆ©ç”¨æ ‡å®šæ•°æ®æ¥æ”¶é›†æ¨¡å‹ä¸­å„å±‚çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œå¹¶è®¡ç®—é‡åŒ–å‚æ•°ä¿å­˜åœ¨ QDQ èŠ‚ç‚¹å½“ä¸­ï¼Œæ­¤å¤–æˆ‘ä»¬è¿˜å¯¹æ¯”äº† Max å’Œ ç›´æ–¹å›¾æ ¡å‡†ä¸¤ç§æ–¹æ³•ï¼Œå‘ç° Max æ–¹æ³•çš„æ€§èƒ½è¦å·®ä¸€äº›ï¼Œè€Œæ•æ„Ÿå±‚åˆ†æçš„æµç¨‹åˆ™æ˜¯å¾ªç¯éå†æ‰€æœ‰å±‚ï¼Œå…³é—­æŸå±‚é‡åŒ–æµ‹è¯• mAP æ€§èƒ½ï¼Œæœ€ç»ˆç»Ÿè®¡å¯¹æ¨¡å‹æ€§èƒ½æœ€å¤§çš„å‡ ä¸ªå±‚ä½œä¸ºæ•æ„Ÿå±‚ï¼Œå…³é—­å…¶é‡åŒ–ä»¥ FP16 çš„æ–¹å¼è¿è¡Œï¼Œé‚£æˆ‘ä»¬åœ¨å®é™…è¿›è¡Œ PTQ é‡åŒ–ä¹‹å‰å°±è¦åšæ•æ„Ÿå±‚çš„åˆ†æï¼Œç»Ÿè®¡å‡ºå“ªäº›å±‚æ˜¯æ•æ„Ÿå±‚åå†è¿›è¡Œé‡åŒ–ï¼Œè¿™æ ·é‡åŒ–å‡ºçš„æ¨¡å‹çš„æ€§èƒ½ä¹Ÿä¼šæ›´é«˜ã€‚æœ€å PTQ é‡åŒ–æ¨¡å‹çš„å¯¼å‡ºè®°å¾—æ‰“å¼€ fake ç®—å­ï¼Œä¹Ÿå°±æ˜¯å°† use_fb_fake_quant è®¾ç½®ä¸º Trueã€‚</p>
<p>è‡³æ­¤ï¼ŒYOLOv7-PTQ é‡åŒ–çš„å…¨éƒ¨å†…å®¹åˆ°è¿™é‡Œå°±è®²å®Œäº†ï¼Œä¸‹èŠ‚å¼€å§‹æˆ‘ä»¬å°†è¿›å…¥ YOLOv7-QAT é‡åŒ–</p>
</blockquote>
<p>æœ¬æ–‡è½¬è‡ª <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_40672115/article/details/134233620">https://blog.csdn.net/qq_40672115/article/details/134233620</a>ï¼Œå¦‚æœ‰ä¾µæƒï¼Œè¯·è”ç³»åˆ é™¤ã€‚</p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>æœ¬æ–‡ä½œè€…ï¼š </strong>å¥”è·‘çš„IC
  </li>
  <li class="post-copyright-link">
      <strong>æœ¬æ–‡é“¾æ¥ï¼š</strong>
      <a href="http://example.com/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%BA%8C)/" title="TensorRTé‡åŒ–å®æˆ˜è¯¾YOLOv7é‡åŒ–ï¼šYOLOv7-PTQé‡åŒ–(äºŒ)">http://example.com/TensorRT/TensorRTé‡åŒ–å®æˆ˜è¯¾YOLOv7é‡åŒ–ï¼šYOLOv7-PTQé‡åŒ–(äºŒ)/</a>
  </li>
  <li class="post-copyright-license">
    <strong>ç‰ˆæƒå£°æ˜ï¼š </strong>æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜å‡ºå¤„ï¼
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C</a>
              <a href="/tags/Tensorrt/" rel="tag"><i class="fa fa-tag"></i> Tensorrt</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9AYOLOv7-PTQ%E9%87%8F%E5%8C%96(%E4%B8%80)/" rel="prev" title="TensorRTé‡åŒ–å®æˆ˜è¯¾YOLOv7é‡åŒ–ï¼šYOLOv7-PTQé‡åŒ–(ä¸€)">
                  <i class="fa fa-chevron-left"></i> TensorRTé‡åŒ–å®æˆ˜è¯¾YOLOv7é‡åŒ–ï¼šYOLOv7-PTQé‡åŒ–(ä¸€)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/TensorRT/TensorRT%E9%87%8F%E5%8C%96%E5%AE%9E%E6%88%98%E8%AF%BEYOLOv7%E9%87%8F%E5%8C%96%EF%BC%9Apytorch/" rel="next" title="TensorRTé‡åŒ–å®æˆ˜è¯¾YOLOv7é‡åŒ–ï¼špytorch">
                  TensorRTé‡åŒ–å®æˆ˜è¯¾YOLOv7é‡åŒ–ï¼špytorch <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">å¥”è·‘çš„IC</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="æ€»è®¿å®¢é‡">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="æ€»è®¿é—®é‡">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="è¿”å›é¡¶éƒ¨">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"zmurder","repo":"zmurder.github.io","client_id":"cf2343f27b6c29efe0bc","client_secret":"3268a1fa92706c7358d5421f88f76a0f7ada3188","admin_user":"zmurder","distraction_free_mode":true,"proxy":"https://strong-caramel-969805.netlify.app/github_access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"4b80694aa401cc35e899f7f923e6bfbc"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
