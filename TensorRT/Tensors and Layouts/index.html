<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
  <meta name="msvalidate.01" content="7EC20DBC74B004C2782077570E15C280">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.14.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8693861384618000"
     crossorigin="anonymous"></script>
    <meta name="description" content="1 简介关于tensor的内存排布已经有几篇文章都提到了。这里发现官网也一篇解释。直接搬过来，不翻译了，怕翻译出现异议。">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensors and Layouts">
<meta property="og:url" content="http://example.com/TensorRT/Tensors%20and%20Layouts/index.html">
<meta property="og:site_name" content="奔跑的IC">
<meta property="og:description" content="1 简介关于tensor的内存排布已经有几篇文章都提到了。这里发现官网也一篇解释。直接搬过来，不翻译了，怕翻译出现异议。">
<meta property="og:locale">
<meta property="og:image" content="http://example.com/TensorRT/Tensors%20and%20Layouts/ex-tensor.png">
<meta property="og:image" content="http://example.com/TensorRT/Tensors%20and%20Layouts/nchw.png">
<meta property="og:image" content="http://example.com/TensorRT/Tensors%20and%20Layouts/nhwc.png">
<meta property="og:image" content="http://example.com/TensorRT/Tensors%20and%20Layouts/NC32HW32.png">
<meta property="article:published_time" content="2024-12-18T14:59:20.263Z">
<meta property="article:modified_time" content="2024-12-18T15:29:55.764Z">
<meta property="article:author" content="奔跑的IC">
<meta property="article:tag" content="git">
<meta property="article:tag" content="C">
<meta property="article:tag" content="Tensorrt">
<meta property="article:tag" content="DeepLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/TensorRT/Tensors%20and%20Layouts/ex-tensor.png">


<link rel="canonical" href="http://example.com/TensorRT/Tensors%20and%20Layouts/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://example.com/TensorRT/Tensors%20and%20Layouts/","path":"TensorRT/Tensors and Layouts/","title":"Tensors and Layouts"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Tensors and Layouts | 奔跑的IC</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">奔跑的IC</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-reorder fa-fw"></i>文章列表</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E7%AE%80%E4%BB%8B"><span class="nav-text">1 简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensors-and-Layouts%EF%83%81"><span class="nav-text">Tensors and Layouts</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor-Descriptor%EF%83%81"><span class="nav-text">Tensor Descriptor</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#WXYZ-Tensor-Descriptor%EF%83%81"><span class="nav-text">WXYZ Tensor Descriptor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-D-Tensor-Descriptor%EF%83%81"><span class="nav-text">3-D Tensor Descriptor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-D-Tensor-Descriptor%EF%83%81"><span class="nav-text">4-D Tensor Descriptor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-D-Tensor-Descriptor%EF%83%81"><span class="nav-text">5-D Tensor Descriptor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Fully-Packed-Tensors%EF%83%81"><span class="nav-text">Fully-Packed Tensors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Partially-Packed-Tensors%EF%83%81"><span class="nav-text">Partially-Packed Tensors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spatially-Packed-Tensors%EF%83%81"><span class="nav-text">Spatially Packed Tensors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Overlapping-Tensors%EF%83%81"><span class="nav-text">Overlapping Tensors</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Layout-Formats%EF%83%81"><span class="nav-text">Data Layout Formats</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Example-Tensor%EF%83%81"><span class="nav-text">Example Tensor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Convolution-Layouts%EF%83%81"><span class="nav-text">Convolution Layouts</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#NCHW-Memory-Layout%EF%83%81"><span class="nav-text">NCHW Memory Layout</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#NHWC-Memory-Layout%EF%83%81"><span class="nav-text">NHWC Memory Layout</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#NC-32HW32-Memory-Layout%EF%83%81"><span class="nav-text">NC&#x2F;32HW32 Memory Layout</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95%EF%BC%9A"><span class="nav-text">附录：</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="奔跑的IC"
      src="/images/zyd.gif">
  <p class="site-author-name" itemprop="name">奔跑的IC</p>
  <div class="site-description" itemprop="description">死磕牛角的IT农民工</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">190</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/TensorRT/Tensors%20and%20Layouts/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/zyd.gif">
      <meta itemprop="name" content="奔跑的IC">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="奔跑的IC">
      <meta itemprop="description" content="死磕牛角的IT农民工">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Tensors and Layouts | 奔跑的IC">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Tensors and Layouts
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-12-18 22:59:20 / 修改时间：23:29:55" itemprop="dateCreated datePublished" datetime="2024-12-18T22:59:20+08:00">2024-12-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TensorRT/" itemprop="url" rel="index"><span itemprop="name">TensorRT</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p>关于tensor的内存排布已经有几篇文章都提到了。这里发现官网也一篇解释。直接搬过来，不翻译了，怕翻译出现异议。</p>
<h2 id="Tensors-and-Layouts"><a href="#Tensors-and-Layouts" class="headerlink" title="Tensors and Layouts"></a>Tensors and Layouts<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#tensors-and-layouts"></a></h2><p>Whether using the graph API or the legacy API, cuDNN operations take tensors as input and produce tensors as output.</p>
<h3 id="Tensor-Descriptor"><a href="#Tensor-Descriptor" class="headerlink" title="Tensor Descriptor"></a>Tensor Descriptor<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#tensor-descriptor"></a></h3><p>The cuDNN library describes data with a generic n-D tensor descriptor defined with the following parameters:</p>
<blockquote>
<ul>
<li>a number of dimensions from 3 to 8</li>
<li>a data type (32-bit floating-point, 64 bit-floating point, 16-bit floating-point…)</li>
<li>an integer array defining the size of each dimension</li>
<li>an integer array defining the stride of each dimension (for  example, the number of elements to add to reach the next element from  the same dimension)</li>
</ul>
</blockquote>
<p>This tensor definition allows, for example, to have some dimensions  overlapping each other within the same tensor by having the stride of  one dimension smaller than the product of the dimension and the stride  of the next dimension. In cuDNN, unless specified otherwise, all  routines will support tensors with overlapping dimensions for  forward-pass input tensors, however, dimensions of the output tensors  cannot overlap. Even though this tensor format supports negative strides (which can be useful for data mirroring), cuDNN routines do not support tensors with negative strides unless specified otherwise.</p>
<h4 id="WXYZ-Tensor-Descriptor"><a href="#WXYZ-Tensor-Descriptor" class="headerlink" title="WXYZ Tensor Descriptor"></a>WXYZ Tensor Descriptor<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#wxyz-tensor-descriptor"></a></h4><p>Tensor descriptor formats are identified using acronyms, with each  letter referencing a corresponding dimension. In this document, the  usage of this terminology implies:</p>
<blockquote>
<ul>
<li>all the strides are strictly positive</li>
<li>the dimensions referenced by the letters are sorted in decreasing order of their respective strides</li>
</ul>
</blockquote>
<h4 id="3-D-Tensor-Descriptor"><a href="#3-D-Tensor-Descriptor" class="headerlink" title="3-D Tensor Descriptor"></a>3-D Tensor Descriptor<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#d-tensor-descriptor"></a></h4><p>A 3-D tensor is commonly used for matrix multiplications, with three  letters: B, M, and N. B represents the batch size (for batch matmul), M  represents the number of rows, and N represents the number of columns.  Refer to the <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/api/cudnn-graph-library.html#cudnn-backend-operation-matmul-descriptor">CUDNN_BACKEND_OPERATION_MATMUL_DESCRIPTOR</a> operation for more information.</p>
<h4 id="4-D-Tensor-Descriptor"><a href="#4-D-Tensor-Descriptor" class="headerlink" title="4-D Tensor Descriptor"></a>4-D Tensor Descriptor<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#four-d-tensor-descriptor"></a></h4><p>A 4-D tensor descriptor is used to define the format for batches of  2D images with 4 letters: N,C,H,W for respectively the batch size, the  number of feature maps, the height and the width. The letters are sorted in decreasing order of the strides. The commonly used 4-D tensor  formats are:</p>
<blockquote>
<ul>
<li>NCHW</li>
<li>NHWC</li>
<li>CHWN</li>
</ul>
</blockquote>
<h4 id="5-D-Tensor-Descriptor"><a href="#5-D-Tensor-Descriptor" class="headerlink" title="5-D Tensor Descriptor"></a>5-D Tensor Descriptor<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#five-d-tensor-descriptor"></a></h4><p>A 5-D tensor descriptor is used to define the format of the batch of  3D images with 5 letters: N,C,D,H,W for respectively the batch size, the number of feature maps, the depth, the height, and the width. The  letters are sorted in decreasing order of the strides. The commonly used 5-D tensor formats are called:</p>
<blockquote>
<ul>
<li>NCDHW</li>
<li>NDHWC</li>
<li>CDHWN</li>
</ul>
</blockquote>
<h4 id="Fully-Packed-Tensors"><a href="#Fully-Packed-Tensors" class="headerlink" title="Fully-Packed Tensors"></a>Fully-Packed Tensors<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#fully-packed-tensors"></a></h4><p>A tensor is defined as <code>XYZ-fully-packed</code> if, and only if:</p>
<blockquote>
<ul>
<li>the number of tensor dimensions is equal to the number of letters preceding the <code>fully-packed</code> suffix</li>
<li>the stride of the i-th dimension is equal to the product of the (i+1)-th dimension by the (i+1)-th stride</li>
<li>the stride of the last dimension is 1</li>
</ul>
</blockquote>
<h4 id="Partially-Packed-Tensors"><a href="#Partially-Packed-Tensors" class="headerlink" title="Partially-Packed Tensors"></a>Partially-Packed Tensors<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#partially-packed-tensors"></a></h4><p>The partially <code>XYZ-packed</code> terminology only applies in the context of a tensor format described  with a superset of the letters used to define a partially-packed tensor. A WXYZ tensor is defined as <code>XYZ-packed</code> if, and only if:</p>
<blockquote>
<ul>
<li>the strides of all dimensions NOT referenced in the <code>-packed</code> suffix are greater or equal to the product of the next dimension by the next stride.</li>
<li>the stride of each dimension referenced in the <code>-packed</code> suffix in position i is equal to the product of the (i+1)-st dimension by the (i+1)-st stride.</li>
<li>if the last tensor’s dimension is present in the <code>-packed</code> suffix, its stride is 1.</li>
</ul>
</blockquote>
<p>For example, an NHWC tensor <code>WC-packed</code> means that the <code>c_stride</code> is equal to 1 and <code>w_stride</code> is equal to <code>c_dim x c_stride</code>. In practice, the <code>-packed</code> suffix is usually applied to the minor dimensions of a tensor but can  be applied to only the major dimensions; for example, an NCHW tensor  that is only <code>N-packed</code>.</p>
<h4 id="Spatially-Packed-Tensors"><a href="#Spatially-Packed-Tensors" class="headerlink" title="Spatially Packed Tensors"></a>Spatially Packed Tensors<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#spatially-packed-tensors"></a></h4><p>Spatially-packed tensors are defined as partially-packed in spatial  dimensions. For example, a spatially-packed 4D tensor would mean that  the tensor is either NCHW HW-packed or CNHW HW-packed.</p>
<h4 id="Overlapping-Tensors"><a href="#Overlapping-Tensors" class="headerlink" title="Overlapping Tensors"></a>Overlapping Tensors<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#overlapping-tensors"></a></h4><p>A tensor is defined to be overlapping if iterating over a full range  of dimensions produces the same address more than once. In practice an  overlapped tensor will have <code>stride[i-1] &lt; stride[i]*dim[i]</code> for some of the <code>i</code> from <code>[1,nbDims]</code> interval.</p>
<h3 id="Data-Layout-Formats"><a href="#Data-Layout-Formats" class="headerlink" title="Data Layout Formats"></a>Data Layout Formats<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#data-layout-formats"></a></h3><p>This section describes how cuDNN tensors are arranged in memory according to several data layout formats.</p>
<p>The recommended way to specify the layout format of a tensor is by  setting its strides accordingly. For compatibility with the v7 API, a  subset of the layout formats can also be configured through the <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/api/cudnn-graph-library.html#cudnntensorformat-t">cudnnTensorFormat_t</a> enum. The enum is only supplied for legacy reasons and is deprecated.</p>
<h4 id="Example-Tensor"><a href="#Example-Tensor" class="headerlink" title="Example Tensor"></a>Example Tensor<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#example-tensor"></a></h4><p>Consider a batch of images with the following dimensions:</p>
<blockquote>
<ul>
<li><strong>N</strong> is the batch size; 1</li>
<li><strong>C</strong> is the number of feature maps (that is, number of channels); 64</li>
<li><strong>H</strong> is the image height; 5</li>
<li><strong>W</strong> is the image width; 4</li>
</ul>
</blockquote>
<p>To keep the example simple, the image pixel elements are expressed as a sequence of integers, 0, 1, 2, 3, and so on.</p>
<img src="/TensorRT/Tensors%20and%20Layouts/ex-tensor.png" class="" title="ex-tensor">
<p>In the following subsections, we’ll use the above example to demonstrate the different layout formats.</p>
<h4 id="Convolution-Layouts"><a href="#Convolution-Layouts" class="headerlink" title="Convolution Layouts"></a>Convolution Layouts<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#convolution-layouts"></a></h4><p>cuDNN supports several layouts for convolution, as described in the following sections.</p>
<h5 id="NCHW-Memory-Layout"><a href="#NCHW-Memory-Layout" class="headerlink" title="NCHW Memory Layout"></a>NCHW Memory Layout<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#nchw-memory-layout"></a></h5><p>The above 4D tensor is laid out in the memory in the NCHW format)as below:</p>
<ol>
<li>Beginning with the first channel (c=0), the elements are arranged contiguously in row-major order.</li>
<li>Continue with second and subsequent channels until the elements of all the channels are laid out.</li>
<li>Proceed to the next batch (if <strong>N</strong> is &gt; 1).</li>
</ol>
<img src="/TensorRT/Tensors%20and%20Layouts/nchw.png" class="" title="nchw">
<h5 id="NHWC-Memory-Layout"><a href="#NHWC-Memory-Layout" class="headerlink" title="NHWC Memory Layout"></a>NHWC Memory Layout<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#nhwc-memory-layout"></a></h5><p>For the NHWC memory layout, the corresponding elements in all the <strong>C</strong> channels are laid out first, as below:</p>
<ol>
<li>Begin with the first element of channel 0, then proceed to the  first element of channel 1, and so on, until the first elements of all  the <strong>C</strong> channels are laid out.</li>
<li>Next, select the second element of channel 0, then proceed to the second element of channel 1, and so on, until the second element of all the channels are laid out.</li>
<li>Follow the row-major order of channel 0 and complete all the elements.</li>
<li>Proceed to the next batch (if <strong>N</strong> is &gt; 1).</li>
</ol>
<img src="/TensorRT/Tensors%20and%20Layouts/nhwc.png" class="" title="nhwc">
<h5 id="NC-32HW32-Memory-Layout"><a href="#NC-32HW32-Memory-Layout" class="headerlink" title="NC/32HW32 Memory Layout"></a>NC/32HW32 Memory Layout<a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html#nc-32hw32-memory-layout"></a></h5><p>The NC/32HW32 is similar to NHWC, with a key difference. For the  NC/32HW32 memory layout, the 64 channels are grouped into two groups of  32 channels each - first group consisting of channels <code>c0</code> through <code>c31</code>, and the second group consisting of channels <code>c32</code> through <code>c63</code>. Then each group is laid out using the NHWC format.</p>
<img src="/TensorRT/Tensors%20and%20Layouts/NC32HW32.png" class="" title="NC32HW32">
<p>For the generalized NC/xHWx layout format, the following observations apply:</p>
<blockquote>
<ul>
<li>Only the channel dimension, <code>C</code>, is grouped into <code>x</code> channels each.</li>
<li>When <code>x = 1</code>, each group has only one channel. Hence, the elements of one channel  (that is, one group) are arranged contiguously (in the row-major order), before proceeding to the next group (that is, next channel). This is  the same as the NCHW format.</li>
<li>When <code>x = C</code>, then NC/xHWx is identical to NHWC, that is, the entire channel depth <code>C</code> is considered as a single group. The case <code>x = C</code> can be thought of as vectorizing the entire <code>C</code> dimension as one big vector, laying out all the <code>C</code>, followed by the remaining dimensions, just like NHWC.</li>
<li>The tensor format <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/api/cudnn-graph-library.html#cudnntensorformat-t">cudnnTensorFormat_t</a> can also be interpreted in the following way - The NCHW INT8x32 format is really <code>N x (C/32) x H x W x 32</code> (32 <code>C``s for every ``W</code>), just as the NCHW INT8x4 format is <code>N x (C/4) x H x W x 4</code> (4 <code>C</code> for every <code>W</code>). Hence the <code>VECT_C</code> name - each <code>W</code> is a vector (4 or 32) of <code>C</code>.</li>
</ul>
</blockquote>
<h1 id="附录："><a href="#附录：" class="headerlink" title="附录："></a>附录：</h1><ul>
<li><a target="_blank" rel="noopener" href="https://zmurder.github.io/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/16-TensorRT%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%E5%AE%9A%E4%B9%89%E8%AF%A6%E8%A7%A3/">https://zmurder.github.io/TensorRT/TensorRT%E4%B8%AD%E6%96%87%E7%89%88%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C/16-TensorRT%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%E5%AE%9A%E4%B9%89%E8%AF%A6%E8%A7%A3/</a></li>
<li><a target="_blank" rel="noopener" href="https://zmurder.github.io/DeepLearning/%E5%9F%BA%E7%A1%80/NCHW%E5%92%8CNHWC/?highlight=chw">https://zmurder.github.io/DeepLearning/%E5%9F%BA%E7%A1%80/NCHW%E5%92%8CNHWC/?highlight=chw</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html">https://docs.nvidia.com/deeplearning/cudnn/v9.4.0/developer/core-concepts.html</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>奔跑的IC
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://example.com/TensorRT/Tensors%20and%20Layouts/" title="Tensors and Layouts">http://example.com/TensorRT/Tensors and Layouts/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/git/" rel="tag"><i class="fa fa-tag"></i> git</a>
              <a href="/tags/C/" rel="tag"><i class="fa fa-tag"></i> C</a>
              <a href="/tags/Tensorrt/" rel="tag"><i class="fa fa-tag"></i> Tensorrt</a>
              <a href="/tags/DeepLearning/" rel="tag"><i class="fa fa-tag"></i> DeepLearning</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/RaspberryPi/%E6%A0%91%E8%8E%93%E6%B4%BE%E9%85%8D%E7%BD%AEVNC/" rel="prev" title="树莓派配置VNC">
                  <i class="fa fa-chevron-left"></i> 树莓派配置VNC
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/git/git-stash%E7%94%A8%E6%B3%95/" rel="next" title="git-stash用法">
                  git-stash用法 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">奔跑的IC</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div><script color="0,0,255" opacity="0.5" zIndex="-1" count="99" src="https://lib.baomitu.com/canvas-nest.js/1.0.1/canvas-nest.js"></script>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"zmurder","repo":"zmurder.github.io","client_id":"cf2343f27b6c29efe0bc","client_secret":"3268a1fa92706c7358d5421f88f76a0f7ada3188","admin_user":"zmurder","distraction_free_mode":true,"proxy":"https://strong-caramel-969805.netlify.app/github_access_token","language":"zh-CN","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/gitalk/1.8.0/gitalk.min.js","integrity":"sha256-MVK9MGD/XJaGyIghSVrONSnoXoGh3IFxLw0zfvzpxR4="},"path_md5":"9289b71260a468b71a9b78435f6f9eda"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
